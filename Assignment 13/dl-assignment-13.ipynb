{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b47e08-f87a-4827-8ad5-e4cee49b3167",
   "metadata": {},
   "source": [
    "# Deep Learning &mdash; Assignment 13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff9600e-970a-4831-861c-2e6141562301",
   "metadata": {},
   "source": [
    "Thirteenth and last assignment for the 2024 Deep Learning course (NWI-IMC070) of the Radboud University."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcd1561-95fc-4425-bc1b-35144aefbeb2",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "**Names:**\n",
    "\n",
    "**Group:**\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aac1d2-6ecd-47d2-8a2e-2341d526931b",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "* Fill in your names and the name of your group.\n",
    "* Answer the questions and complete the code where necessary.\n",
    "* Keep your answers brief, one or two sentences is usually enough.\n",
    "* Re-run the whole notebook before you submit your work.\n",
    "* Save the notebook as a PDF and submit that in Brightspace together with the `.ipynb` notebook file.\n",
    "* The easiest way to make a PDF of your notebook is via File > Print Preview and then use your browser's print option to print to PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4462df-2b54-4fe6-a21e-6f1f19bec30b",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "In this assignment you will\n",
    "1. Implement GradCAM\n",
    "2. Investigate the important features for certain inputs.\n",
    "3. Ingestigate variants of GradCAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb408c1-0a33-4431-a69e-6161c6283f31",
   "metadata": {},
   "source": [
    "## Required software\n",
    "\n",
    "As before you will need these libraries:\n",
    "* `torch` and `torchvision` for PyTorch,\n",
    "\n",
    "All libraries can be installed with `pip install`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666101c3-db46-44eb-87b0-8ef10d974de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['png']\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import PIL\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "from matplotlib import colormaps\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.io.image import read_image\n",
    "\n",
    "# Use the GPU if available\n",
    "def detect_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "device = detect_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e713fd2",
   "metadata": {},
   "source": [
    "## 13.1 A pretrained network (5 points)\n",
    "\n",
    "In this assignment we will be working with a pre-trained network.\n",
    "These are available in torchvision. Look at [the documentation](https://pytorch.org/vision/stable/models.html) for more information on pretrained models.\n",
    "\n",
    "In this assignment we will use the ResNet50 model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20ecc61",
   "metadata": {},
   "source": [
    "**(a) Load a pretrained ResNet50 model, with default weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593c8693",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = ResNet50_Weights.DEFAULT\n",
    "model = ...\n",
    "model = model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d900fdba",
   "metadata": {},
   "source": [
    "Next we will download a couple of random images, which we will use for the rest of this notebook.\n",
    "\n",
    "**(b) Run the code below to download the test images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108ce604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download some images\n",
    "urls = [\n",
    "    'http://images.cocodataset.org/test-stuff2017/000000006149.jpg',\n",
    "    'https://farm8.staticflickr.com/7020/6810252887_01e3d8e4e6_z.jpg',\n",
    "    'http://images.cocodataset.org/val2017/000000039769.jpg',\n",
    "    'http://images.cocodataset.org/train2017/000000140285.jpg',\n",
    "    # Some mroe images to play around with\n",
    "    #'https://farm1.staticflickr.com/35/67223286_72fce12163_z.jpg',\n",
    "    #'https://farm4.staticflickr.com/3587/3508226585_268a2e5b9b_z.jpg',\n",
    "    #'https://farm1.staticflickr.com/6/6947166_ba80959bbb_z.jpg',\n",
    "    #'https://farm3.staticflickr.com/2325/5821134041_b96fef0946_z.jpg',\n",
    "    #'https://farm1.staticflickr.com/115/296513905_ddb2cf6438_z.jpg',\n",
    "    #'https://farm8.staticflickr.com/7020/6810252887_01e3d8e4e6_z.jpg',\n",
    "]\n",
    "\n",
    "def load_url(url):\n",
    "    filename = os.path.basename(url)\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "    return read_image(filename)\n",
    "\n",
    "images = [load_url(url) for url in urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ef4389",
   "metadata": {},
   "source": [
    "The code below shows these images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b06fcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "for i, image in enumerate(images):\n",
    "    plt.subplot(1,len(images), i+1)\n",
    "    plt.imshow(transforms.ToPILImage()(image))\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3906fc42",
   "metadata": {},
   "source": [
    "The pretrained model was trained with a specific image preprocessing. So we should use the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c025890",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = weights.transforms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de15a1f",
   "metadata": {},
   "source": [
    "**(c) Preprocess the images, and store them in a single tensor.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: [`torch.stack`](https://pytorch.org/docs/stable/generated/torch.stack.html) can be useful.\n",
    "\n",
    "Note: you may get a warning about parameters of transform, you can ignore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad35951",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ...\n",
    "\n",
    "# Verify that the batch has the right shape\n",
    "assert(x.shape == torch.Size([len(images), 3, 224, 224]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b5197c",
   "metadata": {},
   "source": [
    "It is also useful to be able to undo the preprocessing, at least the normalization and the dimension permutation, so that we can plot the preprocessed images as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf77daaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize(x):\n",
    "    \"\"\"Make a preprocessed image showable.\"\"\"\n",
    "    x = torch.permute(x, [1,2,0]) # Make the color channels the last dimension\n",
    "    x = x * torch.tensor(preprocess.std)[None,None,:]\n",
    "    x = x + torch.tensor(preprocess.mean)[None,None,:]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b890e8",
   "metadata": {},
   "source": [
    "**(d) Plot the pre-processed images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a25c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "for i, image in enumerate(x):\n",
    "    plt.subplot(1,len(x), i+1)\n",
    "    plt.imshow(unnormalize(image.cpu()))\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d0fa97",
   "metadata": {},
   "source": [
    "**(e) Compare the preprocessed images to the original. How does the default preprocessing make all the images the same size?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc76010d",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9b1d4d",
   "metadata": {},
   "source": [
    "**(f) Run the classifier on the batch of images.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febc5872",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7da802d",
   "metadata": {},
   "source": [
    "The classifier predicts a logit scores, represing the likelihood of 1000 classes.\n",
    "\n",
    "You can use the [`torch.topk`](https://pytorch.org/docs/stable/generated/torch.topk.html) function to get a list of the predicted labels with the highest score. This function returns a tuple with two elements, `result.indices` is the indices, and `result.values` contains the correspondig scores.\n",
    "\n",
    "By themselves, these indices don't tell you what the predicted class actually is. For that you can use the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06895495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def category(index):\n",
    "    \"\"\"Return the textual category for some predictions\"\"\"\n",
    "    return np.array(weights.meta[\"categories\"])[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f89523",
   "metadata": {},
   "source": [
    "**(g) Create a table with the textual label for the top 5 predicted labels for each image.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: [`pd.DataFrame`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) is an easy way of producing a nice looking table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e190584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make a table with the label of the top 5 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee4fc29",
   "metadata": {},
   "source": [
    "**(h) Are there any strange labels in the top 5 predictions for any of the images?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aa305a",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f241c8c7",
   "metadata": {},
   "source": [
    "## 13.2 Hooking into the network (4 points)\n",
    "\n",
    "To understand and visualize which part of the images are 'causing' the predicted labels, we can use [GradCAM](https://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf).\n",
    "\n",
    "GradCAM needs to know the activations and output gradients of the last layer in the model, just before the global pooling layer. The activations are computed during forward propagation (`model.forward()`), and the gradients are computed when calling `y.backward()`, but neither of them are saved anywhere.\n",
    "\n",
    "To capture the activations and gradients we need to use hooks, which allow us to register a function that gets called every time a module's `forward` or `backward` function is called.\n",
    "\n",
    "Take a look at\n",
    "[`torch.Module.register_forward_hook`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_forward_hook) and [`torch.Module.register_full_backward_hook`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b30fe63",
   "metadata": {},
   "source": [
    "**(a) Complete the code below.<span style=\"float:right\"> (2 points)</span>**\n",
    "\n",
    "You can store the activations and gradients as a member variable of the layer, or you can use a global variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bc5c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hooks(layer):\n",
    "    \"\"\"\n",
    "    Add hooks to a layer that store the activations (the output of the layer) in the forward pass,\n",
    "    as well as the gradient wrt. the output in the backward pass\n",
    "    \"\"\"\n",
    "    def forward_hook(layer, args, output):\n",
    "        # This function will be called after the forward pass.\n",
    "        #   args are the inputs of the layer\n",
    "        #   output is the output of the layer\n",
    "        \n",
    "        pass # TODO: store activations\n",
    "\n",
    "    def backward_hook(layer, grad_input, grad_output):\n",
    "        # This function is called after the backward pass\n",
    "        #   grad_input is a *tuple* that contains the gradients wrt the inputs.\n",
    "        #   grad_output is a *tuple* the gradient the gradients wrt the outputs.\n",
    "        pass # TODO: store gradients\n",
    "\n",
    "    # Remove old hooks (if any)\n",
    "    remove_hooks(layer)\n",
    "    \n",
    "    # Register new hooks\n",
    "    layer.forward_hook  = layer.register_forward_hook(forward_hook)\n",
    "    layer.backward_hook = layer.register_full_backward_hook(backward_hook)\n",
    "\n",
    "def remove_hooks(layer):\n",
    "    if hasattr(layer,'forward_hook'):\n",
    "        layer.forward_hook.remove()\n",
    "    if hasattr(layer,'backward_hook'):\n",
    "        layer.backward_hook.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa4715b",
   "metadata": {},
   "source": [
    "**(b) Which layer of the ResNet50 model should the hooks be applied to?<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: you can `print` a model, and you can access a named layer using `model.layer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49b078c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hooked_layer = ... # layer just before global pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebe92ae",
   "metadata": {},
   "source": [
    "**(c) Add the hooks to the layer.<span style=\"float:right\"> (no points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97b8f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_hooks(hooked_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d98341b",
   "metadata": {},
   "source": [
    "**(d) Verify that the hooks work, by completing and running the code below.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e0428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ... # TODO: run model forward on the first image, x[[0]]\n",
    "top_class = ... # TODO: get the top class with torch.argmax\n",
    "# TODO: run model backward for the logit of top predicted class\n",
    "layer_activations = ... # TODO: get stored activations\n",
    "layer_gradients = ...   # TODO: get stored gradients\n",
    "\n",
    "assert layer_activations.shape[1:] == torch.Size([2048, 7, 7]), \"Activations has the wrong shape. Maybe you did not add hooks to the last layer before global pooling.\"\n",
    "assert layer_gradients.shape == layer_activations.shape, \"Gradients have the wrong shape. Make sure to use the output gradients, and note that grad_output is a tuple.\"\n",
    "assert torch.mean(layer_activations) != 0, \"Activations should not be zero\"\n",
    "assert torch.mean(layer_gradients) != 0, \"Gradients should not be zero\"\n",
    "assert torch.mean(layer_activations - layer_gradients) != 0, \"Gradients should not be the same as the activations\"\n",
    "assert top_class == 752, \"Did you use the right image\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80cadb6",
   "metadata": {},
   "source": [
    "## 13.3 GradCAM (9 points)\n",
    "\n",
    "Now we are ready to implement GradCAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264fa0be",
   "metadata": {},
   "source": [
    "**(a) GradCAM uses 'neuron importance weights', based on the gradients. Compute these importance weights.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "See equation (1) of [the paper](https://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded47377",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_weights = ...\n",
    "\n",
    "assert importance_weights.squeeze().shape == torch.Size([2048])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cba7ac0",
   "metadata": {},
   "source": [
    "**(b) Combine the importance weights with the activations to get the gradcam map.<span style=\"float:right\"> (2 point)</span>**\n",
    "\n",
    "See equation (2) of [the paper](https://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf).\n",
    "\n",
    "Hint: To combine two tensors they need to have the same shape. You can make them line up by adding new dimensions to a tensor using `tensor[:,None,:,None]`, where `None` is a new dimension, while `:` is an existing dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692ee801",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradcam_map = ...\n",
    "\n",
    "assert gradcam_map.shape == torch.Size([7,7])\n",
    "assert torch.min(gradcam_map) >= 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b7a861",
   "metadata": {},
   "source": [
    "**(c) Plot the gradcam map.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79466509",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(gradcam_map.detach().cpu());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f74e74",
   "metadata": {},
   "source": [
    "We can resize the map using interplation to make it smoother, and to align it with the input image.\n",
    "\n",
    "**(d) Plot the resized and smoothed GradCAM map.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895d34ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_map(map, size=(224,224)):\n",
    "    return np.asarray(to_pil_image(map.detach().cpu(),'F').resize(size))\n",
    "\n",
    "plt.imshow(resize_map(gradcam_map));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a85a87",
   "metadata": {},
   "source": [
    "**(e) Overlay the gradcam map over the input image.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "You can overlay two images by drawing the second one semi-transparent with `alpha=0.5`.\n",
    "\n",
    "Expected output: the highlighted region should correspond to the predicted label.\n",
    "Although the alignment might be slightly off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f375149",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(unnormalize(x[0].cpu()));\n",
    "plt.imshow(resize_map(gradcam_map), alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fada481",
   "metadata": {},
   "source": [
    "We needed several steps to compute a gradCAM visualisation. Let's clean up the code by encapsulating it in a function.\n",
    "\n",
    "**(f) Complete the code below.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: the model always works on a batch of images, you can turn a single image into a batch by adding a new dimension, `image[None]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98093523",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradcam(image, index):\n",
    "    \"\"\"\n",
    "    Compute a GradCAM map for the given input image, and class index.\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    return gradcam_map\n",
    "\n",
    "assert torch.any(gradcam(x[0],0) != gradcam(x[1],0)), \"Are you using the incorrect global variables?\"\n",
    "assert torch.all(gradcam(x[0],0) == gradcam(x[0],0)), \"Calling the function twice gives different results, perhaps some gradients are not cleared?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d0e073",
   "metadata": {},
   "source": [
    "**(g) Complete the code below to create a class activation map for the top 5 label for each of the images.<span style=\"float:right\"> (2 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea7768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "for i, image in enumerate(x):\n",
    "    top5_indices = ...\n",
    "    for j, index in enumerate(top5_indices):\n",
    "        plt.subplot(len(images),5, i*5 + j + 1)\n",
    "        # TODO: compute and plot gradcam map, overlayed on the image\n",
    "        # TODO: set title to label of the class index\n",
    "        plt.title(...)\n",
    "        plt.title(category(index))\n",
    "        plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44974e50",
   "metadata": {},
   "source": [
    "**(h) Do the gradcam maps make sense? Do the highlighted areas roughly correspond to the location of objects in the image? Mention specific examples.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5940a19d",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7440e3",
   "metadata": {},
   "source": [
    "**(i) Do the highlighted areas correspond *exactly* to the location of objects in the image? Why / why not?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a90b991",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d8b27f",
   "metadata": {},
   "source": [
    "## 13.4 Variants (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d759aea",
   "metadata": {},
   "source": [
    "GradCAM includes a non-linear activation function. The result is that the GradCAM map that is produced only contains positive values. But perhaps the negative values also contain useful information?\n",
    "\n",
    "**(a) Copy and modify the code from part 13.3f and g, by negating the input to the non-linearity.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d229b42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradcam_negative(image, index):\n",
    "    pass # TODO: see 13.3f, but add negation\n",
    "\n",
    "# TODO: see 13.3g\n",
    "\n",
    "assert torch.all((gradcam(x[0],123) > 0) != (gradcam_negative(x[0],123) > 0)), \"The negative and positive GradCAM can not be active in the same locations of the image\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2e90fb",
   "metadata": {},
   "source": [
    "**(b) What are the areas that are highlighted now?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bee9d0",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9a3552",
   "metadata": {},
   "source": [
    "**(c) Plot the negative gradcam map for the bottom 5 predictions instead of the top 5.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: the bottom 5 of `x` is the top 5 of `-x`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a226ec",
   "metadata": {},
   "source": [
    "**(d) What can you learn from these maps? Give at least 1 concrete example.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d19efe2",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd831d33",
   "metadata": {},
   "source": [
    "Instead of looking at the last layer before global pooling, we could in theory also apply GradCAM at a different layer of the network. Perhaps that can give a higher resolution map?\n",
    "\n",
    "**(e) Adapt the gradcam code to take the model and layer of the model as parameters <span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Note: you may need to add hooks at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6c5b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradcam_at(model, layer, image, index):\n",
    "    \"\"\"\n",
    "    Compute a GradCAM map at the given layer of the given model\n",
    "    \"\"\"\n",
    "    pass # TODO: see 13.3f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935bab04",
   "metadata": {},
   "source": [
    "**(f) Now use gradcam at an intermediate layer of the model <span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3c62df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: see 13.3g\n",
    "\n",
    "assert gradcam_map.shape != torch.Size([7,7]), \"Use a different layer\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3423c1ad",
   "metadata": {},
   "source": [
    "**(g) Do the gradcam maps for this earlier layer give a useful visualization?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845445c2",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac91021-4163-4ee0-851f-314b63245a63",
   "metadata": {},
   "source": [
    "## 13.5 Of cats and dogs (5 points)\n",
    "\n",
    "For the second image (`images[1]`, the one with the white background), there is an object that is clearly in the image, but it does not appear in the top 5 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abee3093-a4fb-4f70-bffb-edbf2e0431bd",
   "metadata": {},
   "source": [
    "**(a) Why is this object not detected in the top 5?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f00aa26-fe08-42c7-8eab-18956cac3fcf",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede09279",
   "metadata": {},
   "source": [
    "**(b) How could you reduce the bias of the model for the second image, to make it also detect the other object?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a3c63f",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e6b23e",
   "metadata": {},
   "source": [
    "**(c) Curuously, for this second image, the bottom 5 labels for this image also include dogs. Why would that happen?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a43594",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dbde40",
   "metadata": {},
   "source": [
    "**(d) Create a GradCAM map for the missing object in the image.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: The third images contains some labels that you can use.\n",
    "You can also use the following function to get the corresponding class index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061bc586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_category(query):\n",
    "    \"\"\"Find a category that has or contains the given name.\"\"\"\n",
    "    found = [i for (i,k) in enumerate(weights.meta[\"categories\"]) if k.find(query)!=-1]\n",
    "    if len(found) == 1:\n",
    "        return found[0]\n",
    "    elif len(found) > 1:\n",
    "        raise Exception(\"Multiple categories found: \" + str(category(found)))\n",
    "    else:\n",
    "        raise Exception(\"No category found that matches: \" + str(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e76729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make a gradcam map for another class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff4bf9e",
   "metadata": {},
   "source": [
    "**(e) Are you able to find this object when you go looking for it? Explain your answer briefly.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98015a48",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9435fe89-1c11-4e2c-a453-ae4c801b82de",
   "metadata": {},
   "source": [
    "## The end\n",
    "\n",
    "Well done! Please double check the instructions at the top before you submit your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This assignment has 30 points.*\n",
    "<span style=\"float:right;color:#aaa;font-size:10px;\"> Version cb53811 / 2024-12-05</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}