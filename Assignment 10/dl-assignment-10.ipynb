{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning &mdash; Assignment 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenth assignment for the 2024 Deep Learning course (NWI-IMC070) of the Radboud University."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "**Names:**\n",
    "\n",
    "**Group:**\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "* Fill in your names and the name of your group.\n",
    "* Answer the questions and complete the code where necessary.\n",
    "* Keep your answers concise and to the point, one or two sentences is usually enough.\n",
    "* Re-run the whole notebook before you submit your work.\n",
    "* Save the notebook as a PDF and submit that in Brightspace together with the `.ipynb` notebook file.\n",
    "* The easiest way to make a PDF of your notebook is via File > Print Preview and then use your browser's print option to print to PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "In this assignment you will\n",
    "1. Implement the components of a normalizing flow model.\n",
    "2. Train a normalizing flow on a simple dataset.\n",
    "3. Compare normalizing flows to other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required software\n",
    "\n",
    "As before you will need these libraries:\n",
    "* `torch`\n",
    "* `sklearn`\n",
    "\n",
    "All libraries can be installed with `pip install`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['png']\n",
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.distributions.distribution\n",
    "from torch.distributions import Distribution, Normal, MultivariateNormal\n",
    "import sklearn.datasets\n",
    "\n",
    "# Fix the seed to make the solutions more reproducible\n",
    "torch.manual_seed(12345);\n",
    "\n",
    "# Use the GPU if available\n",
    "def detect_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "device = detect_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Invertible transformations (5 points)\n",
    "\n",
    "A normalizing flow model is built out of invertible functions. We can implement these as a module that not only has a `forward` function, but also an `inverse`.\n",
    "\n",
    "In addition to computing the function $z = f(x)$, we will also need to know the (log) determinant of the Jacobian, that is $\\det(\\partial f(z) / \\partial z)$. While we could use automatic differentiation to compute the Jacobian, that is very expensive, and we would then still need to compute the determinant, and we also need gradients of this log determinant. So instead, we will make our layers return a tuple of the new value and the log determinant.\n",
    "We will have a closer look at what this means in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlowLayer(torch.nn.Module):\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Apply a transformation to z_1, and compute log determinant of the Jacobian.\n",
    "        z can be a tensor or a tuple of tensors.\n",
    "        Return z_2, log det(\u2202z_2/\u2202z_1)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Forward function not implemented\")\n",
    "\n",
    "    def inverse(self, z):\n",
    "        \"\"\"\n",
    "        Transformation that is the inverse of self.forward()\n",
    "        Return z_1, log det(\u2202z_1/\u2202z_2)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Inverse function not implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice aspect of these FlowLayers is that we can test if they are implemented correctly by checking that `layer.inverse(layer.forward(x)[0])[0] == x`.\n",
    "Below is a function that tests that a flow layer is implemented correctly by checking this and other properties.\n",
    "You do not have to understand all the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isclose(a,b):\n",
    "    \"\"\"\n",
    "    Returns True if two tensors or tuples of tensors are equal or close to equal.\n",
    "    \"\"\"\n",
    "    if isinstance(a, tuple):\n",
    "        return len(a) == len(b) and all([isclose(c,d) for c,d in zip(a,b)])\n",
    "    elif isinstance(a, int) or isinstance(a, float):\n",
    "        return isclose(torch.tensor(a), torch.tensor(b))\n",
    "    else:\n",
    "        return torch.isclose(a, b, atol=1e-6).all()\n",
    "\n",
    "def num_samples(a):\n",
    "    \"\"\"\n",
    "    Return the number of samples in a tensor or in a tuple of tensors.\n",
    "    \"\"\"\n",
    "    if isinstance(a, tuple):\n",
    "        return num_samples(a[0])\n",
    "    else:\n",
    "        return len(a)\n",
    "\n",
    "def log_det_jacobian(layer, a, *args):\n",
    "    \"\"\"\n",
    "    Compute logdet of the Jacobian using automatic differentiation.\n",
    "    \"\"\"\n",
    "    if isinstance(a, tuple):\n",
    "        j = torch.autograd.functional.jacobian(lambda *a: layer.forward(a, *args)[0], a)\n",
    "    else:\n",
    "        j = torch.autograd.functional.jacobian(lambda a: layer.forward(a, *args)[0], a)\n",
    "    out = torch.zeros(num_samples(a))\n",
    "    for i in range(num_samples(a)):\n",
    "        if isinstance(j, torch.Tensor):\n",
    "            ji = j[i,:,i,:]\n",
    "        elif isinstance(j[0], torch.Tensor):\n",
    "            if isinstance(a, tuple):\n",
    "                ji = torch.cat([r[i,:,i,:] for r in j], dim=1)\n",
    "            else:\n",
    "                ji = torch.cat([r[i,:,i,:] for r in j], dim=0)\n",
    "        else:\n",
    "            ji = torch.cat([torch.cat([c[i,:,i,:] for c in r], dim=1) for r in j], dim=0)\n",
    "        out[i] = torch.logdet(ji)\n",
    "    return out\n",
    "\n",
    "def scalar_to_vector(x, n):\n",
    "    if not isinstance(x, torch.Tensor):\n",
    "        return torch.tensor(x, dtype=torch.float).repeat(n)\n",
    "    elif x.shape == torch.Size():\n",
    "        return x.repeat(n)\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def test_flow_layer(layer, z, *args):\n",
    "    \"\"\"\n",
    "    Verify that a layer's forward() and inverse() functions are inverses of each other,\n",
    "    and that the log determinant is sensible.\n",
    "    \"\"\"\n",
    "    z2, log_det_forward = layer.forward(z, *args)\n",
    "    z3, log_det_inverse = layer.inverse(z2, *args)\n",
    "    log_det_forward = scalar_to_vector(log_det_forward, num_samples(z))\n",
    "    log_det_inverse = scalar_to_vector(log_det_inverse, num_samples(z))\n",
    "    assert num_samples(z2) == num_samples(z), \\\n",
    "            \"Layer's output should have same number of samples as the input.\"\n",
    "    assert isclose(z3, z), \\\n",
    "            \"Layer's `inverse` function should be the inverse of `forward`.\"\n",
    "    assert isinstance(log_det_forward, torch.Tensor) and log_det_forward.shape == torch.Size([num_samples(z)]), \\\n",
    "            \"Log determinant of Jacobian should be a tensor with one element per sample (hint: `num_samples` gives the number of samples).\"\n",
    "    assert isclose(log_det_forward, -log_det_inverse), \\\n",
    "            \"Log determinant of Jacobian for inverse should be negative of log determinant of Jacobian of forward.\"\n",
    "    assert isclose(log_det_forward, log_det_jacobian(layer, z, *args)), \\\n",
    "            \"Log determinant returned by `forward` should match that computed using `log_det_jacobian`\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How transformations affects probability density\n",
    "\n",
    "Suppose that we have $z \\sim U(0,1)^2$, that is, $z$ has a two dimensional uniform distribution.\n",
    "Now scale the values by a factor $2$, $z_2 = 2z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.rand(1000,2)\n",
    "z2 = 2 * z\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(z[:,0], z[:,1], 'b.', alpha=0.1);\n",
    "plt.xlim(-2.5, 2.5)\n",
    "plt.ylim(-2.5, 2.5)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(z2[:,0], z2[:,1], 'b.', alpha=0.1);\n",
    "plt.xlim(-2.5, 2.5)\n",
    "plt.ylim(-2.5, 2.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) How does scaling affect the area of the square? And how does it affect the probability density inside the square? <span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) If we scale one dimension by a factor 2, and another by a factor 0.5, how does this affect the area and the probability density? <span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The determinant of the Jacobian corresponds to the scale factor of the area. The log determinant of the Jacobian is of course the log of this scale factor.\n",
    "\n",
    "**(c) Use this knowledge to implement an invertible function that scales the input by a scale factor for each dimension.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scale(FlowLayer):\n",
    "    def __init__(self, scale):\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, z):\n",
    "        # TODO: implement\n",
    "        return TODO(scaled_z), TODO(log_det)\n",
    "\n",
    "    def inverse(self, z):\n",
    "        # TODO: implement\n",
    "        return TODO(scaled_z), TODO(log_det)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Test your implementation.<span style=\"float:right\"> (no points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = Scale(torch.tensor([2,2]))\n",
    "test_flow_layer(layer, torch.randn(10,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) If we translate the points by adding a constant, how does this affect the area and probability distribution? <span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Use this knowledge to implement an invertible function that translates the input by a fixed offset.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translate(FlowLayer):\n",
    "    def __init__(self, offset):\n",
    "        self.offset = offset\n",
    "\n",
    "    def forward(self, z):\n",
    "        # TODO: implement\n",
    "        return TODO(scaled_z), TODO(log_det)\n",
    "\n",
    "    def inverse(self, z):\n",
    "        # TODO: implement\n",
    "        return TODO(scaled_z), TODO(log_det)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) Test your implementation.<span style=\"float:right\"> (no pointss)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = Translate(torch.tensor([2,2]))\n",
    "test_flow_layer(layer, torch.randn(10,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Invertible transformations for RealNVP (10 points)\n",
    "\n",
    "In this section we will build all the invertible transformations that we need to implement the RealNVP normalizing flow model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affine coupling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main ingredient of our normalizing flow model is going to be the *affine coupling layer*.\n",
    "This layer takes an input that is split in two parts ($z_A$ and $z_B$), and applies an affine tranformation to $z_B$ using parameters that come from a neural network block applied on $z_A$.\n",
    "\n",
    "In other words:\n",
    "$$z_B' = \\exp(s) z_B + t$$\n",
    "where\n",
    "$$s,t = f(z_A)$$\n",
    "\n",
    "We will handle the splitting of a single tensor into two later. For this layer, the input and output will be a tuple of tensors.\n",
    "\n",
    "**(a) Complete the implementation below.<span style=\"float:right\"> (3 points)</span>**\n",
    "\n",
    "Hint: To get two values from the neural network block $f(z_A)$ (`block` in the code below), you can use `torch.chunk`, which splits a single tensor along a given dimension into a tuple of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineCoupling(FlowLayer):\n",
    "    def __init__(self, block):\n",
    "        super().__init__()\n",
    "        self.block = block\n",
    "\n",
    "    def forward(self, z):\n",
    "        zA, zB = z\n",
    "        # TODO: implement\n",
    "        return (zA, zB), log_det\n",
    "\n",
    "    def inverse(self, z):\n",
    "        zA, zB = z\n",
    "        # TODO: implement\n",
    "        return (zA, zB), log_det"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the neural network blocks in the coupling layers, we will use simple MLPs with two hidden layers and ReLU activation functions.\n",
    "\n",
    "**(b) Complete the implementation of `block` below.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Note: the output will be split into $s$ and $t$ by the AffineCoupling layer, both of which have `dim_B` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block(dim_A, dim_B, dim_hidden):\n",
    "    \"\"\"\n",
    "    Create a neural network block for use in an `AffineCoupling` layer.\n",
    "    Parameters:\n",
    "      dim_A: the dimensionality of z_A\n",
    "      dim_B: the dimensionality of z_B\n",
    "      dim_hidden: the dimensionality of the hidden layers\n",
    "    \"\"\"\n",
    "    # TODO: implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Use the code below to verify that your implementation is correct.<span style=\"float:right\"> (no points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the implementation of the AffineCoupling layer.\n",
    "layer = AffineCoupling(block(5, 4, 12))\n",
    "z = (torch.randn(3, 5), torch.randn(3, 4))\n",
    "test_flow_layer(layer, z)\n",
    "assert not isclose(layer.forward(z)[0], z), \"The layer should not be trivial\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) If we apply two AffineCoupling functions after eachother, does this give a more powerful or flexible function compared to a single AffineCoupling layer? Explain your answer.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plumbing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can actually use the affine coupling layer, we will need to do some plumbing.\n",
    "\n",
    "The affine coupling layers we just defined above needs the input to be split into two parts (called $z_A$ and $z_B$ in the book). We can implement this splitting as a separate flow layer, because the splitting by itself is also an invertible transformation.\n",
    "\n",
    "**(e) Complete the implementation of `Split` below.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "To split a tensor you can use `torch.chunk`, which returns a tuple of tensors.\n",
    "To combine tensors you can use `torch.cat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Split(FlowLayer):\n",
    "    def forward(self, z):\n",
    "        # TODO: implement\n",
    "        return TODO(split_z), TODO(log_det)\n",
    "    def inverse(self, z):\n",
    "        # TODO: implement\n",
    "        return TODO(merged_z), TODO(log_det)\n",
    "\n",
    "class Merge(FlowLayer):\n",
    "    def forward(self, z):\n",
    "        return Split().inverse(z)\n",
    "    def inverse(self, z):\n",
    "        return Split().forward(z)\n",
    "\n",
    "assert isinstance(Split().forward(torch.randn(3, 9)), tuple), \"Split should produce a tuple: split_z, log_det\"\n",
    "assert isinstance(Split().forward(torch.randn(3, 9))[0], tuple), \"Split should produce a tuple\"\n",
    "assert isinstance(Split().inverse((torch.randn(3, 5), torch.randn(3, 4)))[0], torch.Tensor), \"Split's inverse should produce a tensor\"\n",
    "test_flow_layer(Split(), torch.randn(3, 9))\n",
    "test_flow_layer(Merge(), (torch.randn(3, 5), torch.randn(3, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The affine coupling layer always operates on the second part of the tuple, $z_B$.\n",
    "A simple way to also change $z_A$ is to swap the elements in the tuple.\n",
    "\n",
    "**(f) Implement a layer that swaps the two elements of a tuple.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swap(FlowLayer):\n",
    "    # TODO: implement\n",
    "\n",
    "z = (torch.randn(3, 5), torch.randn(3, 4))\n",
    "test_flow_layer(Swap(), z)\n",
    "z = (torch.randn(3, 5), torch.randn(3, 5))\n",
    "assert not isclose(Swap().forward(z)[0], z), \"The layer should not be trivial\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential composition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The composition of multiple invertible functions is still an invertible function.\n",
    "Normaly when composing multiple layers in sequence you would use `torch.nn.Sequential`. But that does not work for our `FlowLayer`s. Instead we can make our own class for sequential composition.\n",
    "\n",
    "**(g) Complete the implementation below.<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialFlow(FlowLayer):\n",
    "    def __init__(self, *layers):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, z):\n",
    "        # TODO: implement\n",
    "\n",
    "    def inverse(self, z):\n",
    "        # TODO: implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) Verify the implementation of `SequentialFlow`.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Make sure to use multiple non-trivial layers to ensure that the composition works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the implementation\n",
    "layer = SequentialFlow(\n",
    "    # TODO: put some layers here\n",
    ")\n",
    "z = ... # TODO: pick some input\n",
    "test_flow_layer(layer, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.3 Distributions (3 points)\n",
    "\n",
    "The other ingredient in a normalizing flow model is a base distribution.\n",
    "To sample from a normalizing flow, draw $z \\sim p_\\text{base}$, and tranform it with an invertible $f_\\text{flow}$.\n",
    "\n",
    "Fortunately for us, common probability distributions are already implemented in `torch.distributions`.\n",
    "\n",
    "Unfortunately, these are mostly univariate distributions. What we need are distributions over vectors.\n",
    "For this we can use a simple adapter class.\n",
    "\n",
    "**(a) Complete the code below.<span style=\"float:right\"> (1 points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multivariate:\n",
    "    def __init__(self, width, dist=Normal(0,1)):\n",
    "        self.width = width\n",
    "        self.dist = dist\n",
    "\n",
    "    def sample(self, n=1):\n",
    "        \"\"\"Draw n samples from this distribution.\"\"\"\n",
    "        # TODO: implement\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        \"\"\"Compute the log probability of each sample in x.\"\"\"\n",
    "        # TODO: implement\n",
    "\n",
    "dist = Multivariate(2)\n",
    "assert dist.sample(10).shape == torch.Size((10, 2))\n",
    "assert dist.log_prob(torch.randn(10,2)).shape == torch.Size((10,))\n",
    "assert dist.log_prob(torch.zeros(1,2))[0] == -torch.log(torch.tensor(2 * torch.pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Use the following code to plot the density and some samples from this distribution.<span style=\"float:right\"> (no points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_log_prob(model, xmin=-2.5, xmax=2.5, steps=500, min_prob=-10, colorbar=True):\n",
    "    with torch.no_grad():\n",
    "        x0, x1 = torch.meshgrid(torch.linspace(xmin, xmax, steps=steps),\n",
    "                                torch.linspace(xmin, xmax, steps=steps), indexing='ij')\n",
    "        x = torch.stack([x0.flatten(), x1.flatten()], axis=1).to(device)\n",
    "        y = model.log_prob(x).cpu().numpy()\n",
    "        y = y.reshape(x0.shape)\n",
    "        y[y < min_prob] = min_prob - 1\n",
    "        plt.contourf(x0, x1, y)\n",
    "        if colorbar:\n",
    "            plt.colorbar()\n",
    "\n",
    "def plot_samples(model, n = 10000, xmin=-2.5, xmax=2.5):\n",
    "    with torch.no_grad():\n",
    "        if isinstance(model, Distribution):\n",
    "            samples = model.sample([n]).cpu()\n",
    "        else:\n",
    "            samples = model.sample(n).cpu()\n",
    "        plt.plot(samples[:, 0], samples[:, 1], 'b.', alpha=0.1)\n",
    "        plt.xlim(xmin, xmax)\n",
    "        plt.ylim(xmin, xmax)\n",
    "\n",
    "def plot_log_prob_and_samples(model, n = 10000, xmin=-2.5, xmax=2.5, steps=500, min_prob=-10):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plot_log_prob(model, xmin, xmax, steps, min_prob)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plot_samples(model, n, xmin, xmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = Multivariate(2, Normal(0,1))\n",
    "plot_log_prob_and_samples(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing flows as distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A normalizing flow model also gives a probability distribution.\n",
    "So we can implement the same functions `sample` and `log_prob`.\n",
    "\n",
    "To sample, take a sample from the base distribution and apply a transformation.\n",
    "\n",
    "To compute the log probability, apply the inverse transformation, and compensate for the change in area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Complete the code below.<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizingFlow(torch.nn.Module):\n",
    "    def __init__(self, base_distribution, *layers):\n",
    "        super().__init__()\n",
    "        self.base_distribution = base_distribution\n",
    "        self.flow = SequentialFlow(*layers)\n",
    "\n",
    "    def sample(self, n=1):\n",
    "        \"\"\"Draw n samples from this distribution.\"\"\"\n",
    "        # TODO\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        \"\"\"Compute the log probability of each sample in x.\"\"\"\n",
    "        # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) As a simple test, we can visualize a normalizing flow with fixed layers.<span style=\"float:right\"> (no points)</span>**\n",
    "\n",
    "This distribution should be identical to a multivariate normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = NormalizingFlow(\n",
    "    Multivariate(2, Normal(0,1)),\n",
    "    Scale(torch.tensor([0.5,1])),\n",
    "    Translate(torch.tensor([1,-1]))\n",
    ")\n",
    "plot_log_prob_and_samples(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = MultivariateNormal(torch.tensor([1.0,-1.0]), torch.diag(torch.tensor([0.5,1.0]) ** 2))\n",
    "plot_log_prob_and_samples(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.4 Training (2 points)\n",
    "\n",
    "To train a normalizing flow model, we can maximize the log likelihood of the data, or equivalently minimize the negative log likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Implement the `train` function.<span style=\"float:right\"> (2 points)</span>**\n",
    "\n",
    "Assume that the data set contains `x,y` pairs, and ignore `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, num_epochs=100, lr=0.001, optimizer=torch.optim.Adam, device=device):\n",
    "    \"\"\"\n",
    "    Train a normalizing flow model on the given data set.\n",
    "    \"\"\"\n",
    "    # TODO: Your code here.\n",
    "    # Hint: See previous assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.5 Two moons dataset (6 points)\n",
    "\n",
    "In this assignment we will once again use the noisy two moons dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100000\n",
    "noisy_moons = sklearn.datasets.make_moons(n_samples=n_samples, noise=.1)\n",
    "noisy_moons[0][:, 0] -= np.mean(noisy_moons[0][:, 0])\n",
    "noisy_moons[0][:, 0] /= np.std(noisy_moons[0][:, 0])\n",
    "noisy_moons[0][:, 1] -= np.mean(noisy_moons[0][:, 1])\n",
    "noisy_moons[0][:, 1] /= np.std(noisy_moons[0][:, 1])\n",
    "plt.plot(noisy_moons[0][:, 0], noisy_moons[0][:, 1], '.', alpha=0.05);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_moons_points = torch.tensor(noisy_moons[0], dtype=torch.float32)\n",
    "noisy_moons_labels = torch.tensor(noisy_moons[1])\n",
    "moon_dataset = torch.utils.data.TensorDataset(noisy_moons_points, noisy_moons_labels)\n",
    "moon_data_loader = torch.utils.data.DataLoader(moon_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Create a normalizing flow model.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "The model should have 4 affine coupling layers.\n",
    "The neural network block should use a hidden dimension of 16.\n",
    "Use a standard normal distribution as the base distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ... # TODO: Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Train the normalizing flow model.<span style=\"float:right\"> (1 points)</span>**\n",
    "\n",
    "Hint: the negative log probability per sample of the data after training should be < 1.9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Plot the distribution of the trained model.<span style=\"float:right\"> (no points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_log_prob_and_samples(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Plot the distribution of the trained model on top of the data.<span style=\"float:right\"> (no points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(noisy_moons[0][:, 0], noisy_moons[0][:, 1], 'r.', alpha=0.05)\n",
    "plot_samples(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) If you look closely, there is a region connecting the two moons that has a non-zero probability according to the model, which is not in the training data. Can you explain why this happens?<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Note: Where these regions occur depends on the initialization of the model. These regions become smaller with more training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of transforming points from the base distribution to the target distribution, we can use the inverse transformation of `model.flow` to tranform points back from the target distribution.\n",
    "\n",
    "**(f) Transform the points in the dataset with this inverse tranformation, and plot them. <span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inverse_samples(model, data):\n",
    "    # TODO: implement\n",
    "\n",
    "plot_inverse_samples(model, noisy_moons_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) What distribution do you expect for the inversely transformed training data? And does the plot match your expectations? <span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect what the different layers in the model are doing by making a copy of the trained model that skips some of the coupling layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_model(model, num_coupling_layers):\n",
    "    \"\"\"Create a new model that uses only the first affine coupling layers.\"\"\"\n",
    "    layers = []\n",
    "    for layer in model.flow.layers:\n",
    "        if isinstance(layer, AffineCoupling):\n",
    "            if num_coupling_layers > 0:\n",
    "                num_coupling_layers -= 1\n",
    "                layers.append(layer)\n",
    "        else:\n",
    "            layers.append(layer)\n",
    "    return NormalizingFlow(model.base_distribution, *layers)\n",
    "\n",
    "def plot_model_steps(model, steps):\n",
    "    plt.figure(figsize=(3*steps, 3))\n",
    "    for i in range(steps):\n",
    "        plt.subplot(1, steps, i + 1)\n",
    "        plot_log_prob(part_model(model,i), colorbar=False)\n",
    "\n",
    "plot_model_steps(model, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) Train the model again, and compare the above plot between the two runs. Are the models similar?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.6 Discussion (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) How does the normalizing flow model differ from the GAN trained in assignment 9.7b?<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Compare to both the generator and discriminator plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) We have used a standard normal distribution as the base distribution. Another choice is a uniform distribution. What is a potential problem with using a uniform distribution?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) According to the manifold hypothisis, real data often lies on a low dimensional manifold in a high dimensional space. Can you use this idea in a normalizing flow model, by having a low dimensional latent space? Explain your answer.<span class=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end\n",
    "\n",
    "Well done! Please double check the instructions at the top before you submit your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This assignment has 29 points.*\n",
    "<span style=\"float:right;color:#aaa;font-size:10px;\"> Version 36dc7f6 / 2024-11-21</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}