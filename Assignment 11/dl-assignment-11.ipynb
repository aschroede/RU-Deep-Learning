{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning &mdash; Assignment 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eleventh assignment for the 2024 Deep Learning course (NWI-IMC070) of the Radboud University."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "**Names:** Andrew Schroeder and Fynn Gerding\n",
    "\n",
    "**Group:** 17\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "* Fill in your names and the name of your group.\n",
    "* Answer the questions and complete the code where necessary.\n",
    "* Keep your answers brief and to the point, one or two sentences is usually enough.\n",
    "* Re-run the whole notebook before you submit your work.\n",
    "* Save the notebook as a PDF and submit that in Brightspace together with the `.ipynb` notebook file.\n",
    "* The easiest way to make a PDF of your notebook is via File > Print Preview and then use your browser's print option to print to PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "In this assignment you will\n",
    "1. Build a variational autoencoder\n",
    "2. Extend the model to a conditional VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required software\n",
    "\n",
    "As before you will need these libraries:\n",
    "* `torch` and `torchvision` for PyTorch.\n",
    "\n",
    "All libraries can be installed with `pip install`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['png']\n",
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from IPython import display\n",
    "\n",
    "# Fix the seed to make the solutions more reproducible\n",
    "torch.manual_seed(1);\n",
    "\n",
    "# Use the GPU if available\n",
    "def detect_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "device = detect_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.1 MNIST dataset (no points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we will use the MNIST digit dataset. This dataset consists of 28×28 binary images and has 60000 training examples divided over 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Run the code below to load the MNIST dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:05<00:00, 1728906.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 303681.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:02<00:00, 738295.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 11326116.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'data'\n",
    "train_data = datasets.MNIST(data_dir, train=True,  download=True, transform=transforms.ToTensor())\n",
    "test_data  = datasets.MNIST(data_dir, train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "batch_size = 32\n",
    "data_loaders = {\n",
    "    'train': torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True),\n",
    "    'val':   torch.utils.data.DataLoader(test_data,  batch_size=batch_size),\n",
    "}\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 Variational Autoencoder (VAE) (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a Variational Autoencoder. This model consists of two networks: an encoder and a decoder.\n",
    "The encoder produces a distribution in the latent space, represented as the parameters of a normal distribution. The decoder takes the latent space representation and produces an output in the data space.\n",
    "\n",
    "**(a) Complete the implementation below.<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_size=2):\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        # Components of the encoder network\n",
    "        self.encoder_part1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1, stride=2), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1, stride=2), nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(7*7*64, 16), nn.ReLU()\n",
    "        )\n",
    "        self.encoder_mean   = nn.Linear(16, latent_size)\n",
    "        self.encoder_logvar = nn.Linear(16, latent_size)\n",
    "        \n",
    "        # Components of the decoder\n",
    "        self.decoder_part1_z = nn.Linear(latent_size, 7*7*64)\n",
    "        self.decoder_part2 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1, output_padding=1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, padding=1, output_padding=1, stride=2),\n",
    "            # TODO: Choose an appropriate activation function for the final layer.\n",
    "            nn.Sigmoid()\n",
    "\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder_part1(x)\n",
    "        return self.encoder_mean(h), self.encoder_logvar(h)\n",
    "\n",
    "    def sample_latent(self, mean_z, logvar_z):\n",
    "        eps = torch.randn_like(mean_z)\n",
    "        std_z = torch.exp(0.5 * logvar_z)\n",
    "        # TODO: turn the sample ε from N(0,1) into a sample from N(μ,σ)\n",
    "        return eps*std_z + mean_z\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = self.decoder_part1_z(z)\n",
    "        h = torch.reshape(h, (-1,64,7,7)) # Unflatten\n",
    "        return self.decoder_part2(h)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_z, logvar_z = self.encode(x)\n",
    "        z = self.sample_latent(mean_z, logvar_z)\n",
    "        return self.decode(z), mean_z, logvar_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are some unit tests for the VAE\n",
    "samples = VAE().sample_latent(torch.ones(10000), torch.ones(10000))\n",
    "assert F.mse_loss(torch.mean(samples), torch.tensor(1.)) < 1e-3, \\\n",
    "      'sample_latent should produce values with the specified mean'\n",
    "assert F.mse_loss(torch.log(torch.var(samples)), torch.tensor(1.)) < 1e-3, \\\n",
    "      'sample_latent should produce values with the specified log variance'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The encoder produces two outputs that together give the parameters of a normal distribution: mean and logvar, so $\\mu$ and $\\log(\\sigma^2)$. The latter might seem strange, but there is a good reason for doing it this way. \n",
    "\n",
    "**(b) What can go wrong if the encoder network directly outputs mean and standard deviation (μ,σ)?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 Loss function (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss for a variational autoencoder consists of two parts:\n",
    "1. The reconstruction loss, which is the log likelihood of the data,\n",
    "$L_\\text{R} = \\log P(x\\mid z)$.\n",
    "2. The Kulback-Leibler divergence from the encoder output to the target distribution,\n",
    "$L_\\text{KL}= KL(Q(z)\\| P(z))$.\n",
    "\n",
    "In our case the data is binary, so we can use [binary cross entropy](https://pytorch.org/docs/stable/generated/torch.nn.functional.binary_cross_entropy.html) for the reconstruction loss.\n",
    "\n",
    "The derivation of the KL loss term can be found in appendix B of the VAE paper; [Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014](https://arxiv.org/pdf/1312.6114.pdf). Be careful:\n",
    "* the paper defines $-D_{KL}$, not $D_{KL}$\n",
    "* the sum is only over the latent space. In our code this corresponds to `axis=1`. Use the mean over the samples in the batch (`axis=0`).\n",
    "\n",
    "**(a) Implement the KL loss term below.<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_loss(recon_x, x):\n",
    "    # The reconstruction loss is binary cross entropy\n",
    "    # Note: we normalize the loss wrt. the batch size (len(x)), but not the size of the image\n",
    "    return F.binary_cross_entropy(recon_x, x, reduction='sum') / len(x)\n",
    "\n",
    "def kl_loss(mean_z, logvar_z):\n",
    "    # The KL divergence between a standard normal distribution and\n",
    "    #  a normal distribution with given mean and log-variance.\n",
    "    # TODO: your code here\n",
    "    print(f\"Shape of mean_z: {mean_z.shape}\")\n",
    "    print(f\"Shape of logvar_z: {logvar_z.shape}\")\n",
    "    logvar_z = torch.mean(logvar_z, dim=0)\n",
    "    mean_z = torch.mean(mean_z, dim=0)\n",
    "    kl_loss = -0.5 * torch.sum(1 +  logvar_z - mean_z.pow(2) - logvar_z.exp())\n",
    "    return kl_loss\n",
    "\n",
    "def loss_function(recon_x, x, mean_z, logvar_z):\n",
    "    l_recon = reconstruction_loss(recon_x, x)\n",
    "    l_kl    = kl_loss(mean_z, logvar_z)\n",
    "    return l_recon + l_kl, l_recon, l_kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of mean_z: torch.Size([1, 1])\n",
      "Shape of logvar_z: torch.Size([1, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mean(): could not infer output dtype. Input dtype must be either a floating point or complex dtype. Got: Long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Here are some unit tests for the loss function\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mkl_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, \\\n\u001b[1;32m      3\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKL loss should be 0 for μ=0, σ=1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m kl_loss(torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m0\u001b[39m]]), torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \\\n\u001b[1;32m      5\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKL loss should be > 0 for μ=0, σ<1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m kl_loss(torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m0\u001b[39m]]), torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1\u001b[39m]])) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \\\n\u001b[1;32m      7\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKL loss should be > 0 for μ=0, σ>1\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[13], line 12\u001b[0m, in \u001b[0;36mkl_loss\u001b[0;34m(mean_z, logvar_z)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of mean_z: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_z\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of logvar_z: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlogvar_z\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m logvar_z \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogvar_z\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m mean_z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(mean_z, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     14\u001b[0m kl_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m  logvar_z \u001b[38;5;241m-\u001b[39m mean_z\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m-\u001b[39m logvar_z\u001b[38;5;241m.\u001b[39mexp())\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mean(): could not infer output dtype. Input dtype must be either a floating point or complex dtype. Got: Long"
     ]
    }
   ],
   "source": [
    "# Here are some unit tests for the loss function\n",
    "assert kl_loss(torch.tensor([[0]]), torch.tensor([[0]])) == 0, \\\n",
    "      'KL loss should be 0 for μ=0, σ=1'\n",
    "assert kl_loss(torch.tensor([[0]]), torch.tensor([[-1]])) > 0, \\\n",
    "      'KL loss should be > 0 for μ=0, σ<1'\n",
    "assert kl_loss(torch.tensor([[0]]), torch.tensor([[1]])) > 0, \\\n",
    "      'KL loss should be > 0 for μ=0, σ>1'\n",
    "assert kl_loss(torch.tensor([[1]]), torch.tensor([[0]])) > 0, \\\n",
    "      'KL loss should be > 0 for μ!=0, σ=1'\n",
    "assert kl_loss(torch.tensor([[0]]), torch.tensor([[1]])) == \\\n",
    "       kl_loss(torch.tensor([[0,0]]), torch.tensor([[1,1]])) / 2, \\\n",
    "      'Take the sum over the latent dimensions'\n",
    "assert kl_loss(torch.tensor([[0,0,1]]), torch.tensor([[0,1,-0.5]])) == \\\n",
    "       kl_loss(torch.tensor([[0,0,1],[0,0,1]]), torch.tensor([[0,1,-0.5],[0,1,-0.5]])), \\\n",
    "      'Take the mean over the items in the batch or normalize wrt. batch size (see also reconstruction_loss)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4 Training our VAE (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Complete the training loop below<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loaders, num_epochs=10, lr=1e-3):\n",
    "    train_loader = data_loaders['train']\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    plotter = Plotter(xlabel='epoch', xlim=[1, num_epochs], figsize=(10, 5),\n",
    "                      legend=['train loss', 'train recon. loss', 'train KL loss',\n",
    "                              'val loss', 'val recon. loss', 'val KL loss'])\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = Metrics(3)\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # TODO: compute the outputs and loss\n",
    "            # TODO: backpropagate and apply optimizer\n",
    "            # Track our progress\n",
    "            metric.add(len(x), loss.item(), loss_recon.item(), loss_kl.item())\n",
    "        # Compute validation loss\n",
    "        val_loss, val_loss_recon, val_loss_kl = evaluate(model, data_loaders['val'], device=device)\n",
    "        # Plot\n",
    "        train_loss, train_loss_recon, train_loss_kl = metric.mean()\n",
    "        plotter.add(epoch + 1,\n",
    "                    (train_loss, train_loss_recon, train_loss_kl,\n",
    "                     val_loss, val_loss_recon, val_loss_kl))\n",
    "    print(f'training loss {train_loss:.3f}, val loss {val_loss:.3f}')\n",
    "    print(f'training reconstruction loss {train_loss_recon:.3f}, val reconstruction loss {val_loss_recon:.3f}')\n",
    "    print(f'training KL loss {train_loss_kl:.3f}, val KL loss {val_loss_kl:.3f}')\n",
    "\n",
    "def evaluate(model, test_loader, device=device):\n",
    "    \"\"\"\n",
    "    Evaluate a model on the given dataset.\n",
    "    Return total loss, reconstruction loss, KL loss\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        metrics = Metrics(3)\n",
    "        for x, _ in test_loader:\n",
    "            x = x.to(device)\n",
    "            # TODO: compute the outputs and loss\n",
    "            metrics.add(len(x), loss.item(), loss_recon.item(), loss_kl.item())\n",
    "        return metrics.mean()\n",
    "\n",
    "class Metrics:\n",
    "    \"\"\"Accumulate mean values of one or more metrics.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.count = 0\n",
    "        self.sum = (0,) * n\n",
    "    def add(self, count, *values):\n",
    "        self.count += count\n",
    "        self.sum = tuple(s + count * v for s,v in zip(self.sum,values))\n",
    "    def mean(self):\n",
    "        return tuple(s / self.count for s in self.sum)\n",
    "\n",
    "class Plotter:\n",
    "    \"\"\"For plotting data in animation.\"\"\"\n",
    "    # Based on d2l.Animator\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 nrows=1, ncols=1,\n",
    "                 figsize=(10, 5)):\n",
    "        # Incrementally plot multiple lines\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes,]\n",
    "        # Use a function to capture arguments\n",
    "        def config_axes():\n",
    "            axis = self.axes[0]\n",
    "            axis.set_xlabel(xlabel), axis.set_ylabel(ylabel)\n",
    "            axis.set_xscale(xscale), axis.set_yscale(yscale)\n",
    "            axis.set_xlim(xlim),     axis.set_ylim(ylim)\n",
    "            if legend:\n",
    "                axis.legend(legend)\n",
    "            axis.grid()\n",
    "        self.config_axes = config_axes\n",
    "        self.X, self.Y = None, None\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # Add multiple data points into the figure\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y in zip(self.X, self.Y):\n",
    "            self.axes[0].plot(x, y)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Train the model.<span style=\"float:right\"> (no points)</span>**\n",
    "\n",
    "Hint: the training and test loss should both be around 160."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "train(model, data_loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) If you were to increase the number of latent dimensions, how would that affect the reconstruction loss and the KL loss terms?<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Give an answer based on your theoretical understanding of VAEs, you don't have to actually do the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.5 Visualizing the latent space (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the function below to visualize the 2D latent space, by running the decoder on $z$ values sampled at regular intervals.\n",
    "\n",
    "**(a) Complete the code below and run it to plot the latent space.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent(model, bounds=3, n=31):\n",
    "    # display a n*n 2D manifold of digits\n",
    "    digit_size = 28\n",
    "    figsize = 10\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates corresponding to the 2D plot\n",
    "    # of digit classes in the latent space\n",
    "    grid_x = np.linspace(-bounds, bounds, n)\n",
    "    grid_y = np.linspace(-bounds, bounds, n)[::-1]\n",
    "\n",
    "    for i, yi in enumerate(grid_y):\n",
    "        for j, xi in enumerate(grid_x):\n",
    "            # TODO: run the decoder on z = [xi,yi].\n",
    "            x_decoded = ...\n",
    "            figure[\n",
    "                i * digit_size : (i + 1) * digit_size,\n",
    "                j * digit_size : (j + 1) * digit_size,\n",
    "            ] = x_decoded.detach().cpu().numpy()\n",
    "\n",
    "    plt.figure(figsize=(figsize, figsize))\n",
    "    start_range = digit_size // 2\n",
    "    end_range = n * digit_size + start_range\n",
    "    pixel_range = np.arange(start_range, end_range, digit_size)\n",
    "    sample_range_x = np.round(grid_x, 1)\n",
    "    sample_range_y = np.round(grid_y, 1)\n",
    "    plt.xticks(pixel_range, sample_range_x)\n",
    "    plt.yticks(pixel_range, sample_range_y)\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.imshow(figure, cmap=\"Greys_r\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_latent(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Would it be possible to use this latent representation to make a digit classifier? Explain your answer.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) If you retrain the model, would you expect the latent space to look exactly the same. If not, what differences can you expect?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way of visualizing the latent space is by making a scatter plot of the training data in the latent space.\n",
    "\n",
    "**(d) Complete and run the code below to make a scatterplot of the training data.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterplot_latent(model, data_loader, bounds=3):\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    zs, ys = [], []\n",
    "    for x, y in itertools.islice(data_loader, 100):\n",
    "        # TODO: compute mean z\n",
    "        z_mean = ...\n",
    "        zs.append(z_mean.detach().cpu())\n",
    "        ys.append(y)\n",
    "    zs = torch.cat(zs).numpy()\n",
    "    ys = torch.cat(ys).numpy()\n",
    "    \n",
    "    cmap = plt.get_cmap('jet', 10)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.scatter(zs[:, 0], zs[:, 1], c=ys, cmap=cmap, alpha=0.8, vmin=-0.5, vmax=9.5)\n",
    "    plt.colorbar(ticks=np.arange(0, 10))\n",
    "    plt.xlabel(\"z[0]\")\n",
    "    plt.ylabel(\"z[1]\")\n",
    "    plt.xlim(-bounds, bounds)\n",
    "    plt.ylim(-bounds, bounds)\n",
    "    plt.show()\n",
    "\n",
    "scatterplot_latent(model, data_loaders['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Compare this figure to the one from `plot_latent`. How are the plots related?<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Note: Don't just answer \"both visualize the latent space\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Compared to the figure from `plot_latent`, what information about the VAE is shown in this figure but not in the previous one?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) What distribution should we expect the points in the latent space to follow, based on the KL divergence term in the loss function?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) Look at the distribution of the data in the latent space. Does the plot match the answer to the previous question? If not, why?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.6 Conditional Variational Autoencoder (10 points)\n",
    "\n",
    "An extension of variational autoencoders uses labels to *condition* the encoder and decoder models.\n",
    "In this *conditional VAE*, the decoder becomes $P(x|z,y)$ and the encoder $Q(z|x,y)$.\n",
    "In practice, this means that the label $y$ is given as an extra input to the both the encoder and the decoder.\n",
    "\n",
    "For details see the paper [Semi-Supervised Learning with Deep Generative Models; Kingma, Rezende, Mohamed, Welling; 2014](https://arxiv.org/pdf/1406.5298.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the labels in the decoder, we can concatenate the label with the latent vector. Or equivalently, we can use separate weights for $z$ and $y$ in the first layer, so that layer computes $W_z \\cdot z + W_y \\cdot y + b$.\n",
    "\n",
    "Similarly for the encoder, except there we will still use a convolutional layer for $x$, combined with a fully connected layer for $y$.\n",
    "\n",
    "**(a) Complete the implementation of the conditional VAE below.<span style=\"float:right\"> (3 points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalVAE(nn.Module):\n",
    "    def __init__(self, latent_size=2, num_classes=10):\n",
    "        super(ConditionalVAE, self).__init__()\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        # Components of the encoder network\n",
    "        # TODO: split the first layer from the previous encoder network into a separate variable,\n",
    "        #       and add a layer to use with the y input\n",
    "        self.encoder_part1_x = ...\n",
    "        self.encoder_part1_y = ...\n",
    "        self.encoder_part2 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1, stride=2), nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(7*7*64, 16), nn.ReLU()\n",
    "        )\n",
    "        self.encoder_mean   = nn.Linear(16, latent_size)\n",
    "        self.encoder_logvar = nn.Linear(16, latent_size)\n",
    "\n",
    "        # Components of the decoder network\n",
    "        self.decoder_part1_z = nn.Linear(latent_size, 7*7*64)\n",
    "        # TODO: add layer to use with the y input\n",
    "        self.decoder_part2 = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=3, padding=1, output_padding=1, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=3, padding=1, output_padding=1, stride=2),\n",
    "            # TODO: see VAE\n",
    "        )\n",
    "\n",
    "    def encode(self, x, y):\n",
    "        h = self.encoder_part1_x(x) + self.encoder_part1_y(y).reshape(-1,32,14,14)\n",
    "        h = self.encoder_part2(h)\n",
    "        return self.encoder_mean(h), self.encoder_logvar(h)\n",
    "\n",
    "    def sample_latent(self, mean_z, logvar_z):\n",
    "        eps = torch.randn_like(mean_z)\n",
    "        std_z = torch.exp(0.5 * logvar_z)\n",
    "        # TODO: see VAE\n",
    "\n",
    "    def decode(self, z, y):\n",
    "        # TODO: use a first layer that combines z and y\n",
    "        h = ...\n",
    "        h = torch.reshape(h, (-1,64,7,7))\n",
    "        return self.decoder_part2(h)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        mean_z, logvar_z = self.encode(x, y)\n",
    "        z = self.sample_latent(mean_z, logvar_z)\n",
    "        return self.decode(z, y), mean_z, logvar_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Copy the training code from section 11.4, and modify it for a conditional VAE.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: To train the conditional VAE we need to use one-hot encoding of the labels. You can use the following code for that:\n",
    "\n",
    "    y = F.one_hot(y,10).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cvae(model, data_loaders, num_epochs=10, lr=1e-3, device=device):\n",
    "    # TODO: your code here\n",
    "\n",
    "def evaluate_cvae(model, test_loader, device=device):\n",
    "    \"\"\"\n",
    "    Evaluate a conditional model on the given dataset.\n",
    "    Return total loss, reconstruction loss, KL loss\n",
    "    \"\"\"\n",
    "    # TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Train a conditional VAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae_model = ConditionalVAE().to(device)\n",
    "train_cvae(cvae_model, data_loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Adapt the `plot_latent` function from section 11.5 for conditional VAEs, and use your function to visualize the latent space for the classes `4` and `8`.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_cvae(...):\n",
    "    # TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) What do the latent dimensions represent? Is this the same for all labels?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Adapt `scatterplot_latent` to show the distribution in the latent space.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterplot_latent_cvae(cvae_model, data_loader, bounds=3):\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "\n",
    "scatterplot_latent_cvae(cvae_model, data_loaders['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) How is this distribution in the latent space different from the distribution of the VAE? Compare to your answer to that for question 11.5 g and h. What is the cause of these differences?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) Would it be possible to classify digits based on the latent representation of the conditional VAE? Explain your answer.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(i) Describe how you could use a conditional VAE to change the label or content of an image, while keeping the style as similar as possible.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.7 Discussion (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Is the conditional VAE a strict improvement over the normal VAE in all cases?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Compare the latent representation vector $z$ in the VAE with the input for the generator in a GAN. They are both small vectors, and they are both often called $z$. In what way are they the same, and in what way are the different?<span class=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) A VAE and a normalizing flow mode both allow you to transform between the data space and a latent space, and in both models the latent space has a standard normal distribution. Yet there are some differences. Give 2 differences between VAE and NF models.<span class=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end\n",
    "\n",
    "Well done! Please double check the instructions at the top before you submit your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This assignment has 29 points.*\n",
    "<span style=\"float:right;color:#aaa;font-size:10px;\"> Version b117570 / 2024-11-25</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
