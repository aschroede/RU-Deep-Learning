{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning &mdash; Assignment 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twelfth assignment for the 2024 Deep Learning course (NWI-IMC070) of the Radboud University."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "**Names:**\n",
    "\n",
    "**Group:**\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "* Fill in your names and the name of your group.\n",
    "* Answer the questions and complete the code where necessary.\n",
    "* Keep your answers brief, one or two sentences is usually enough.\n",
    "* Re-run the whole notebook before you submit your work.\n",
    "* Save the notebook as a PDF and submit that in Brightspace together with the `.ipynb` notebook file.\n",
    "* The easiest way to make a PDF of your notebook is via File > Print Preview and then use your browser's print option to print to PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "In this assignment you will\n",
    "1. Build a diffusion model\n",
    "2. Extend the model to a class-conditional version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required software\n",
    "\n",
    "As before you will need these libraries:\n",
    "* `torch` and `torchvision` for PyTorch,\n",
    "\n",
    "All libraries can be installed with `pip install`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from IPython import display\n",
    "\n",
    "# fix the seed, so outputs are exactly reproducible\n",
    "torch.manual_seed(12345);\n",
    "\n",
    "# Use the GPU if available\n",
    "def detect_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "device = detect_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1 MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we will once again be using the MNIST digit dataset. This dataset consists of 28\u00d728 binary images and has 60000 training examples divided over 10 classes. We split this into 55000 images for training and 5000 images for validation.\n",
    "\n",
    "As preprocessing, we pad the images to 32x32 pixels and scale the intensities to [-1, +1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Run the code below to load the MNIST dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Pad(2),               # pad to 32x32 pixels\n",
    "    transforms.Normalize(0.5, 0.5),  # normalize to [-1, +1]\n",
    "])\n",
    "train_val_data = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "# Split into train and validation set\n",
    "train_data, val_data = torch.utils.data.random_split(train_val_data, [55000, 5000])\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=1000)\n",
    "data_loaders = {\n",
    "    'train': train_loader,\n",
    "    'val':   val_loader,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2 Training images (4 points)\n",
    "\n",
    "We will implement a model from the paper [Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239.pdf) by Ho et al., 2020.\n",
    "\n",
    "We reuse some parameter settings from the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_steps = 1000\n",
    "beta = torch.linspace(1e-4, 0.02, diffusion_steps)\n",
    "alpha = 1.0 - beta\n",
    "alpha_bar = torch.cumprod(alpha, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these settings, we can generate a sequence of noisy images with\n",
    "\\begin{aligned}\n",
    "\\mathbf{x}_t \n",
    "&= \\sqrt{\\alpha_t}\\mathbf{x}_{t-1} + \\sqrt{1 - \\alpha_t}\\boldsymbol{\\epsilon}_{t-1} & \\text{ where } \\boldsymbol{\\epsilon}_{t-1}, \\boldsymbol{\\epsilon}_{t-2}, \\dots \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}).\n",
    "\\end{aligned}\n",
    "\n",
    "There is a closed-form solution to compute $x_t$ directly from $x_0$ (see the paper or [this blog](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/#forward-diffusion-process)):\n",
    "\\begin{aligned}\n",
    "\\mathbf{x}_t &= \\sqrt{\\bar{\\alpha}_t}\\mathbf{x}_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\boldsymbol{\\epsilon} & \\text{ where } \\boldsymbol{\\epsilon} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}).\n",
    "\\end{aligned}\n",
    "\n",
    "**(a) Implement this closed-form solution in the code below to generate random images.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "*Expected output: you should see some images with recognizable shapes and some images with noise.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample some original images\n",
    "x_0, y = next(iter(train_loader))\n",
    "\n",
    "plt.figure(figsize=(19, 21))\n",
    "for i, t in enumerate(range(0, diffusion_steps, 100)):\n",
    "    # TODO generate random noise\n",
    "    noise = torch.zeros_like(x_0)\n",
    "    \n",
    "    # TODO add noise to the original image to compute x_t\n",
    "    x_t = x_0\n",
    "\n",
    "    plt.subplot(3, diffusion_steps // 100, i + 1)\n",
    "    plt.imshow(x_t[0, 0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(f't={t}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Describe how we should interpret these images, and how they can be used to train the diffusion model.<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we will need a minibatch with multiple images and multiple time steps.\n",
    "\n",
    "**(c) Complete the function below to add noise to a minibatch of images.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noisy_samples(x_0, beta):\n",
    "    '''\n",
    "    Create noisy samples for the minibatch x_0.\n",
    "    Return the noisy image, the noise, and the time for each sample.\n",
    "    '''\n",
    "    alpha = 1.0 - beta\n",
    "    alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "\n",
    "    # sample a random time t for each sample in the minibatch\n",
    "    t = torch.randint(beta.shape[0], size=(x_0.shape[0],), device=x_0.device)\n",
    "    # TODO generate noise\n",
    "    noise = ...\n",
    "    # TODO add the noise to each sample\n",
    "    x_t = ...\n",
    "    return x_t, noise, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Try out your new function by generating a few noisy samples.**\n",
    "\n",
    "*Expected output: you should see the samples in the minibatch with different levels of noise, depending on the time $t$ for each sample.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0, y = next(iter(train_loader))\n",
    "\n",
    "x_t, noise, sampled_t = generate_noisy_samples(x_0, beta)\n",
    "assert x_t.shape == x_0.shape\n",
    "assert noise.shape == x_0.shape\n",
    "assert sampled_t.shape == (x_0.shape[0],)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(32):\n",
    "    plt.subplot(4, 8, i + 1)\n",
    "    plt.imshow(x_t[i, 0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title(f't={sampled_t[i]}')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.3 Helper functions\n",
    "\n",
    "We will use some predefined components to construct our model, based on [an existing implementation](https://github.com/dome272/Diffusion-Models-pytorch) on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, h_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.h_size = h_size\n",
    "        self.mha = nn.MultiheadAttention(h_size, 4, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([h_size])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([h_size]),\n",
    "            nn.Linear(h_size, h_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h_size, h_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_ln = self.ln(x)\n",
    "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
    "        attention_value = attention_value + x\n",
    "        attention_value = self.ff_self(attention_value) + attention_value\n",
    "        return attention_value\n",
    "\n",
    "class SAWrapper(nn.Module):\n",
    "    def __init__(self, h_size, num_s):\n",
    "        super(SAWrapper, self).__init__()\n",
    "        self.sa = nn.Sequential(*[SelfAttention(h_size) for _ in range(1)])\n",
    "        self.num_s = num_s\n",
    "        self.h_size = h_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.h_size, self.num_s * self.num_s).swapaxes(1, 2)\n",
    "        x = self.sa(x)\n",
    "        x = x.swapaxes(2, 1).view(-1, self.h_size, self.num_s, self.num_s)\n",
    "        return x\n",
    "\n",
    "# U-Net code adapted from: https://github.com/milesial/Pytorch-UNet\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, mid_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            return F.gelu(x + self.double_conv(x))\n",
    "        else:\n",
    "            return self.double_conv(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, in_channels, residual=True)\n",
    "            self.conv2 = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(\n",
    "                in_channels, in_channels // 2, kernel_size=2, stride=2\n",
    "            )\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4 Diffusion model (5 points)\n",
    "\n",
    "Similar to Ho et al. and to several online implementations, we will use a U-Net with self-attention and positional embedding as our diffusion model.\n",
    "\n",
    "**(a) Familiarize yourself with the architecture of this U-Net.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, c_in=1, c_out=1, width=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.width = width\n",
    "        bilinear = True\n",
    "        self.inc = DoubleConv(c_in, self.width)\n",
    "        self.down1 = Down(self.width, self.width*2)\n",
    "        self.down2 = Down(self.width*2, self.width*4)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down3 = Down(self.width*4, self.width*8 // factor)\n",
    "        self.up1 = Up(self.width*8, self.width*4 // factor, bilinear)\n",
    "        self.up2 = Up(self.width*4, self.width*2 // factor, bilinear)\n",
    "        self.up3 = Up(self.width*2, self.width, bilinear)\n",
    "        self.outc = OutConv(self.width, c_out)\n",
    "        self.sa1 = SAWrapper(self.width*4, 8)\n",
    "        self.sa2 = SAWrapper(self.width*4, 4)\n",
    "        self.sa3 = SAWrapper(self.width*2, 8)\n",
    "\n",
    "    def pos_encoding(self, t, channels, embed_size):\n",
    "        inv_freq = 1.0 / (\n",
    "            10000\n",
    "            ** (torch.arange(0, channels, 2, device=t.device).float() / channels)\n",
    "        )\n",
    "        pos_enc_a = torch.sin(t[:, None].repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t[:, None].repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc.view(-1, channels, 1, 1).repeat(1, 1, embed_size, embed_size)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        Model is U-Net with added positional encodings and self-attention layers.\n",
    "        \"\"\"\n",
    "        device = x.device\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1) + self.pos_encoding(t, self.width*2, 16)\n",
    "        x3 = self.down2(x2) + self.pos_encoding(t, self.width*4, 8)\n",
    "        x3 = self.sa1(x3)\n",
    "        x4 = self.down3(x3) + self.pos_encoding(t, self.width*4, 4)\n",
    "        x4 = self.sa2(x4)\n",
    "        x = self.up1(x4, x3) + self.pos_encoding(t, self.width*2, 8)\n",
    "        x = self.sa3(x)\n",
    "        x = self.up2(x, x2) + self.pos_encoding(t, self.width, 16)\n",
    "        x = self.up3(x, x1) + self.pos_encoding(t, self.width, 32)\n",
    "        output = self.outc(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) What does the positional encoding encode? Why would this be useful?<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here on what it encodes.\n",
    "\n",
    "TODO Your answer here on why it is useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Describe how this model will be used. What do the inputs and outputs represent?<span style=\"float:right\"> (3 points)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Used for:* TODO Your answer here.\n",
    "\n",
    "*Inputs:* TODO Your answer here.\n",
    "\n",
    "*Outputs:* TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.5 Training the model (7 points)\n",
    "\n",
    "We will train our diffusion model using Algorithm 1 from the paper [Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239.pdf) by Ho et al., 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) The algorithm uses $\\mathbf{x}_0$. How do you obtain $\\mathbf{x}_0$ during training?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Which two values are compared in the loss on line 5 of the algorithm?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Implement this procedure in the training loop below.<span style=\"float:right\"> (3 points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loaders, beta, num_epochs=10, lr=1e-3, device=device):\n",
    "    \"\"\"Train a diffusion model\"\"\"\n",
    "    train_loader = data_loaders['train']\n",
    "    validation_loader = data_loaders['val']\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    plotter = Plotter(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                      legend=['train loss', 'validation loss'])\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        metrics = Metrics(1)\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # TODO compute the loss for minibatch x\n",
    "            # hint: we already have a function to generate noisy images\n",
    "            loss = ...\n",
    "            \n",
    "            # Optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track our progress\n",
    "            metrics.add(len(x), loss.item())\n",
    "        train_loss = metrics.mean()[0]\n",
    "            \n",
    "        # Compute validation loss\n",
    "        validation_loss = evaluate(model, validation_loader, beta, device=device)\n",
    "        \n",
    "        # Plot\n",
    "        plotter.add(epoch + 1, (train_loss, validation_loss))\n",
    "    \n",
    "    train_time = time.time() - start_time\n",
    "    print(f'training loss {train_loss:.3g}, validation loss {validation_loss:.3g}')\n",
    "    print(f'{metrics.count * num_epochs / train_time:.1f} examples/sec '\n",
    "          f'on {str(device)}')\n",
    "\n",
    "def evaluate(model, test_loader, beta, device=device):\n",
    "    \"\"\"Evaluate a diffusion model by computing the loss.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        metrics = Metrics(1)\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            # TODO compute the loss for minibatch x\n",
    "            loss = ...\n",
    "            metrics.add(len(x), loss.item())\n",
    "    return metrics.mean()[0]\n",
    "\n",
    "class Metrics:\n",
    "    \"\"\"Accumulate mean values of one or more metrics.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.count = 0\n",
    "        self.sum = (0,) * n\n",
    "    def add(self, count, *values):\n",
    "        self.count += count\n",
    "        self.sum = tuple(s + count * v for s,v in zip(self.sum,values))\n",
    "    def mean(self):\n",
    "        return tuple(s / self.count for s in self.sum)\n",
    "\n",
    "class Plotter:\n",
    "    \"\"\"For plotting data in animation.\"\"\"\n",
    "    # Based on d2l.Animator\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        # Incrementally plot multiple lines\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes,]\n",
    "        # Use a function to capture arguments\n",
    "        def config_axes():\n",
    "            axis = self.axes[0]\n",
    "            axis.set_xlabel(xlabel), axis.set_ylabel(ylabel)\n",
    "            axis.set_xscale(xscale), axis.set_yscale(yscale)\n",
    "            axis.set_xlim(xlim),     axis.set_ylim(ylim)\n",
    "            if legend:\n",
    "                axis.legend(legend)\n",
    "            axis.grid()\n",
    "        self.config_axes = config_axes\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # Add multiple data points into the figure\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) How does the training time depend on the number of diffusion steps $T$?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Train the model.**\n",
    "\n",
    "*Expected output: in our implementation, the training loss started at around 0.1 and went down quickly to 0.03 and lower.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet().to(device)\n",
    "train(model, data_loaders, beta, num_epochs=20, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Has the training converged? Do you think we should train longer?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.6 Sampling from the model (9 points)\n",
    "\n",
    "Once the model is trained, we can sample from it using Algorithm 2 from paper [Denoising Diffusion Probabilistic Models](https://arxiv.org/pdf/2006.11239.pdf):\n",
    "\n",
    "**Algorithm 2**:  \n",
    "1: $\\mathbf{x}_T \\sim \\mathcal{N}(\\mathbf 0, \\mathbf I)$  \n",
    "2: for $t = T, \\dots, 1$ do  \n",
    "3: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\mathbf{z} \\sim \\mathcal{N}(\\mathbf 0, \\mathbf I)$ if $t > 1$, else $\\mathbf{z} = \\mathbf{0}$  \n",
    "4: &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\mathbf{x}_{t-1} = \\frac{1}{\\sqrt{a_t}} \\left( \\mathbf{x}_t - \\frac{1-a_t}{\\sqrt{1-\\bar{a}_t}} \\mathbf{\\epsilon}_\\theta \\left(\\mathbf{x}_t, t\\right)\\right) + \\mathbf{\\sigma}_t \\mathbf{z}$  \n",
    "5: end for  \n",
    "6: return $\\mathbf{x}_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) In step 3 of algorithm 2, $\\mathbf{z}$ is set to $\\mathbf{0}$ some times. What is the effect of this?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) In step 4 of algorithm 2, $\\mathbf{x}_{t-1}$ is computed based on three ingredients: $\\mathbf{x}_t$, $\\mathbf{\\epsilon}_\\theta \\left(\\mathbf{x}_t, t\\right)$, and $\\mathbf{z}$. What do these represent? <span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $\\mathbf{x}_t$: TODO Your answer here.\n",
    "* $\\mathbf{\\epsilon}_\\theta \\left(\\mathbf{x}_t, t\\right)$: TODO Your answer here.\n",
    "* $\\mathbf{z}$: TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) How does the sampling time depend on the number of diffusion steps $T$?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Complete the code below to sample a minibatch from the model.<span style=\"float:right\"> (2 points)</span>**\n",
    "\n",
    "* Use the equations in Algorithm 2.\n",
    "* Use $\\sigma_t = \\sqrt{\\beta_t}$, as suggested in the paper.\n",
    "* Keep in mind that Algorithm 2 uses $t=1$ as the first time step, whereas we use $t=0$.\n",
    "\n",
    "*Expected output: after training, your model should generate fairly realistic, clean images when given random inputs.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model(x, model, beta, device=device):\n",
    "    # keep track of x at different time steps\n",
    "    x_hist = []\n",
    "    alpha = 1.0 - beta \n",
    "    alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "    with torch.no_grad():\n",
    "        # loop over all time steps in reverse order\n",
    "        for i in reversed(range(0, beta.shape[0])):\n",
    "            # copy the time step for each sample in the minibatch\n",
    "            t = (torch.ones(x.shape[0]) * i).long().to(device)\n",
    "            # TODO compute the next value of x\n",
    "            x = x\n",
    "            if i % 100 == 0:\n",
    "                x_hist.append(x.detach().cpu().numpy())\n",
    "    return x, x_hist\n",
    "\n",
    "def plot_x_hist(x_hist):\n",
    "    # plot the generated images\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(len(x_hist)):\n",
    "        for j in range(10):\n",
    "            plt.subplot(10, 10, j * 10 + i + 1)\n",
    "            plt.imshow(x_hist[i][j, 0], cmap='gray')\n",
    "            plt.axis('off')\n",
    "\n",
    "\n",
    "# TODO initialize x with the right values\n",
    "# shape: [10, 1, 32, 32]\n",
    "x = torch.zeros_like(x_0[:10]).to(device)\n",
    "\n",
    "x, x_hist = sample_from_model(x, model, beta)\n",
    "plot_x_hist(x_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Explain the X and Y axes of this figure.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) In a variational autoencoder or a GAN, the output is determined by the latent representation. How does that work for this diffusion model?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) Look at the generated intermediate samples over time in question (d). Do we need all of the steps? Why/why not?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.7 Experiments (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed initialization\n",
    "\n",
    "How does the end result depend on the initialization? We will generate multiple images from the same initial noise to find out.\n",
    "\n",
    "**(a) Complete and run the code below.</span>**\n",
    "\n",
    "*Expected output: the model should still produce recognizable shapes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO initialize x=x_T with the right values for one image\n",
    "# (see earlier question)\n",
    "# shape: [1, 1, 32, 32]\n",
    "x = torch.zeros_like(x_0[:1]).to(device) # TODO: change\n",
    "# repeat this to generate 10 images from the same initialization\n",
    "x = x.repeat(10, 1, 1, 1)\n",
    "\n",
    "x, x_hist = sample_from_model(x, model, beta)\n",
    "plot_x_hist(x_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Does the model always produce the same output from the same initial input? Why, or why not?<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No randomness between time steps\n",
    "\n",
    "To check the influence of noise during sampling, we can remove the term $\\sigma_t \\mathbf{z}$ from Algorithm 2.\n",
    "\n",
    "**(c) Create a new function `deterministic_sample_from_model`, based on `sample_from_model`, that does not include this term.<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deterministic_sample_from_model(x, model, beta, device=device):\n",
    "    # TODO copy sample_from_model and remove the random term sigma * z\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Generate some samples using the new function.**\n",
    "\n",
    "*Expected output: you should get a different result than before.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO initialize x with the right values\n",
    "# (see earlier questions)\n",
    "# shape: [10, 1, 32, 32]\n",
    "x = torch.zeros_like(x_0[:10]).to(device)\n",
    "\n",
    "x, x_hist = deterministic_sample_from_model(x, model, beta)\n",
    "plot_x_hist(x_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) What can you conclude from these results? Is the random noise during sampling important?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.8 Making the model conditional (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the conditional VAE in the previous assignment, we can make the diffusion model conditional by including class labels. This allows us to generate samples from a specific digit.\n",
    "\n",
    "We will include the class information alongside the existing positional encoding, using a [`torch.nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) layer to map the 10 digits to a higher-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Study the implemenation of UNetConditional to see how this works.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetConditional(nn.Module):\n",
    "    def __init__(self, c_in=1, c_out=1, n_classes=10, width=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.width = width\n",
    "        bilinear = True\n",
    "        self.inc = DoubleConv(c_in, self.width)\n",
    "        self.down1 = Down(self.width, self.width*2)\n",
    "        self.down2 = Down(self.width*2, self.width*4)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down3 = Down(self.width*4, self.width*8 // factor)\n",
    "        self.up1 = Up(self.width*8, self.width*4 // factor, bilinear)\n",
    "        self.up2 = Up(self.width*4, self.width*2 // factor, bilinear)\n",
    "        self.up3 = Up(self.width*2, self.width, bilinear)\n",
    "        self.outc = OutConv(self.width, c_out)\n",
    "        self.sa1 = SAWrapper(self.width*4, 8)\n",
    "        self.sa2 = SAWrapper(self.width*4, 4)\n",
    "        self.sa3 = SAWrapper(self.width*2, 8)\n",
    "        \n",
    "        self.label_embedding = nn.Embedding(n_classes, self.width*4)\n",
    "\n",
    "    def pos_encoding(self, t, channels, embed_size):\n",
    "        inv_freq = 1.0 / (\n",
    "            10000\n",
    "            ** (torch.arange(0, channels, 2, device=t.device).float() / channels)\n",
    "        )\n",
    "        pos_enc_a = torch.sin(t[:, None].repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t[:, None].repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc.view(-1, channels, 1, 1).repeat(1, 1, embed_size, embed_size)\n",
    "\n",
    "    def label_encoding(self, label, channels, embed_size):\n",
    "        return self.label_embedding(label)[:, :channels, None, None].repeat(1, 1, embed_size, embed_size)\n",
    "    \n",
    "    def forward(self, x, t, label):\n",
    "        \"\"\"\n",
    "        Model is U-Net with added positional encodings and self-attention layers.\n",
    "        \"\"\"\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1) + self.pos_encoding(t, self.width*2, 16) + self.label_encoding(label, self.width*2, 16)\n",
    "        x3 = self.down2(x2) + self.pos_encoding(t, self.width*4, 8) + self.label_encoding(label, self.width*4, 8)\n",
    "        x3 = self.sa1(x3)\n",
    "        x4 = self.down3(x3) + self.pos_encoding(t, self.width*4, 4) + self.label_encoding(label, self.width*4, 4)\n",
    "        x4 = self.sa2(x4)\n",
    "        x = self.up1(x4, x3) + self.pos_encoding(t, self.width*2, 8) + self.label_encoding(label, self.width*2, 8)\n",
    "        x = self.sa3(x)\n",
    "        x = self.up2(x, x2) + self.pos_encoding(t, self.width, 16) + self.label_encoding(label, self.width, 16)\n",
    "        x = self.up3(x, x1) + self.pos_encoding(t, self.width, 32) + self.label_encoding(label, self.width, 32)\n",
    "        output = self.outc(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) As in [the paper by Ho et al.](https://arxiv.org/pdf/2006.11239.pdf), the position and label encoding are added in every layer of the model, instead of as an input to the first layer only. Why do you think the authors made this choice?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Create a new function `train_conditional` to train this model.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_conditional(model, data_loaders, beta, num_epochs=10, lr=1e-3, device=device):\n",
    "    # TODO implement the training loop, based on your earlier train function\n",
    "\n",
    "def evaluate_conditional(model, test_loader, beta, device=device):\n",
    "    # TODO implement the evaluation loop, based on your earlier evaluate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Train the conditional model.**\n",
    "\n",
    "*Expected output: in our implementation, the training loss started at around 0.1 and went down quickly to 0.03 and lower.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conditional = UNetConditional().to(device)\n",
    "train_conditional(model_conditional, data_loaders, beta, num_epochs=20, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional sampling\n",
    "\n",
    "**(e) Modify the sampling function to include a label.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_model_conditional(x, model, beta, label):\n",
    "    # TODO copy and modify sample_from_model to include the label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Sample a few digits with label `3`.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "*Expected output: you should see recognizable images with the number you requested.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO sample some digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) Complete the code to sample and plot 10 samples for every digit.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_per_class = []\n",
    "for label in range(10):\n",
    "    # sample 10 digits with this label\n",
    "    x = torch.zeros_like(x_0[:10]).to(device)\n",
    "    x_per_class.append(x.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        plt.subplot(10, 10, j * 10 + i + 1)\n",
    "        plt.imshow(x_per_class[j][i, 0], cmap='gray')\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) Compare the output of the conditional model with that of the unconditional model. Which one is better?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.9 Discussion (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Compare the sources of randomness in our diffusion model with that in the variational autoencoder and the GAN in earlier assignments. What are the main differences?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Would you be able to train a good digit classification model on the initial input to the sampling function? Why, or why not?<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: for variational autoencoders, normalzing flows, and GANs, there is a clear link between the input (a latent feature vector) and the output of the decoder. How does this work for our diffusion model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) When loading the data, we normalized the image intensities to [-1, +1], instead of [0, 1] or [0, 255]. Why is this a good input range for this diffusion model?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) In this assignment, we use a $\\beta$ schedule that has a small $\\beta=1e-4$ at the initial time steps ($t=0$), and a larger $\\beta=0.02$ at the end ($t=T$). Why is it useful to choose an increasing $\\beta$?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) What would happen if we made $\\beta$ very small?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) What would happen if we made $\\beta$ very large?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end\n",
    "\n",
    "Well done! Please double check the instructions at the top before you submit your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This assignment has 42 points.*\n",
    "<span style=\"float:right;color:#aaa;font-size:10px;\"> Version 70d23d4 / 2024-12-03</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}