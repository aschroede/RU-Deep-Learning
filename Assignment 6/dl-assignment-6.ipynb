{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning &mdash; Assignment 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sixth assignment for the 2023 Deep Learning course (NWI-IMC070) of the Radboud University."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "**Names:** Andrew Schroeder and Fynn Gerding\n",
    "\n",
    "**Group:** 17\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "* Fill in your names and the name of your group.\n",
    "* Answer the questions and complete the code where necessary.\n",
    "* Keep your answers brief, one or two sentences is usually enough.\n",
    "* Re-run the whole notebook before you submit your work.\n",
    "* Save the notebook as a PDF and submit that in Brightspace together with the `.ipynb` notebook file.\n",
    "* The easiest way to make a PDF of your notebook is via File > Print Preview and then use your browser's print option to print to PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "In this assignment you will\n",
    "1. Construct a PyTorch `DataSet`.\n",
    "1. Train and modify a transformer network.\n",
    "1. Learn how to use a model based on the PyTorch documentation.\n",
    "1. Experiment with a translation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required software\n",
    "\n",
    "If you haven't done so already, you will need to install the following additional libraries:\n",
    "* `torch` for PyTorch,\n",
    "\n",
    "All libraries can be installed with `pip install`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import time\n",
    "from random import Random\n",
    "from typing import (List, Optional)\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import (IterableDataset, DataLoader)\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "# fix the seed, so outputs are exactly reproducible\n",
    "torch.manual_seed(12345);\n",
    "\n",
    "# Use the GPU if available\n",
    "def detect_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "device = detect_device()\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Learning to calculate (5 points)\n",
    "\n",
    "In this assignment we are going to train a neural network to do mathematics.\n",
    "When communicating between humans, mathematics is expressed with words and formulas.\n",
    "The simplest of these are formulas with a numeric answer. For example, we might ask what is `100+50`, to which the answer is `150`.\n",
    "\n",
    "To teach a computer how to do this task, we are going to need a dataset.\n",
    "\n",
    "Below is a function that generates a random formula. Study it, and see if you understand its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_integer(length: int, signed: bool = True, rng: Random = Random()):\n",
    "    max = int(math.pow(10, length))\n",
    "    min = -max if signed else 0\n",
    "    return rng.randint(min, max)\n",
    "\n",
    "def random_formula(complexity: int, signed: bool = True, rng: Random = Random()):\n",
    "    \"\"\"\n",
    "    Generate a random formula of the form \"a+b\" or \"a-b\".\n",
    "    complexity is the maximum number of digits in the numbers.\n",
    "    \"\"\"\n",
    "    a = random_integer(complexity, signed, rng)\n",
    "    b = random_integer(complexity, False, rng)\n",
    "    is_addition = not signed or rng.choice([False, True])\n",
    "    if is_addition:\n",
    "        return (f\"{a}+{b}\", str(a + b))\n",
    "    else:\n",
    "        return (f\"{a}-{b}\", str(a - b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('649+864', '1513')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 123456\n",
    "random_formula(3, rng=Random(seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `rng` argument allows us to reproduce the same random numbers, which you can verify by running the code below multiple times. But if you change the seed to `None` then the random generator is initialized differently each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649+864 = 1513\n",
      "-940-819 = -1759\n",
      "954-2 = 952\n",
      "-896-274 = -1170\n",
      "-762-954 = -1716\n"
     ]
    }
   ],
   "source": [
    "def random_formulas(complexity, signed, count, seed):\n",
    "    \"\"\"\n",
    "    Iterator that yields the given count of random formulas\n",
    "    \"\"\"\n",
    "    rng = Random(seed)\n",
    "    for i in range(count):\n",
    "        yield random_formula(complexity, signed, rng=rng)\n",
    "\n",
    "for q, a in random_formulas(3, True, 5, seed):\n",
    "    print(f'{q} = {a}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to treat these expressions as sequences of tokens, where each character is a token. In addition we will need tokens to denote begin-of-sequence and end-of-sequence, as well as padding, for which we will use `'<bos>'`, `'<eos>'`, and `'<pad>'` respectively, as was done in the lecture.\n",
    "\n",
    "### Creating a vocabulary\n",
    "\n",
    "The vocabulary is the set of all possible tokens. \n",
    "\n",
    "To store the vocabulary, we will use a helper class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"A vocabulary.\"\"\"\n",
    "    def __init__(self, tokens: List[str], specials: Optional[List[str]] = None):\n",
    "        \"\"\"\n",
    "        Construct a vocabulary given a list of tokens, and an optional list of special tokens\n",
    "        \"\"\"\n",
    "        self.tokens = tokens\n",
    "        if specials is not None:\n",
    "            self.tokens.extend(specials)\n",
    "        self.tokens.sort()\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.tokens)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        return self.lookup_index(token)\n",
    "\n",
    "    def lookup_index(self, token: str) -> int:\n",
    "        return self.token_to_idx[token]\n",
    "\n",
    "    def lookup_indices(self, tokens: List[str]) -> List[int]:\n",
    "        return [self.lookup_index(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, index: int) -> str:\n",
    "        return self.tokens[index]\n",
    "\n",
    "    def lookup_tokens(self, indices: List[int]) -> List[str]:\n",
    "        return [self.lookup_token(index) for index in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset we know beforehand what the vocabulary will be, so we can easily define it by hand.\n",
    "\n",
    "**(a) What are the tokens in this dataset? Complete the code below.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in all possible tokens\n",
    "vocab = Vocab([str(i) for i in range(0,10,1)], specials=['+', '-', '=', '<eos>', '<pad>', '<bos>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the vocabulary to double check that it makes sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 16\n",
      "Vocabulary: ['+', '-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '<bos>', '<eos>', '<pad>', '=']\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size:', len(vocab))\n",
    "print('Vocabulary:', vocab.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to tokenize and encode formula.\n",
    "\n",
    "**(b) Complete the code below.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_encode(string: str, vocab=vocab) -> List[int]:\n",
    "    # TODO: Tokenize the string and encode using the vocabulary.\n",
    "    #       Include an end-of-string token (but not a begin-of-string token).\n",
    "    return vocab.lookup_indices(list(string)) + [vocab.lookup_index('<eos>')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it on a random formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question 649+864 and answer 1513\n",
      "are encoded as [8, 6, 11, 0, 10, 8, 6, 13] and [3, 7, 3, 5, 13]\n"
     ]
    }
   ],
   "source": [
    "q, a = random_formula(3, rng=Random(seed))\n",
    "print('The question', q, 'and answer', a)\n",
    "print('are encoded as', tokenize_and_encode(q), 'and', tokenize_and_encode(a))\n",
    "\n",
    "# Check tokenize_and_encode\n",
    "assert ''.join(vocab.lookup_tokens(tokenize_and_encode(q))) == q + '<eos>'\n",
    "assert len(tokenize_and_encode(q)) == len(q) + 1\n",
    "assert tokenize_and_encode(\"1+2\") == vocab.lookup_indices(['1','+','2','<eos>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding and trimming\n",
    "\n",
    "Next, to be able to work with a whole dataset of these encoded sequences, they all need to be the same length.\n",
    "\n",
    "**(c) Implement the function below that pads or trims the encoded token sequence as needed.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: see [d2l section 10.5.3](http://d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html#loading-sequences-of-fixed-length) for a very similar function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_trim(tokens: List[int], target_length: int, vocab=vocab):\n",
    "    # TODO return a padded or trimmed sequence\n",
    "    pad_token = '<pad>'\n",
    "    output = []\n",
    "    for i, token in zip(range(target_length), tokens):\n",
    "        output.append(token)\n",
    "    while len(output) < target_length:\n",
    "        output.append(vocab.lookup_index(pad_token))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 6, 11, 0, 10, 8, 6, 13, 14, 14]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad or trim q to get a sequence of 10 tokens\n",
    "pad_or_trim(tokenize_and_encode(q), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check pad_or_trim\n",
    "assert len(pad_or_trim([1,2,3,4,5],10)) == 10\n",
    "assert len(pad_or_trim(list(range(20)),10)) == 10\n",
    "assert vocab.lookup_tokens(pad_or_trim([1,2,3,4,5],10)[5:]) == ['<pad>','<pad>','<pad>','<pad>','<pad>'], \\\n",
    "       f\"Incorrect padding tokens, found {vocab.to_tokens(pad_or_trim([1,2,3,4,5],10)[5:])}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translating tokens\n",
    "\n",
    "We can use `vocab.lookup_tokens` to convert the encoded token sequence back to something more readable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6', '4', '9', '+', '8', '6', '4', '<eos>', '<pad>', '<pad>']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_tokens(pad_or_trim(tokenize_and_encode(q), 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we define the `decode_tokens` function to convert entire lists or tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['5' '1' '3' '+' '1' '3' '2' '3' '<eos>' '<pad>']\n",
      " ['4' '1' '2' '+' '4' '2' '<eos>' '<pad>' '<pad>' '<pad>']]\n"
     ]
    }
   ],
   "source": [
    "def decode_tokens(t, vocab=vocab):\n",
    "    # convert a list, tensor, or array of encoded tokens\n",
    "    if isinstance(t, torch.Tensor):\n",
    "        t = t.detach().cpu()\n",
    "    else:\n",
    "        t = torch.tensor(t)\n",
    "    return np.asarray(vocab.lookup_tokens(list(t.flatten()))).reshape(*t.shape)\n",
    "\n",
    "# convert all tokens at once\n",
    "print(decode_tokens([pad_or_trim(tokenize_and_encode('513+1323'), 10),\n",
    "                     pad_or_trim(tokenize_and_encode('412+42'), 10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dataset\n",
    "\n",
    "The most convenient way to use a data generating function for training a neural network is to wrap it in a PyTorch `Dataset`. In this case, we will use an [IterableDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset), which can be used as an iterator to walk over the samples in the dataset.\n",
    "\n",
    "**(d) Complete the code below.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FormulaDataset(IterableDataset):\n",
    "    def __init__(self, complexity, signed, count, seed=None, vocab=vocab, device=device):\n",
    "        super(FormulaDataset).__init__()\n",
    "        self.seed = seed\n",
    "        self.complexity = complexity\n",
    "        self.signed = signed\n",
    "        self.count = count\n",
    "        self.vocab = vocab\n",
    "        self.max_question_length = 2 * complexity + 3\n",
    "        self.max_answer_length = complexity + 2\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.count\n",
    "    \n",
    "    def __iter__(self):\n",
    "        rng = Random(self.seed)\n",
    "        for _ in range(self.count):\n",
    "            q, a = random_formula(self.complexity, self.signed, rng=rng)\n",
    "            q = pad_or_trim(tokenize_and_encode(q), self.max_question_length, self.vocab)\n",
    "            a = pad_or_trim(tokenize_and_encode(a), self.max_answer_length, self.vocab)\n",
    "            yield torch.tensor(q, device=self.device), torch.tensor(a, device=self.device)\n",
    "\n",
    "    # TODO: Complete the class definition.\n",
    "    #       See the documentation for IterableDataset for examples.\n",
    "    #       Make sure that the values yielded by the iterator are pairs of torch tensors.\n",
    "    #       To create a repeatable dataset, always start with the same random seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Define a training set with 10000 formulas and a validation set with 5000 formulas, both with complexity 3.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Note: make sure that the training and validation set are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity = 3\n",
    "signed = True\n",
    "# TODO: Your code here.\n",
    "train_data = FormulaDataset(complexity, signed, 10_000, seed)\n",
    "val_data   = FormulaDataset(complexity, signed, 5_000, seed+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we wrap each dataset in a `DataLoader` to create minibatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loaders\n",
    "batch_size = 125\n",
    "data_loaders = {\n",
    "    'train': torch.utils.data.DataLoader(train_data, batch_size=batch_size),\n",
    "    'val':   torch.utils.data.DataLoader(val_data, batch_size=batch_size),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below checks that the datasets are defined correctly\n",
    "train_loader = data_loaders['train']\n",
    "val_loader = data_loaders['val']\n",
    "\n",
    "from typing import Tuple\n",
    "from typing_extensions import assert_type\n",
    "for (name, loader), expected_size in zip(data_loaders.items(), [10000,5000]):\n",
    "    first_batch = next(iter(loader))\n",
    "    assert len(first_batch) == 2, \\\n",
    "           f\"The {name} dataset should yield (question, answer) pairs when iterated over.\"\n",
    "    assert torch.is_tensor(first_batch[0]), \\\n",
    "           f\"The questions in the {name} dataset should be torch.tensors\"\n",
    "    assert tuple(first_batch[0].shape) == (batch_size, 2*complexity+3), \\\n",
    "           f\"The questions in the {name} dataset should be of size (batch_size, max_question_length), i.e. {batch_size,2*complexity+3}, found {tuple(first_batch[0].shape)}\"\n",
    "    assert first_batch[0].dtype in [torch.int32,torch.int64], \\\n",
    "           f\"The questions in the {name} dataset should be encoded as integers, found {first_batch[0].dtype}\"\n",
    "    assert torch.equal(next(iter(loader))[0], next(iter(loader))[0]), \\\n",
    "           f\"The {name} dataset should be deterministic, it should produce the same data each time\"\n",
    "    assert all([len(batch[0]) == batch_size for batch in iter(loader)]), \\\n",
    "           f\"Batches should all have the right size. Perhaps the batch size does not evenly divide the dataset size?\"\n",
    "    assert sum([len(batch[0]) for batch in iter(loader)]) == expected_size, \\\n",
    "           f\"{name} dataset does not have the right size, expected {expected_size}, found {sum([len(batch[0]) for batch in iter(loader)])}.\"\n",
    "assert not torch.equal(next(iter(train_loader))[0], next(iter(val_loader))[0]), \\\n",
    "       \"The training data and validation data should not be the same\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Transformer inputs (10 points)\n",
    "\n",
    "There is a detailed description of the transformer model in Bishop chapter 12, and in [d2l chapter 11](http://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html). We will not use the code from the d2l book, and instead use [PyTorch's built-in Transformer layers](https://pytorch.org/docs/stable/nn.html#transformer-layers).\n",
    "\n",
    "However, some details we still need to implement ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive inputs\n",
    "\n",
    "We will be training the decoder of the transformer as an autoregressive model.\n",
    "\n",
    "The code below takes a batch of data from the training set, and it generates a shifted version of the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_targets(y, bos_token=vocab['<bos>']):\n",
    "    \"\"\"\n",
    "    Shift a sequence of tokens by 1 position, and add `bos_token` at the start.\n",
    "    \"\"\"\n",
    "    bos = torch.tensor(bos_token, dtype=y.dtype, device=y.device).expand(y.shape[0], 1)\n",
    "    y_prev = torch.cat((bos, y[:, :-1]), axis=1)\n",
    "    return y_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1' '5' '1' '3' '<eos>']\n",
      " ['-' '1' '7' '5' '9']\n",
      " ['9' '5' '2' '<eos>' '<pad>']\n",
      " ['-' '1' '1' '7' '0']\n",
      " ['-' '1' '7' '1' '6']]\n",
      "[['<bos>' '1' '5' '1' '3']\n",
      " ['<bos>' '-' '1' '7' '5']\n",
      " ['<bos>' '9' '5' '2' '<eos>']\n",
      " ['<bos>' '-' '1' '1' '7']\n",
      " ['<bos>' '-' '1' '7' '1']]\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "y_prev = shift_targets(y)\n",
    "\n",
    "# print the first five samples\n",
    "print(decode_tokens(y)[:5])\n",
    "print(decode_tokens(y_prev)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Look at the values for the example above. What is `y_prev` used for during training of a transformer model?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y_prev` denotes the input of the transformer model, as it describes the sequence with a shift to the left by one token.\n",
    "\n",
    "For example, the transformer input would be `['<bos>' '1' '5' '1' '3']` and the output should be `'1' '5' '1' '3' '<eos>']`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Why do some rows of `y_prev` end in `'<eos>'`, but not all? Is this a problem?<span style=\"float:right\"> (1 point)</span>** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the sentences that were shorter than the length `4` (including the `<eos>` token `5`) end with an `<eos>` token, as here the shift by one removes only `<pad>` tokens.\n",
    "\n",
    "If the input of the model (`y_prev`) ends in `<eos>`, the most reasonable output prediction would be `<pad>`. If there is no `<eos>` token yet, the network's prediction should meaningfully capture the prediction of what will most likely come next (some number or special character in our case). Thus, some ending in `<eos>` and some not is not a problem but desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masks\n",
    "\n",
    "Training a transformer uses masked self-attention, so we need some masks. Here are two functions that make these masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(size, device=device):\n",
    "    \"\"\"\n",
    "    Mask that indicates that tokens at a position are not allowed to attend to\n",
    "    tokens in subsequent positions.\n",
    "    \"\"\"\n",
    "    mask = (torch.tril(torch.ones((size, size), device=device))) == 0\n",
    "    return mask\n",
    "\n",
    "def generate_padding_mask(tokens, padding_token):\n",
    "    \"\"\"\n",
    "    Mask that indicates which tokens should be ignored because they are padding.\n",
    "    \"\"\"\n",
    "    if not isinstance(tokens, torch.Tensor):\n",
    "        tokens = torch.tensor(tokens)\n",
    "    return tokens == padding_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Generate a padding mask for a random encoded token string.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: make sure that `tokens` is a torch.tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8,  6, 11,  0, 10,  8,  6, 13, 14, 14])\n",
      "['6' '4' '9' '+' '8' '6' '4' '<eos>' '<pad>' '<pad>']\n",
      "tensor([False, False, False, False, False, False, False, False,  True,  True])\n"
     ]
    }
   ],
   "source": [
    "q, a = random_formula(3, rng=Random(seed))\n",
    "# TODO: your code here\n",
    "tokens = torch.tensor(pad_or_trim(tokenize_and_encode(q), 10))\n",
    "padding_mask = generate_padding_mask(tokens, vocab.lookup_index('<pad>'))\n",
    "print(tokens)\n",
    "print(decode_tokens(tokens))\n",
    "print(padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More tests\n",
    "assert list(generate_padding_mask(torch.tensor(pad_or_trim(tokenize_and_encode(\"1+1\"), 8)), vocab['<pad>'])) == [False]*4 + [True]*4, \"Something is wrong with generate_padding_mask\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) How will this mask be used by a transformer?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer will not attend the `<pad>` tokens, due to the attention mask being `true` at these positions. It's important as the padding tokens don't contribute any useful information and just equalise sequence lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below illustrates what the output of `generate_square_subsequent_mask` looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n",
      "tensor([[False,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True],\n",
      "        [False, False, False,  True,  True],\n",
      "        [False, False, False, False,  True],\n",
      "        [False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "square_subsequent_mask = generate_square_subsequent_mask(y.shape[1])\n",
    "\n",
    "print(square_subsequent_mask.shape)\n",
    "print(square_subsequent_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) How and why should this mask be used? State your answer in terms of `x`,  `y` and/or `y_prev`.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To introduce temporal causality into the transformer, we apply this subsequent attention mask: The next output only depends on inputs that come _before_ the current one.\n",
    "\n",
    "# TODO: CONTINUE WITH THIS ANSWER LATER!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Give an example where it could make sense to use a different mask in a transformer network, instead of the `square_subsequent_mask`?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "Our discrete vocabulary is not suitable as the input for a transformer. We need an embedding function to map our input vocabulary to a continuous, high-dimensional space.\n",
    "\n",
    "We will use the `torch.nn.Embedding` class to for this. As you can read in the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding), this class maps each token in our vocabulary to a specific point in embedding space, its embedding vector. We will use this embedding vector as the input features for the next layer of our model.\n",
    "\n",
    "The parameters of the embedding are trainable: the embedding vector of each token is optimized along with the rest of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) Define an embedding that maps our vocabulary to a 5-dimensional space.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(16, 5)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Your code here.\n",
    "embedding = nn.Embedding(len(vocab), 5, device=device)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the embedding to some sequences from our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8,  6, 11,  0, 10,  8,  6, 13, 14],\n",
      "        [ 1, 11,  6,  2,  1, 10,  3, 11, 13],\n",
      "        [11,  7,  6,  1,  4, 13, 14, 14, 14]])\n",
      "tensor([[[ 1.5282,  0.4787, -1.3222, -0.2525, -2.0878],\n",
      "         [ 0.3857, -0.4650, -0.6855, -0.0400,  1.5445],\n",
      "         [-0.5270,  0.2969,  1.6919,  0.3020,  0.0561],\n",
      "         [-0.1282, -0.3737,  1.0683,  1.4699, -0.0245],\n",
      "         [-0.4661,  2.0167, -0.9744,  0.5161,  1.1131],\n",
      "         [ 1.5282,  0.4787, -1.3222, -0.2525, -2.0878],\n",
      "         [ 0.3857, -0.4650, -0.6855, -0.0400,  1.5445],\n",
      "         [ 0.3226,  1.8013, -0.3064,  1.2641, -1.0928],\n",
      "         [ 0.5746,  0.9841, -0.3564, -1.1278,  0.6000]],\n",
      "\n",
      "        [[ 1.1041, -0.8531, -0.8407,  0.7472,  0.1542],\n",
      "         [-0.5270,  0.2969,  1.6919,  0.3020,  0.0561],\n",
      "         [ 0.3857, -0.4650, -0.6855, -0.0400,  1.5445],\n",
      "         [-0.3487,  0.1794,  0.2893, -0.5633,  0.1424],\n",
      "         [ 1.1041, -0.8531, -0.8407,  0.7472,  0.1542],\n",
      "         [-0.4661,  2.0167, -0.9744,  0.5161,  1.1131],\n",
      "         [ 0.6427,  0.4626,  0.4081, -1.5313, -0.6521],\n",
      "         [-0.5270,  0.2969,  1.6919,  0.3020,  0.0561],\n",
      "         [ 0.3226,  1.8013, -0.3064,  1.2641, -1.0928]],\n",
      "\n",
      "        [[-0.5270,  0.2969,  1.6919,  0.3020,  0.0561],\n",
      "         [ 1.1718, -0.3803,  1.7336, -0.4511, -0.8936],\n",
      "         [ 0.3857, -0.4650, -0.6855, -0.0400,  1.5445],\n",
      "         [ 1.1041, -0.8531, -0.8407,  0.7472,  0.1542],\n",
      "         [-0.1719, -0.3395, -1.2016,  0.0678, -1.4114],\n",
      "         [ 0.3226,  1.8013, -0.3064,  1.2641, -1.0928],\n",
      "         [ 0.5746,  0.9841, -0.3564, -1.1278,  0.6000],\n",
      "         [ 0.5746,  0.9841, -0.3564, -1.1278,  0.6000],\n",
      "         [ 0.5746,  0.9841, -0.3564, -1.1278,  0.6000]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([3, 9])\n",
      "torch.Size([3, 9, 5])\n"
     ]
    }
   ],
   "source": [
    "# take the first batch\n",
    "x, y = next(iter(train_loader))\n",
    "# take three samples\n",
    "x = x[:3]\n",
    "# print the shapes\n",
    "print(x)\n",
    "print(embedding(x))\n",
    "print(x.shape)\n",
    "print(embedding(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) Explain the output shape.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the input `x` is `torch.Size([3, 9])` where 3 is the number of samples and 9 being the number of tokens. The embedding then translates this into a `torch.Size([3, 9, 5])` tensor where every token is expanded into a length 5 vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the embedding vectors, or the dimensionality of the embedding space, does not depend on the number of tokens in our vocabulary. We are free to choose an embedding size that fits our problem.\n",
    "\n",
    "For example, let's try an embedding with 2 dimensions, and plot the initial embedding for the tokens in our vocabulary.\n",
    "\n",
    "**(i) Create an embedding with 2 dimensions and plot the embedding for all tokens.<span style=\"float:right\"> (no points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Your code here.\n",
    "embedding = nn.Embedding(len(vocab), 2)\n",
    "\n",
    "# embed all tokens of our vocabulary\n",
    "x = torch.arange(len(vocab))\n",
    "emb = embedding(x).detach().cpu().numpy()\n",
    "\n",
    "plt.scatter(emb[:, 0], emb[:, 1]);\n",
    "for i, token in enumerate(vocab.tokens):\n",
    "    plt.annotate(token, (emb[i,0]+0.04, emb[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we need to balance the complexity of our networks: a larger embedding will increase the number of parameters in our model, but increase the risk of overfitting.\n",
    "\n",
    "**(j) Would this 2-dimensional embedding space be large enough for our problem?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integer numbers could be represented in a single dimension. Using the same dimension for math operations might not be very intuitive, so adding a 2nd dimension would probably be helpful. Thus a 2-dimensional embedding space may be enough for such a simple problem. In reality though, features are often scattered across multiple dimensions, so it could happen that in practice more dimensions result in a better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using an embedding, we could also use a simple one-hot encoding to map the words in the vocabulary to feature vectors. However, practical applications of natural language processing never do this. Why not?\n",
    "\n",
    "**(k) Explain the practical advantage of embeddings over one-hot encoding.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few advantage of embeddings compared to one-hot encodings:\n",
    "* Embeddings are constant in dimensionality, regardless of the amount of unique tokens\n",
    "* Embeddings are able to capture meaning within data, as the dimensionality reduction forces the network to represent a pattern in the data meaningfully. In contrast, one-hot encodings are all orthogonal and therefore they don't represent any inherent relationship between tokens.\n",
    "* For neural networks, a continuous input is often better for the optimisation and gradients, one-hot encodings are by definition binary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 `torch.nn.Transformer` (8 points)\n",
    "\n",
    "<div style=\"float: left\"><a href=\"https://cs.ru.nl/~gvtulder/vaswani-fig-1-highlight.png\"><img src=\"https://cs.ru.nl/~gvtulder/vaswani-fig-1-highlight.png\" width=\"300\"></a></div>\n",
    "\n",
    "We now have all required inputs for our transformer.\n",
    "\n",
    "Consult the documentation for the [`torch.nn.Transformer`](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html) class of PyTorch. This class implements a full Transformer as described in [\"Attention Is All You Need\"](https://arxiv.org/pdf/1706.03762.pdf), the paper that introduced this architecture.\n",
    "\n",
    "The `Transformer` class implements the main part of the of the Transformer architecture, shown highlighted in the image on the left (see also Fig. 1 in \"Attention Is All You Need\").\n",
    "\n",
    "For a given input sequence, it applies one or more encoder layers, followed by one or more decoder layers, to compute an output sequence that we can then process further.\n",
    "\n",
    "Because the `Transformer` class takes care of most of the complicated parts of the model, we can concentrate providing the inputs and outputs: the grayed-out areas in the image.\n",
    "\n",
    "Check out the parameters for the `Transformer` class and the inputs and outputs of its `forward` function.\n",
    "<br style=\"clear: both\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Which parameter of the `Transformer` class should we base on our embedding?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torch.nn.Transformer()` has an attribute `d_model` that describes the expected dimensionality of each input to the encoder / decoder. The expected dimensionality of the input is equivalent to the dimension of the embedding vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Given fixed input and output dimensions, which parameters of the `Transformer` can we use to change the complexity of our network?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of parameters that determine the complexity of the `torch.nn.Transformer()`. They are:\n",
    "\n",
    "* `nhead` the number of attention heads used\n",
    "* `num_encoder_layers` the number of layers in the transformer encoder\n",
    "* `num_decoder_layers` the number of layers in the transformer decoder\n",
    "* `dim_feedforward` the dimensionality of the fully connected layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) When using the `Transformer` class, where should we use the masks that we defined earlier?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers offer various ways to mask the data. The padding mask that we defined in 6.2 (c) refers to the `src_key_padding_mask` of the torch transformer (masking irrelevant tokens so that they are not attended), while the square subsequent mask refers to the `tgt_mask` parameter (initroducing causality into the decoder)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Complete the code for the TransformerNetwork.<span style=\"float:right\"> (5 points)</span>**\n",
    "\n",
    "Construct a network with the following architecture (see the image in the previous section for an overview):\n",
    "1. An embedding layer that embeds the input tokens into a space of size `dim_hidden`.\n",
    "2. A dropout layer (not shown in the image).\n",
    "3. A [Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html) with the specified parameters (`dim_hidden`, `num_heads`, `num_layers`, `dim_feedforward`, and `dropout`).<br>Note: you will need to pass `batch_first=True`, to indicate that the first dimension runs over the batch and not over the sequence.\n",
    "4. A final linear prediction layer that takes the output of the transformer to `dim_vocab` possible classes.\n",
    "\n",
    "Don't worry about positional encoding for now, we will add that later.\n",
    "\n",
    "The `forward` function should generate the appropriate masks and combine the layers defined in `__init__` to compute the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerNetwork(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim_vocab=len(vocab), padding_token=vocab['<pad>'],\n",
    "                 num_layers=2, num_heads=4, dim_hidden=64, dim_feedforward=64,\n",
    "                 dropout=0.01, positional_encoding=False):\n",
    "        super().__init__()\n",
    "        self.padding_token = padding_token\n",
    "        # TODO: Your code here.\n",
    "        self.embedding    = nn.Embedding(len(vocab), dim_hidden)\n",
    "        self.dropout      = nn.Dropout(dropout)\n",
    "        self.transformer  = torch.nn.Transformer(\n",
    "            d_model=dim_hidden, nhead=num_heads, num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers, dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout, batch_first=True)\n",
    "        self.predict      = nn.Linear(dim_hidden, dim_vocab)\n",
    "        if positional_encoding:\n",
    "            self.pos_encoding = ... # Fill this in later\n",
    "        else:\n",
    "            self.pos_encoding = torch.nn.Identity()\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # TODO: Your code here.\n",
    "        # Combine self.embedding, self.dropout, self.transformer, self.predict\n",
    "\n",
    "        # Embedding\n",
    "        src_embeddings = self.embedding(src)\n",
    "        tgt_embeddings = self.embedding(tgt)\n",
    "\n",
    "        # Dropout\n",
    "        src_embeddings = self.dropout(src_embeddings)\n",
    "        tgt_embeddings = self.dropout(tgt_embeddings)\n",
    "\n",
    "        src_key_padding_mask = generate_padding_mask(src, self.padding_token)\n",
    "        print(\"src_key_padding_mask\")\n",
    "        print(src_key_padding_mask[:5])\n",
    "        tgt_key_padding_mask = generate_padding_mask(tgt, self.padding_token)\n",
    "        print(\"tgt_key_padding_mask\")\n",
    "        print(tgt_key_padding_mask[:5])\n",
    "        causal_mask = generate_square_subsequent_mask(tgt.shape[1])\n",
    "        # memory_key_padding_mask = src_key_padding_mask\n",
    "\n",
    "        # Transformer\n",
    "        output = self.transformer(src_embeddings, tgt_embeddings,\n",
    "                                  #src_mask = src_key_padding_mask,\n",
    "                                  tgt_mask = causal_mask,\n",
    "                                  #memory_mask = tgt_key_padding_mask, \n",
    "                                  src_key_padding_mask=src_key_padding_mask, \n",
    "                                  tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                  tgt_is_causal=True)\n",
    "        # Predict\n",
    "        output = self.predict(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Try the transformer with an example batch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape torch.Size([125, 9])\n",
      "y.shape torch.Size([125, 5])\n",
      "y_prev.shape torch.Size([125, 5])\n",
      "src_key_padding_mask\n",
      "tensor([[False, False, False, False, False, False, False, False,  True],\n",
      "        [False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False]])\n",
      "tgt_key_padding_mask\n",
      "tensor([[False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False]])\n",
      "y_pred.shape torch.Size([125, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "net = TransformerNetwork(dim_feedforward=72)\n",
    "x, y = next(iter(train_loader))\n",
    "y_prev = shift_targets(y)\n",
    "\n",
    "print('x.shape', x.shape)\n",
    "print('y.shape', y.shape)\n",
    "print('y_prev.shape', y_prev.shape)\n",
    "\n",
    "y_pred = net(x, y_prev)\n",
    "print('y_pred.shape', y_pred.shape)\n",
    "\n",
    "# check the shape against what we expected\n",
    "np.testing.assert_equal(list(y_pred.shape), [y.shape[0], y.shape[1], len(vocab)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert these predictions to tokens (but they're obviously random):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['3' '9' '7' '9' '6']\n",
      " ['3' '4' '<bos>' '8' '9']\n",
      " ['3' '1' '<pad>' '4' '<pad>']\n",
      " ['3' '4' '<bos>' '<bos>' '<bos>']\n",
      " ['3' '<pad>' '<bos>' '4' '9']]\n"
     ]
    }
   ],
   "source": [
    "print(decode_tokens(torch.argmax(y_pred, dim=2))[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_key_padding_mask\n",
      "tensor([[False, False, False, False, False, False, False, False,  True],\n",
      "        [False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False]])\n",
      "tgt_key_padding_mask\n",
      "tensor([[False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False]])\n",
      "src_key_padding_mask\n",
      "tensor([[False, False, False, False, False, False, False, False,  True,  True,\n",
      "          True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False,  True,\n",
      "          True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False,  True,  True,  True,  True,\n",
      "          True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False,  True,\n",
      "          True,  True,  True,  True],\n",
      "        [False, False, False, False, False, False, False, False, False,  True,\n",
      "          True,  True,  True,  True]])\n",
      "tgt_key_padding_mask\n",
      "tensor([[False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False],\n",
      "        [False, False, False, False, False]])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Adding padding to x should not affect the output of the network. Check src_key_padding_mask and memory_key_padding_mask. The former controls self attention to padding tokens in the encoder, the latter controls cross attention from decoder to encoder.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Check that the forward function behaves correctly\u001b[39;00m\n\u001b[1;32m     16\u001b[0m net\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(torch\u001b[38;5;241m.\u001b[39misclose( \\\n\u001b[1;32m     18\u001b[0m             net(x, y_prev), \\\n\u001b[1;32m     19\u001b[0m             net(torch\u001b[38;5;241m.\u001b[39mcat((x,torch\u001b[38;5;241m.\u001b[39mtensor(vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mexpand(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m5\u001b[39m)), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), y_prev), atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)), \\\n\u001b[1;32m     20\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdding padding to x should not affect the output of the network. Check src_key_padding_mask and memory_key_padding_mask. The former controls self attention to padding tokens in the encoder, the latter controls cross attention from decoder to encoder.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(torch\u001b[38;5;241m.\u001b[39misclose( \\\n\u001b[1;32m     22\u001b[0m             net(x, y_prev), \\\n\u001b[1;32m     23\u001b[0m             net(x, torch\u001b[38;5;241m.\u001b[39mcat((y_prev,torch\u001b[38;5;241m.\u001b[39mtensor(vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mexpand(y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m5\u001b[39m)), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m], atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)), \\\n\u001b[1;32m     24\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdding padding to y should not affect the output of the network. Check tgt_key_padding_mask.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mall(torch\u001b[38;5;241m.\u001b[39misclose( \\\n\u001b[1;32m     26\u001b[0m             net(x, y_prev)[:,:\u001b[38;5;241m2\u001b[39m], \\\n\u001b[1;32m     27\u001b[0m             net(x, y_prev[:,:\u001b[38;5;241m2\u001b[39m]), atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m)), \\\n\u001b[1;32m     28\u001b[0m        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe presence of later tokens in y should not affect the output for earlier tokens. Check tgt_mask.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Adding padding to x should not affect the output of the network. Check src_key_padding_mask and memory_key_padding_mask. The former controls self attention to padding tokens in the encoder, the latter controls cross attention from decoder to encoder."
     ]
    }
   ],
   "source": [
    "# Check that the transformer is defined correctly\n",
    "assert isinstance(net.embedding, torch.nn.Embedding)\n",
    "assert isinstance(net.dropout, torch.nn.Dropout)\n",
    "assert isinstance(net.transformer, torch.nn.Transformer)\n",
    "assert isinstance(net.predict, torch.nn.Linear)\n",
    "# Check parameters of transformer\n",
    "assert net.transformer.d_model == 64\n",
    "assert net.transformer.nhead == 4\n",
    "assert net.transformer.batch_first == True\n",
    "assert net.transformer.encoder.num_layers == 2\n",
    "assert net.transformer.decoder.num_layers == 2\n",
    "assert net.transformer.encoder.layers[0].linear1.out_features == 72\n",
    "assert net.dropout.p == 0.01\n",
    "assert net.transformer.encoder.layers[0].dropout.p == 0.01\n",
    "# Check that the forward function behaves correctly\n",
    "net.train(False)\n",
    "assert torch.all(torch.isclose( \\\n",
    "            net(x, y_prev), \\\n",
    "            net(torch.cat((x,torch.tensor(vocab['<pad>']).expand(x.shape[0], 5)), axis=1), y_prev), atol=1e-5)), \\\n",
    "       \"Adding padding to x should not affect the output of the network. Check src_key_padding_mask and memory_key_padding_mask. The former controls self attention to padding tokens in the encoder, the latter controls cross attention from decoder to encoder.\"\n",
    "assert torch.all(torch.isclose( \\\n",
    "            net(x, y_prev), \\\n",
    "            net(x, torch.cat((y_prev,torch.tensor(vocab['<pad>']).expand(y.shape[0], 5)), axis=1))[:,:-5], atol=1e-5)), \\\n",
    "       \"Adding padding to y should not affect the output of the network. Check tgt_key_padding_mask.\"\n",
    "assert torch.all(torch.isclose( \\\n",
    "            net(x, y_prev)[:,:2], \\\n",
    "            net(x, y_prev[:,:2]), atol=1e-5)), \\\n",
    "       \"The presence of later tokens in y should not affect the output for earlier tokens. Check tgt_mask.\"\n",
    "assert torch.all(torch.isclose( \\\n",
    "            net(x, y_prev), \\\n",
    "            net(torch.flip(x, [1]), y_prev), atol=1e-5)), \\\n",
    "       \"Order of x should not matter for a transformer network. Check src_mask.\"\n",
    "assert not torch.all(torch.isclose( \\\n",
    "            net(x, torch.flip(y_prev, [1])), \\\n",
    "            torch.flip(net(x, y_prev), [1]), atol=1e-5)), \\\n",
    "       \"Order of y should matter for a transformer network. Check tgt_mask.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Training (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will base the training code on last week's code. A complication in computing the loss and accuracy are the padding tokens. So, before we work on the training loop itself, we need to update the `accuracy` function so it ingores these `<pad>` tokens. Let's do this in a generic way\n",
    "\n",
    "**(a) Copy the `accuracy` function from last week, and add a parameter `ignore_index`. The tokens with `true_y == ignore_index` should be ignored.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: you can select elements from a tensor with `some_tensor[include]` where `include` is a tensor of booleans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred_y, true_y, ignore_index=None):\n",
    "    # TODO: Your code here.\n",
    "    # Hint: See assignment 3 or 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the accuracy function.\n",
    "assert accuracy(torch.tensor([[1,0,0],[0.4,0.5,0.1],[0,1,0],[0.4,0.1,0.5]]), torch.tensor([0,1,2,2]), 1) == 2/3\n",
    "assert accuracy(torch.tensor([[1,0,0],[0.4,0.5,0.1],[0,1,0],[0.4,0.1,0.5]]), torch.tensor([0,1,2,2]), 2) == 1\n",
    "assert accuracy(torch.tensor([[1,0,0],[0.4,0.5,0.1],[0,1,0],[0.4,0.1,0.5]]), torch.tensor([0,1,2,2]), 3) == 3/4\n",
    "assert accuracy(torch.tensor([[1,0,0],[0.4,0.5,0.1],[0,1,0],[0.4,0.1,0.5]]), torch.tensor([2,2,1,2]), 2) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Write a training loop for the transformer model.<span style=\"float:right\"> (4 points)</span>**\n",
    "\n",
    "See last week's assignment for inspiration.\n",
    "The code is mostly the same with the following changes:\n",
    " * The cross-entropy loss function and accuracy should ignore all `<pad>` tokens. (Use `ignore_index`, see the [documentation of CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).)\n",
    " * The network expects `y_prev` as an extra input.\n",
    " * The output of the network contains a batch of N samples, with maximum length L, and gives logits over C classes, so it has size (N,L,C). But `CrossEntropyLoss` and `accuracy` expect a tensor of size (N,C,L). You can use [torch.Tensor.transpose](https://pytorch.org/docs/stable/generated/torch.transpose.html) to change the output to the right shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data_loaders, num_epochs=100, lr=0.001, optimizer=torch.optim.Adam, device=device):\n",
    "    \"\"\"\n",
    "    Train a network on the given data set.\n",
    "    After every epoch compute validation loss and accuracy.\n",
    "    \"\"\"\n",
    "    # TODO: Your code here.\n",
    "    # Hint: See assignment 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Train a transformer network. Use 100 epochs with a learning rate of 0.001<span style=\"float:right\"> (no points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Briefly discuss the results. Has the training converged? Is this a good calculator?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Run the trained network with input `\"123+123\"` and `\"321+321\"`.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, q, a):\n",
    "    # Run net to predict the output given the input `q` and y_prev based on `a`.\n",
    "    # Return predicted y\n",
    "    with torch.no_grad():\n",
    "        # TODO: Your code here.\n",
    "        pass\n",
    "\n",
    "for src, tgt in [('123+123', '246'), ('321+321', '642')]:\n",
    "    print(f'For {src}={tgt}')\n",
    "    y_pred = predict(net, src, tgt)\n",
    "    print('  y_pred[0]', y_pred[0])\n",
    "    print('  encoded', torch.argmax(y_pred, dim=-1))\n",
    "    print('  tokens', decode_tokens(torch.argmax(y_pred, dim=-1)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Compare the predictions for the first element of y with the two different inputs. Can you explain what happens?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) Does the validation accuracy estimate how often the model is able to answers formulas correctly? Explain your answer.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) If the forward function takes the shifted output `y_prev` as input, how can we use it if we don't know the output yet?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Positional encoding (5 points)\n",
    "\n",
    "We did not yet include positional encoding in the network.\n",
    "PyTorch does not include such an encoder. So, here we define such a module ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "    def __init__(self, num_hiddens, max_len=100):\n",
    "        super().__init__()\n",
    "        # Create a long enough matrix of position encodings P\n",
    "        positions = torch.arange(max_len, dtype=torch.float32)\n",
    "        freqs = torch.pow(max_len, (1 + 2 * torch.arange(num_hiddens / 2)) / num_hiddens)\n",
    "        X = positions[:,None] / freqs[None,:]\n",
    "        self.P = torch.zeros((max_len, num_hiddens))\n",
    "        self.P[:, 0::2] = torch.sin(X)\n",
    "        self.P[:, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X + self.P[None, :X.shape[1], :].to(X.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Add positional encoding to the TransformerModel.<span style=\"float:right\"> (point given in earlier question)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modify the code in 6.3d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Construct and train a network with positional encoding<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your answer here\n",
    "net_pos = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) How does the performance of a model with positional encoding compare to a model without?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Run the trained network with input `\"123+123\"` and `\"321+321\"`.<span style=\"float:right\"> (no points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Compare the predictions for the first element of y with what you found earlier. Can you explain what happens?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Explain in your own words why positional encoding is used in transformer networks.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) Look at the learning curve. Can you suggest a way to improve the model?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) Optional: if time permits, try to train an even better model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 Predicting for new samples (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting an output given a new sample requires an appropriate search algorithm (see [d2l chapter 10.8](https://d2l.ai/chapter_recurrent-modern/beam-search.html)). Here, we will implement the simplest form: a greedy search algorithm that selects the token with the highest probability at each time step.\n",
    "\n",
    "**(a) Describe this search strategy in pseudo-code.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Implement a greedy search function to predict a sequence using `net_pos`.<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_greedy(net, src, length):\n",
    "    # predict an output sequence of the given (maximum) length given input string src\n",
    "    with torch.no_grad():\n",
    "        # TODO: Your code here.\n",
    "        pass\n",
    "\n",
    "predicted_sequence = predict_greedy(net_pos, '123+123', 6)\n",
    "print(decode_tokens(predicted_sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Does this search strategy give a high-quality prediction? Why, or why not?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) What alternative search strategy could we use to improve the predictions? Why would this help?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 Discussion (4 points)\n",
    "\n",
    "Last week, we looked at recurrent neural networks such as the LSTM. Both recurrent neural networks and transformers work with sequences, but in recent years the transformer has become more popular than the recurrent models.\n",
    "\n",
    "**(a) An advantage of transformers over recurrent neural is that they can be faster to train. Why is that?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Does this advantage also hold when predicting outputs for new sequences? Why, or why not?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Why is positional encoding often used in transformers, but not in convolutional or recurrent neural networks?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of a recurrent neural network makes it very suitable for online predictions, such as real-time translation, because it only depends on prior inputs. You can design an architecture where the RNN produces an output token for every input token given to it, and it can produce that output without having to wait for the rest of the input.\n",
    "\n",
    "Note: 'online' means producing outputs continuously as new input comes in, as opposed to collecting a full dataset and analyzing it afterwards, it has nothing to do with the internet.\n",
    "\n",
    "**(d) How would a transformer work in an online application? Do you need to change the architecture?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end\n",
    "\n",
    "Well done! Please double check the instructions at the top before you submit your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This assignment has 47 points.*\n",
    "<span style=\"float:right;color:#aaa;font-size:10px;\"> Version 4f237eb / 2024-10-08</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
