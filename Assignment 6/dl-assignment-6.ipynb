{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning &mdash; Assignment 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sixth assignment for the 2023 Deep Learning course (NWI-IMC070) of the Radboud University."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "**Names:** Andrew Schroeder and Fynn Gerding\n",
    "\n",
    "**Group:** 17\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "* Fill in your names and the name of your group.\n",
    "* Answer the questions and complete the code where necessary.\n",
    "* Keep your answers brief, one or two sentences is usually enough.\n",
    "* Re-run the whole notebook before you submit your work.\n",
    "* Save the notebook as a PDF and submit that in Brightspace together with the `.ipynb` notebook file.\n",
    "* The easiest way to make a PDF of your notebook is via File > Print Preview and then use your browser's print option to print to PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "In this assignment you will\n",
    "1. Construct a PyTorch `DataSet`.\n",
    "1. Train and modify a transformer network.\n",
    "1. Learn how to use a model based on the PyTorch documentation.\n",
    "1. Experiment with a translation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required software\n",
    "\n",
    "If you haven't done so already, you will need to install the following additional libraries:\n",
    "* `torch` for PyTorch,\n",
    "\n",
    "All libraries can be installed with `pip install`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import time\n",
    "from random import Random\n",
    "from typing import (List, Optional)\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import (IterableDataset, DataLoader)\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import pandas as pd\n",
    "\n",
    "# fix the seed, so outputs are exactly reproducible\n",
    "torch.manual_seed(12345);\n",
    "\n",
    "# Use the GPU if available\n",
    "def detect_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "device = detect_device()\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Learning to calculate (5 points)\n",
    "\n",
    "In this assignment we are going to train a neural network to do mathematics.\n",
    "When communicating between humans, mathematics is expressed with words and formulas.\n",
    "The simplest of these are formulas with a numeric answer. For example, we might ask what is `100+50`, to which the answer is `150`.\n",
    "\n",
    "To teach a computer how to do this task, we are going to need a dataset.\n",
    "\n",
    "Below is a function that generates a random formula. Study it, and see if you understand its parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_integer(length: int, signed: bool = True, rng: Random = Random()):\n",
    "    max = int(math.pow(10, length))\n",
    "    min = -max if signed else 0\n",
    "    return rng.randint(min, max)\n",
    "\n",
    "def random_formula(complexity: int, signed: bool = True, rng: Random = Random()):\n",
    "    \"\"\"\n",
    "    Generate a random formula of the form \"a+b\" or \"a-b\".\n",
    "    complexity is the maximum number of digits in the numbers.\n",
    "    \"\"\"\n",
    "    a = random_integer(complexity, signed, rng)\n",
    "    b = random_integer(complexity, False, rng)\n",
    "    is_addition = not signed or rng.choice([False, True])\n",
    "    if is_addition:\n",
    "        return (f\"{a}+{b}\", str(a + b))\n",
    "    else:\n",
    "        return (f\"{a}-{b}\", str(a - b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('649+864', '1513')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 123456\n",
    "random_formula(3, rng=Random(seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `rng` argument allows us to reproduce the same random numbers, which you can verify by running the code below multiple times. But if you change the seed to `None` then the random generator is initialized differently each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "649+864 = 1513\n",
      "-940-819 = -1759\n",
      "954-2 = 952\n",
      "-896-274 = -1170\n",
      "-762-954 = -1716\n"
     ]
    }
   ],
   "source": [
    "def random_formulas(complexity, signed, count, seed):\n",
    "    \"\"\"\n",
    "    Iterator that yields the given count of random formulas\n",
    "    \"\"\"\n",
    "    rng = Random(seed)\n",
    "    for i in range(count):\n",
    "        yield random_formula(complexity, signed, rng=rng)\n",
    "\n",
    "for q, a in random_formulas(3, True, 5, seed):\n",
    "    print(f'{q} = {a}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to treat these expressions as sequences of tokens, where each character is a token. In addition we will need tokens to denote begin-of-sequence and end-of-sequence, as well as padding, for which we will use `'<bos>'`, `'<eos>'`, and `'<pad>'` respectively, as was done in the lecture.\n",
    "\n",
    "### Creating a vocabulary\n",
    "\n",
    "The vocabulary is the set of all possible tokens. \n",
    "\n",
    "To store the vocabulary, we will use a helper class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"A vocabulary.\"\"\"\n",
    "    def __init__(self, tokens: List[str], specials: Optional[List[str]] = None):\n",
    "        \"\"\"\n",
    "        Construct a vocabulary given a list of tokens, and an optional list of special tokens\n",
    "        \"\"\"\n",
    "        self.tokens = tokens\n",
    "        if specials is not None:\n",
    "            self.tokens.extend(specials)\n",
    "        self.tokens.sort()\n",
    "        self.token_to_idx = {token: idx for idx, token in enumerate(self.tokens)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        return self.lookup_index(token)\n",
    "\n",
    "    def lookup_index(self, token: str) -> int:\n",
    "        return self.token_to_idx[token]\n",
    "\n",
    "    def lookup_indices(self, tokens: List[str]) -> List[int]:\n",
    "        return [self.lookup_index(token) for token in tokens]\n",
    "\n",
    "    def lookup_token(self, index: int) -> str:\n",
    "        return self.tokens[index]\n",
    "\n",
    "    def lookup_tokens(self, indices: List[int]) -> List[str]:\n",
    "        return [self.lookup_token(index) for index in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset we know beforehand what the vocabulary will be, so we can easily define it by hand.\n",
    "\n",
    "**(a) What are the tokens in this dataset? Complete the code below.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: fill in all possible tokens\n",
    "vocab = Vocab([str(i) for i in range(0,10,1)], specials=['+', '-', '=', '<eos>', '<pad>', '<bos>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the vocabulary to double check that it makes sense:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 16\n",
      "Vocabulary: ['+', '-', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '<bos>', '<eos>', '<pad>', '=']\n"
     ]
    }
   ],
   "source": [
    "print('Vocabulary size:', len(vocab))\n",
    "print('Vocabulary:', vocab.tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to tokenize and encode formula.\n",
    "\n",
    "**(b) Complete the code below.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_encode(string: str, vocab=vocab) -> List[int]:\n",
    "    # TODO: Tokenize the string and encode using the vocabulary.\n",
    "    #       Include an end-of-string token (but not a begin-of-string token).\n",
    "    return vocab.lookup_indices(list(string)) + [vocab.lookup_index('<eos>')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it on a random formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question 649+864 and answer 1513\n",
      "are encoded as [8, 6, 11, 0, 10, 8, 6, 13] and [3, 7, 3, 5, 13]\n"
     ]
    }
   ],
   "source": [
    "q, a = random_formula(3, rng=Random(seed))\n",
    "print('The question', q, 'and answer', a)\n",
    "print('are encoded as', tokenize_and_encode(q), 'and', tokenize_and_encode(a))\n",
    "\n",
    "# Check tokenize_and_encode\n",
    "assert ''.join(vocab.lookup_tokens(tokenize_and_encode(q))) == q + '<eos>'\n",
    "assert len(tokenize_and_encode(q)) == len(q) + 1\n",
    "assert tokenize_and_encode(\"1+2\") == vocab.lookup_indices(['1','+','2','<eos>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding and trimming\n",
    "\n",
    "Next, to be able to work with a whole dataset of these encoded sequences, they all need to be the same length.\n",
    "\n",
    "**(c) Implement the function below that pads or trims the encoded token sequence as needed.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: see [d2l section 10.5.3](http://d2l.ai/chapter_recurrent-modern/machine-translation-and-dataset.html#loading-sequences-of-fixed-length) for a very similar function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_trim(tokens: List[int], target_length: int, vocab=vocab):\n",
    "    # TODO return a padded or trimmed sequence\n",
    "    pad_token = '<pad>'\n",
    "    output = []\n",
    "    for i, token in zip(range(target_length), tokens):\n",
    "        output.append(token)\n",
    "    while len(output) < target_length:\n",
    "        output.append(vocab.lookup_index(pad_token))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 6, 11, 0, 10, 8, 6, 13, 14, 14]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pad or trim q to get a sequence of 10 tokens\n",
    "pad_or_trim(tokenize_and_encode(q), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check pad_or_trim\n",
    "assert len(pad_or_trim([1,2,3,4,5],10)) == 10\n",
    "assert len(pad_or_trim(list(range(20)),10)) == 10\n",
    "assert vocab.lookup_tokens(pad_or_trim([1,2,3,4,5],10)[5:]) == ['<pad>','<pad>','<pad>','<pad>','<pad>'], \\\n",
    "       f\"Incorrect padding tokens, found {vocab.to_tokens(pad_or_trim([1,2,3,4,5],10)[5:])}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translating tokens\n",
    "\n",
    "We can use `vocab.lookup_tokens` to convert the encoded token sequence back to something more readable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6', '4', '9', '+', '8', '6', '4', '<eos>', '<pad>', '<pad>']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_tokens(pad_or_trim(tokenize_and_encode(q), 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we define the `decode_tokens` function to convert entire lists or tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['5' '1' '3' '+' '1' '3' '2' '3' '<eos>' '<pad>']\n",
      " ['4' '1' '2' '+' '4' '2' '<eos>' '<pad>' '<pad>' '<pad>']]\n"
     ]
    }
   ],
   "source": [
    "def decode_tokens(t, vocab=vocab):\n",
    "    # convert a list, tensor, or array of encoded tokens\n",
    "    if isinstance(t, torch.Tensor):\n",
    "        t = t.detach().cpu()\n",
    "    else:\n",
    "        t = torch.tensor(t)\n",
    "    return np.asarray(vocab.lookup_tokens(list(t.flatten()))).reshape(*t.shape)\n",
    "\n",
    "# convert all tokens at once\n",
    "print(decode_tokens([pad_or_trim(tokenize_and_encode('513+1323'), 10),\n",
    "                     pad_or_trim(tokenize_and_encode('412+42'), 10)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a dataset\n",
    "\n",
    "The most convenient way to use a data generating function for training a neural network is to wrap it in a PyTorch `Dataset`. In this case, we will use an [IterableDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset), which can be used as an iterator to walk over the samples in the dataset.\n",
    "\n",
    "**(d) Complete the code below.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FormulaDataset(IterableDataset):\n",
    "    def __init__(self, complexity, signed, count, seed=None, vocab=vocab, device=device):\n",
    "        super(FormulaDataset).__init__()\n",
    "        self.seed = seed\n",
    "        self.complexity = complexity\n",
    "        self.signed = signed\n",
    "        self.count = count\n",
    "        self.vocab = vocab\n",
    "        self.max_question_length = 2 * complexity + 3\n",
    "        self.max_answer_length = complexity + 2\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.count\n",
    "    \n",
    "    def __iter__(self):\n",
    "        rng = Random(self.seed)\n",
    "        for _ in range(self.count):\n",
    "            q, a = random_formula(self.complexity, self.signed, rng=rng)\n",
    "            q = pad_or_trim(tokenize_and_encode(q), self.max_question_length, self.vocab)\n",
    "            a = pad_or_trim(tokenize_and_encode(a), self.max_answer_length, self.vocab)\n",
    "            yield torch.tensor(q, device=self.device), torch.tensor(a, device=self.device)\n",
    "\n",
    "    # TODO: Complete the class definition.\n",
    "    #       See the documentation for IterableDataset for examples.\n",
    "    #       Make sure that the values yielded by the iterator are pairs of torch tensors.\n",
    "    #       To create a repeatable dataset, always start with the same random seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Define a training set with 10000 formulas and a validation set with 5000 formulas, both with complexity 3.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Note: make sure that the training and validation set are different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "complexity = 3\n",
    "signed = True\n",
    "# TODO: Your code here.\n",
    "train_data = FormulaDataset(complexity, signed, 10_000, seed)\n",
    "val_data   = FormulaDataset(complexity, signed, 5_000, seed+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we wrap each dataset in a `DataLoader` to create minibatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loaders\n",
    "batch_size = 125\n",
    "data_loaders = {\n",
    "    'train': torch.utils.data.DataLoader(train_data, batch_size=batch_size),\n",
    "    'val':   torch.utils.data.DataLoader(val_data, batch_size=batch_size),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below checks that the datasets are defined correctly\n",
    "train_loader = data_loaders['train']\n",
    "val_loader = data_loaders['val']\n",
    "\n",
    "from typing import Tuple\n",
    "from typing_extensions import assert_type\n",
    "for (name, loader), expected_size in zip(data_loaders.items(), [10000,5000]):\n",
    "    first_batch = next(iter(loader))\n",
    "    assert len(first_batch) == 2, \\\n",
    "           f\"The {name} dataset should yield (question, answer) pairs when iterated over.\"\n",
    "    assert torch.is_tensor(first_batch[0]), \\\n",
    "           f\"The questions in the {name} dataset should be torch.tensors\"\n",
    "    assert tuple(first_batch[0].shape) == (batch_size, 2*complexity+3), \\\n",
    "           f\"The questions in the {name} dataset should be of size (batch_size, max_question_length), i.e. {batch_size,2*complexity+3}, found {tuple(first_batch[0].shape)}\"\n",
    "    assert first_batch[0].dtype in [torch.int32,torch.int64], \\\n",
    "           f\"The questions in the {name} dataset should be encoded as integers, found {first_batch[0].dtype}\"\n",
    "    assert torch.equal(next(iter(loader))[0], next(iter(loader))[0]), \\\n",
    "           f\"The {name} dataset should be deterministic, it should produce the same data each time\"\n",
    "    assert all([len(batch[0]) == batch_size for batch in iter(loader)]), \\\n",
    "           f\"Batches should all have the right size. Perhaps the batch size does not evenly divide the dataset size?\"\n",
    "    assert sum([len(batch[0]) for batch in iter(loader)]) == expected_size, \\\n",
    "           f\"{name} dataset does not have the right size, expected {expected_size}, found {sum([len(batch[0]) for batch in iter(loader)])}.\"\n",
    "assert not torch.equal(next(iter(train_loader))[0], next(iter(val_loader))[0]), \\\n",
    "       \"The training data and validation data should not be the same\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Transformer inputs (10 points)\n",
    "\n",
    "There is a detailed description of the transformer model in Bishop chapter 12, and in [d2l chapter 11](http://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html). We will not use the code from the d2l book, and instead use [PyTorch's built-in Transformer layers](https://pytorch.org/docs/stable/nn.html#transformer-layers).\n",
    "\n",
    "However, some details we still need to implement ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoregressive inputs\n",
    "\n",
    "We will be training the decoder of the transformer as an autoregressive model.\n",
    "\n",
    "The code below takes a batch of data from the training set, and it generates a shifted version of the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_targets(y, bos_token=vocab['<bos>']):\n",
    "    \"\"\"\n",
    "    Shift a sequence of tokens by 1 position, and add `bos_token` at the start.\n",
    "    \"\"\"\n",
    "    bos = torch.tensor(bos_token, dtype=y.dtype, device=y.device).expand(y.shape[0], 1)\n",
    "    y_prev = torch.cat((bos, y[:, :-1]), axis=1)\n",
    "    return y_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1' '5' '1' '3' '<eos>']\n",
      " ['-' '1' '7' '5' '9']\n",
      " ['9' '5' '2' '<eos>' '<pad>']\n",
      " ['-' '1' '1' '7' '0']\n",
      " ['-' '1' '7' '1' '6']]\n",
      "[['<bos>' '1' '5' '1' '3']\n",
      " ['<bos>' '-' '1' '7' '5']\n",
      " ['<bos>' '9' '5' '2' '<eos>']\n",
      " ['<bos>' '-' '1' '1' '7']\n",
      " ['<bos>' '-' '1' '7' '1']]\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_loader))\n",
    "y_prev = shift_targets(y)\n",
    "\n",
    "# print the first five samples\n",
    "print(decode_tokens(y)[:5])\n",
    "print(decode_tokens(y_prev)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Look at the values for the example above. What is `y_prev` used for during training of a transformer model?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y_prev` denotes the input of the transformer model, as it describes the sequence with a shift to the left by one token.\n",
    "\n",
    "For example, the transformer input would be `['<bos>' '1' '5' '1' '3']` and the output should be `'1' '5' '1' '3' '<eos>']`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Why do some rows of `y_prev` end in `'<eos>'`, but not all? Is this a problem?<span style=\"float:right\"> (1 point)</span>** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the sentences that were shorter than the length `4` (including the `<eos>` token `5`) end with an `<eos>` token, as here the shift by one removes only `<pad>` tokens.\n",
    "\n",
    "If the input of the model (`y_prev`) ends in `<eos>`, the most reasonable output prediction would be `<pad>`. If there is no `<eos>` token yet, the network's prediction should meaningfully capture the prediction of what will most likely come next (some number or special character in our case). Thus, some ending in `<eos>` and some not is not a problem but desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masks\n",
    "\n",
    "Training a transformer uses masked self-attention, so we need some masks. Here are two functions that make these masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(size, device=device):\n",
    "    \"\"\"\n",
    "    Mask that indicates that tokens at a position are not allowed to attend to\n",
    "    tokens in subsequent positions.\n",
    "    \"\"\"\n",
    "    mask = (torch.tril(torch.ones((size, size), device=device))) == 0\n",
    "    return mask\n",
    "\n",
    "def generate_padding_mask(tokens, padding_token):\n",
    "    \"\"\"\n",
    "    Mask that indicates which tokens should be ignored because they are padding.\n",
    "    \"\"\"\n",
    "    if not isinstance(tokens, torch.Tensor):\n",
    "        tokens = torch.tensor(tokens)\n",
    "    return tokens == padding_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Generate a padding mask for a random encoded token string.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: make sure that `tokens` is a torch.tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 8,  6, 11,  0, 10,  8,  6, 13, 14, 14])\n",
      "['6' '4' '9' '+' '8' '6' '4' '<eos>' '<pad>' '<pad>']\n",
      "tensor([False, False, False, False, False, False, False, False,  True,  True])\n"
     ]
    }
   ],
   "source": [
    "q, a = random_formula(3, rng=Random(seed))\n",
    "# TODO: your code here\n",
    "tokens = torch.tensor(pad_or_trim(tokenize_and_encode(q), 10))\n",
    "padding_mask = generate_padding_mask(tokens, vocab.lookup_index('<pad>'))\n",
    "print(tokens)\n",
    "print(decode_tokens(tokens))\n",
    "print(padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More tests\n",
    "assert list(generate_padding_mask(torch.tensor(pad_or_trim(tokenize_and_encode(\"1+1\"), 8)), vocab['<pad>'])) == [False]*4 + [True]*4, \"Something is wrong with generate_padding_mask\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) How will this mask be used by a transformer?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformer will not attend the `<pad>` tokens, due to the attention mask being `true` at these positions. It's important as the padding tokens don't contribute any useful information and just equalise sequence lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below illustrates what the output of `generate_square_subsequent_mask` looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 5])\n",
      "tensor([[False,  True,  True,  True,  True],\n",
      "        [False, False,  True,  True,  True],\n",
      "        [False, False, False,  True,  True],\n",
      "        [False, False, False, False,  True],\n",
      "        [False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "square_subsequent_mask = generate_square_subsequent_mask(y.shape[1])\n",
    "\n",
    "print(square_subsequent_mask.shape)\n",
    "print(square_subsequent_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) How and why should this mask be used? State your answer in terms of `x`,  `y` and/or `y_prev`.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When predicting the output, the decoder will go token-by-token. Every next token is dependend on all of the previous ones. To simulate this temporal causality in the training data, we apply this subsequent attention mask: The next output only depends on inputs that come _before_ the current one. Like this, the model is shown the answer one step at a time and always learns to predict the next token based on the earlier ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Give an example where it could make sense to use a different mask in a transformer network, instead of the `square_subsequent_mask`?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case the transformer is bi-directional, for example BERT, it is explicitly desired that context from before and after the current input is considered. Then, a subsequent mask like defined above would not be applicable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "\n",
    "Our discrete vocabulary is not suitable as the input for a transformer. We need an embedding function to map our input vocabulary to a continuous, high-dimensional space.\n",
    "\n",
    "We will use the `torch.nn.Embedding` class to for this. As you can read in the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding), this class maps each token in our vocabulary to a specific point in embedding space, its embedding vector. We will use this embedding vector as the input features for the next layer of our model.\n",
    "\n",
    "The parameters of the embedding are trainable: the embedding vector of each token is optimized along with the rest of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) Define an embedding that maps our vocabulary to a 5-dimensional space.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(16, 5)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Your code here.\n",
    "embedding = nn.Embedding(len(vocab), 5, device=device)\n",
    "print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the embedding to some sequences from our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 8,  6, 11,  0, 10,  8,  6, 13, 14],\n",
      "        [ 1, 11,  6,  2,  1, 10,  3, 11, 13],\n",
      "        [11,  7,  6,  1,  4, 13, 14, 14, 14]])\n",
      "tensor([[[ 1.5282,  0.4787, -1.3222, -0.2525, -2.0878],\n",
      "         [ 0.3857, -0.4650, -0.6855, -0.0400,  1.5445],\n",
      "         [-0.5270,  0.2969,  1.6919,  0.3020,  0.0561],\n",
      "         [-0.1282, -0.3737,  1.0683,  1.4699, -0.0245],\n",
      "         [-0.4661,  2.0167, -0.9744,  0.5161,  1.1131],\n",
      "         [ 1.5282,  0.4787, -1.3222, -0.2525, -2.0878],\n",
      "         [ 0.3857, -0.4650, -0.6855, -0.0400,  1.5445],\n",
      "         [ 0.3226,  1.8013, -0.3064,  1.2641, -1.0928],\n",
      "         [ 0.5746,  0.9841, -0.3564, -1.1278,  0.6000]],\n",
      "\n",
      "        [[ 1.1041, -0.8531, -0.8407,  0.7472,  0.1542],\n",
      "         [-0.5270,  0.2969,  1.6919,  0.3020,  0.0561],\n",
      "         [ 0.3857, -0.4650, -0.6855, -0.0400,  1.5445],\n",
      "         [-0.3487,  0.1794,  0.2893, -0.5633,  0.1424],\n",
      "         [ 1.1041, -0.8531, -0.8407,  0.7472,  0.1542],\n",
      "         [-0.4661,  2.0167, -0.9744,  0.5161,  1.1131],\n",
      "         [ 0.6427,  0.4626,  0.4081, -1.5313, -0.6521],\n",
      "         [-0.5270,  0.2969,  1.6919,  0.3020,  0.0561],\n",
      "         [ 0.3226,  1.8013, -0.3064,  1.2641, -1.0928]],\n",
      "\n",
      "        [[-0.5270,  0.2969,  1.6919,  0.3020,  0.0561],\n",
      "         [ 1.1718, -0.3803,  1.7336, -0.4511, -0.8936],\n",
      "         [ 0.3857, -0.4650, -0.6855, -0.0400,  1.5445],\n",
      "         [ 1.1041, -0.8531, -0.8407,  0.7472,  0.1542],\n",
      "         [-0.1719, -0.3395, -1.2016,  0.0678, -1.4114],\n",
      "         [ 0.3226,  1.8013, -0.3064,  1.2641, -1.0928],\n",
      "         [ 0.5746,  0.9841, -0.3564, -1.1278,  0.6000],\n",
      "         [ 0.5746,  0.9841, -0.3564, -1.1278,  0.6000],\n",
      "         [ 0.5746,  0.9841, -0.3564, -1.1278,  0.6000]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "torch.Size([3, 9])\n",
      "torch.Size([3, 9, 5])\n"
     ]
    }
   ],
   "source": [
    "# take the first batch\n",
    "x, y = next(iter(train_loader))\n",
    "# take three samples\n",
    "x = x[:3]\n",
    "# print the shapes\n",
    "print(x)\n",
    "print(embedding(x))\n",
    "print(x.shape)\n",
    "print(embedding(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) Explain the output shape.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of the input `x` is `torch.Size([3, 9])` where 3 is the number of samples and 9 being the number of tokens. The embedding then translates this into a `torch.Size([3, 9, 5])` tensor where every token is expanded into a length 5 vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the embedding vectors, or the dimensionality of the embedding space, does not depend on the number of tokens in our vocabulary. We are free to choose an embedding size that fits our problem.\n",
    "\n",
    "For example, let's try an embedding with 2 dimensions, and plot the initial embedding for the tokens in our vocabulary.\n",
    "\n",
    "**(i) Create an embedding with 2 dimensions and plot the embedding for all tokens.<span style=\"float:right\"> (no points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Your code here.\n",
    "embedding = nn.Embedding(len(vocab), 2)\n",
    "\n",
    "# embed all tokens of our vocabulary\n",
    "x = torch.arange(len(vocab))\n",
    "emb = embedding(x).detach().cpu().numpy()\n",
    "\n",
    "plt.scatter(emb[:, 0], emb[:, 1]);\n",
    "for i, token in enumerate(vocab.tokens):\n",
    "    plt.annotate(token, (emb[i,0]+0.04, emb[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we need to balance the complexity of our networks: a larger embedding will increase the number of parameters in our model, but increase the risk of overfitting.\n",
    "\n",
    "**(j) Would this 2-dimensional embedding space be large enough for our problem?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integer numbers could be represented in a single dimension. Using the same dimension for math operations might not be very intuitive, so adding a 2nd dimension would probably be helpful. Thus a 2-dimensional embedding space may be enough for such a simple problem. In reality though, features are often scattered across multiple dimensions, so it could happen that in practice more dimensions result in a better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using an embedding, we could also use a simple one-hot encoding to map the words in the vocabulary to feature vectors. However, practical applications of natural language processing never do this. Why not?\n",
    "\n",
    "**(k) Explain the practical advantage of embeddings over one-hot encoding.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few advantage of embeddings compared to one-hot encodings:\n",
    "* Embeddings are constant in dimensionality, regardless of the amount of unique tokens\n",
    "* Embeddings are able to capture meaning within data, as the dimensionality reduction forces the network to represent a pattern in the data meaningfully. In contrast, one-hot encodings are all orthogonal and therefore they don't represent any inherent relationship between tokens.\n",
    "* For neural networks, a continuous input is often better for the optimisation and gradients, one-hot encodings are by definition binary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 `torch.nn.Transformer` (8 points)\n",
    "\n",
    "<div style=\"float: left\"><a href=\"https://cs.ru.nl/~gvtulder/vaswani-fig-1-highlight.png\"><img src=\"https://cs.ru.nl/~gvtulder/vaswani-fig-1-highlight.png\" width=\"300\"></a></div>\n",
    "\n",
    "We now have all required inputs for our transformer.\n",
    "\n",
    "Consult the documentation for the [`torch.nn.Transformer`](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html) class of PyTorch. This class implements a full Transformer as described in [\"Attention Is All You Need\"](https://arxiv.org/pdf/1706.03762.pdf), the paper that introduced this architecture.\n",
    "\n",
    "The `Transformer` class implements the main part of the of the Transformer architecture, shown highlighted in the image on the left (see also Fig. 1 in \"Attention Is All You Need\").\n",
    "\n",
    "For a given input sequence, it applies one or more encoder layers, followed by one or more decoder layers, to compute an output sequence that we can then process further.\n",
    "\n",
    "Because the `Transformer` class takes care of most of the complicated parts of the model, we can concentrate providing the inputs and outputs: the grayed-out areas in the image.\n",
    "\n",
    "Check out the parameters for the `Transformer` class and the inputs and outputs of its `forward` function.\n",
    "<br style=\"clear: both\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Which parameter of the `Transformer` class should we base on our embedding?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `torch.nn.Transformer()` has an attribute `d_model` that describes the expected dimensionality of each input to the encoder / decoder. The expected dimensionality of the input is equivalent to the dimension of the embedding vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Given fixed input and output dimensions, which parameters of the `Transformer` can we use to change the complexity of our network?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of parameters that determine the complexity of the `torch.nn.Transformer()`. They are:\n",
    "\n",
    "* `nhead` the number of attention heads used\n",
    "* `num_encoder_layers` the number of layers in the transformer encoder\n",
    "* `num_decoder_layers` the number of layers in the transformer decoder\n",
    "* `dim_feedforward` the dimensionality of the fully connected layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) When using the `Transformer` class, where should we use the masks that we defined earlier?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers offer various ways to mask the data. The padding mask that we defined in 6.2 (c) refers to the `src_key_padding_mask` of the torch transformer (masking irrelevant tokens so that they are not attended), while the square subsequent mask refers to the `tgt_mask` parameter (initroducing causality into the decoder).\n",
    "\n",
    "We should use the masks as follows\n",
    "1. `src_key_padding_mask`: generate_padding_mask. Used for masking the padding tokens on the input to the encoder\n",
    "2. `tgt_key_padding_mask`: generate_padding_mask. Used for masking the padding tokens on the input to the decoder\n",
    "3. `memory_key_padding_mask`: generate_padding_mask. Using for masking padding on the cross attention data going from encoder to decoder\n",
    "4. `tgt_mask`: generate_square_subsequent_mask. Used for masking future tokesn in the decoder to prevent it from cheating and looking at the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Complete the code for the TransformerNetwork.<span style=\"float:right\"> (5 points)</span>**\n",
    "\n",
    "Construct a network with the following architecture (see the image in the previous section for an overview):\n",
    "1. An embedding layer that embeds the input tokens into a space of size `dim_hidden`.\n",
    "2. A dropout layer (not shown in the image).\n",
    "3. A [Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html) with the specified parameters (`dim_hidden`, `num_heads`, `num_layers`, `dim_feedforward`, and `dropout`).<br>Note: you will need to pass `batch_first=True`, to indicate that the first dimension runs over the batch and not over the sequence.\n",
    "4. A final linear prediction layer that takes the output of the transformer to `dim_vocab` possible classes.\n",
    "\n",
    "Don't worry about positional encoding for now, we will add that later.\n",
    "\n",
    "The `forward` function should generate the appropriate masks and combine the layers defined in `__init__` to compute the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerNetwork(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim_vocab=len(vocab), padding_token=vocab['<pad>'],\n",
    "                 num_layers=2, num_heads=4, dim_hidden=64, dim_feedforward=64,\n",
    "                 dropout=0.01, positional_encoding=False):\n",
    "        super().__init__()\n",
    "        self.padding_token = padding_token\n",
    "        # TODO: Your code here.\n",
    "        self.embedding    = nn.Embedding(len(vocab), dim_hidden)\n",
    "        self.dropout      = nn.Dropout(dropout)\n",
    "        self.transformer  = torch.nn.Transformer(\n",
    "            d_model=dim_hidden, nhead=num_heads, num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers, dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout, batch_first=True)\n",
    "        self.predict      = nn.Linear(dim_hidden, dim_vocab)\n",
    "        if positional_encoding:\n",
    "            self.pos_encoding = PositionalEncoding(num_hiddens=dim_hidden)\n",
    "        else:\n",
    "            self.pos_encoding = torch.nn.Identity()\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        # Compute embedding\n",
    "        src_embedding = self.embedding(src)\n",
    "        tgt_embedding = self.embedding(tgt)\n",
    "\n",
    "        # Apply positional encoding\n",
    "        src_embedding = self.pos_encoding(src_embedding)\n",
    "        tgt_embedding = self.pos_encoding(tgt_embedding)\n",
    "\n",
    "        # Apply dropout\n",
    "        src_embedding = self.dropout(src_embedding)\n",
    "        tgt_embedding = self.dropout(tgt_embedding)\n",
    "\n",
    "        # Construct masks\n",
    "        src_key_padding_mask = generate_padding_mask(src, self.padding_token)\n",
    "        tgt_key_padding_mask = generate_padding_mask(tgt, self.padding_token)\n",
    "\n",
    "        tgt_mask = generate_square_subsequent_mask(tgt.shape[1])\n",
    "\n",
    "        # Feed through transformer\n",
    "        output = self.transformer(src_embedding, tgt_embedding,\n",
    "                                  src_key_padding_mask=src_key_padding_mask, # Ignore padding on encoder side\n",
    "                                  tgt_key_padding_mask=tgt_key_padding_mask, # Ignore padding on decoder side\n",
    "                                  memory_key_padding_mask=src_key_padding_mask, # Ignore padding in cross attention from encoder to decoder\n",
    "                                  tgt_mask=tgt_mask) # Prevent decoder from cheating by looking into the future. \n",
    "        \n",
    "        # Predict\n",
    "        output = self.predict(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Try the transformer with an example batch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape torch.Size([125, 9])\n",
      "y.shape torch.Size([125, 5])\n",
      "y_prev.shape torch.Size([125, 5])\n",
      "y_pred.shape torch.Size([125, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "net = TransformerNetwork(dim_feedforward=72)\n",
    "x, y = next(iter(train_loader))\n",
    "y_prev = shift_targets(y)\n",
    "\n",
    "print('x.shape', x.shape)\n",
    "print('y.shape', y.shape)\n",
    "print('y_prev.shape', y_prev.shape)\n",
    "\n",
    "y_pred = net(x, y_prev)\n",
    "print('y_pred.shape', y_pred.shape)\n",
    "\n",
    "# check the shape against what we expected\n",
    "np.testing.assert_equal(list(y_pred.shape), [y.shape[0], y.shape[1], len(vocab)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert these predictions to tokens (but they're obviously random):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['+' '4' '6' '4' '6']\n",
      " ['8' '6' '4' '0' '8']\n",
      " ['8' '9' '8' '2' '4']\n",
      " ['9' '6' '4' '<eos>' '6']\n",
      " ['8' '6' '4' '6' '4']]\n"
     ]
    }
   ],
   "source": [
    "print(decode_tokens(torch.argmax(y_pred, dim=2))[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the transformer is defined correctly\n",
    "assert isinstance(net.embedding, torch.nn.Embedding)\n",
    "assert isinstance(net.dropout, torch.nn.Dropout)\n",
    "assert isinstance(net.transformer, torch.nn.Transformer)\n",
    "assert isinstance(net.predict, torch.nn.Linear)\n",
    "# Check parameters of transformer\n",
    "assert net.transformer.d_model == 64\n",
    "assert net.transformer.nhead == 4\n",
    "assert net.transformer.batch_first == True\n",
    "assert net.transformer.encoder.num_layers == 2\n",
    "assert net.transformer.decoder.num_layers == 2\n",
    "assert net.transformer.encoder.layers[0].linear1.out_features == 72\n",
    "assert net.dropout.p == 0.01\n",
    "assert net.transformer.encoder.layers[0].dropout.p == 0.01\n",
    "# Check that the forward function behaves correctly\n",
    "net.train(False)\n",
    "assert torch.all(torch.isclose( \\\n",
    "            net(x, y_prev), \\\n",
    "            net(torch.cat((x,torch.tensor(vocab['<pad>']).expand(x.shape[0], 5)), axis=1), y_prev), atol=1e-5)), \\\n",
    "       \"Adding padding to x should not affect the output of the network. Check src_key_padding_mask and memory_key_padding_mask. The former controls self attention to padding tokens in the encoder, the latter controls cross attention from decoder to encoder.\"\n",
    "assert torch.all(torch.isclose( \\\n",
    "            net(x, y_prev), \\\n",
    "            net(x, torch.cat((y_prev,torch.tensor(vocab['<pad>']).expand(y.shape[0], 5)), axis=1))[:,:-5], atol=1e-5)), \\\n",
    "       \"Adding padding to y should not affect the output of the network. Check tgt_key_padding_mask.\"\n",
    "assert torch.all(torch.isclose( \\\n",
    "            net(x, y_prev)[:,:2], \\\n",
    "            net(x, y_prev[:,:2]), atol=1e-5)), \\\n",
    "       \"The presence of later tokens in y should not affect the output for earlier tokens. Check tgt_mask.\"\n",
    "assert torch.all(torch.isclose( \\\n",
    "            net(x, y_prev), \\\n",
    "            net(torch.flip(x, [1]), y_prev), atol=1e-5)), \\\n",
    "       \"Order of x should not matter for a transformer network. Check src_mask.\"\n",
    "assert not torch.all(torch.isclose( \\\n",
    "            net(x, torch.flip(y_prev, [1])), \\\n",
    "            torch.flip(net(x, y_prev), [1]), atol=1e-5)), \\\n",
    "       \"Order of y should matter for a transformer network. Check tgt_mask.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Training (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will base the training code on last week's code. A complication in computing the loss and accuracy are the padding tokens. So, before we work on the training loop itself, we need to update the `accuracy` function so it ingores these `<pad>` tokens. Let's do this in a generic way\n",
    "\n",
    "**(a) Copy the `accuracy` function from last week, and add a parameter `ignore_index`. The tokens with `true_y == ignore_index` should be ignored.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: you can select elements from a tensor with `some_tensor[include]` where `include` is a tensor of booleans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred_y, true_y, ignore_index=None):\n",
    "    # Computes the mean accuracy, disregarding ignore_index`.\n",
    "    if ignore_index is not None:\n",
    "        mask = true_y != ignore_index\n",
    "        pred_y = pred_y[mask]\n",
    "        true_y = true_y[mask]\n",
    "\n",
    "    if pred_y.shape[1] == 1:\n",
    "        # binary classification\n",
    "        correct = (pred_y[:, 0] > 0).to(true_y.dtype) == true_y\n",
    "    else:\n",
    "        # multi-class classification\n",
    "        correct = pred_y.argmax(dim=1) == true_y\n",
    "    return int(correct.sum()) / len(true_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the accuracy function.\n",
    "assert accuracy(torch.tensor([[1,0,0],[0.4,0.5,0.1],[0,1,0],[0.4,0.1,0.5]]), torch.tensor([0,1,2,2]), 1) == 2/3\n",
    "assert accuracy(torch.tensor([[1,0,0],[0.4,0.5,0.1],[0,1,0],[0.4,0.1,0.5]]), torch.tensor([0,1,2,2]), 2) == 1\n",
    "assert accuracy(torch.tensor([[1,0,0],[0.4,0.5,0.1],[0,1,0],[0.4,0.1,0.5]]), torch.tensor([0,1,2,2]), 3) == 3/4\n",
    "assert accuracy(torch.tensor([[1,0,0],[0.4,0.5,0.1],[0,1,0],[0.4,0.1,0.5]]), torch.tensor([2,2,1,2]), 2) == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Write a training loop for the transformer model.<span style=\"float:right\"> (4 points)</span>**\n",
    "\n",
    "See last week's assignment for inspiration.\n",
    "The code is mostly the same with the following changes:\n",
    " * The cross-entropy loss function and accuracy should ignore all `<pad>` tokens. (Use `ignore_index`, see the [documentation of CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).)\n",
    " * The network expects `y_prev` as an extra input.\n",
    " * The output of the network contains a batch of N samples, with maximum length L, and gives logits over C classes, so it has size (N,L,C). But `CrossEntropyLoss` and `accuracy` expect a tensor of size (N,C,L). You can use [torch.Tensor.transpose](https://pytorch.org/docs/stable/generated/torch.transpose.html) to change the output to the right shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plotter:\n",
    "    \"\"\"For plotting data in animation.\"\"\"\n",
    "    # Based on d2l.Animator\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "                 ylim=None, xscale='linear', yscale='linear',\n",
    "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
    "                 figsize=(3.5, 2.5)):\n",
    "        \"\"\"Defined in :numref:`sec_utils`\"\"\"\n",
    "        # Incrementally plot multiple lines\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        self.fig, self.axes = plt.subplots(nrows, ncols, figsize=figsize)\n",
    "        if nrows * ncols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # Use a function to capture arguments\n",
    "        def config_axes():\n",
    "            axis = self.axes[0]\n",
    "            axis.set_xlabel(xlabel), axis.set_ylabel(ylabel)\n",
    "            axis.set_xscale(xscale), axis.set_yscale(yscale)\n",
    "            axis.set_xlim(xlim),     axis.set_ylim(ylim)\n",
    "            if legend:\n",
    "                axis.legend(legend)\n",
    "            axis.grid()\n",
    "        self.config_axes = config_axes\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "\n",
    "    def add(self, x, y):\n",
    "        # Add multiple data points into the figure\n",
    "        if not hasattr(y, \"__len__\"):\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"):\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display.display(self.fig)\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "class Metrics:\n",
    "    \"\"\"Accumulate mean values of one or more metrics.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.count = 0\n",
    "        self.sum = (0,) * n\n",
    "    def add(self, count, *values):\n",
    "        self.count += count\n",
    "        self.sum = tuple(s + count * v for s,v in zip(self.sum,values))\n",
    "    def mean(self):\n",
    "        return tuple(s / self.count for s in self.sum)\n",
    "    \n",
    "def evaluate(net, test_loader, loss_function=torch.nn.CrossEntropyLoss(), device=device):\n",
    "    \"\"\"\n",
    "    Evaluate a model on the given dataset.\n",
    "    Return loss, accuracy\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        metrics = Metrics(2)\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_prev = shift_targets(y)\n",
    "            pred_y = net(x, y_prev)\n",
    "            loss = loss_function(pred_y.transpose(1,2), y)\n",
    "            acc = accuracy(pred_y, y, ignore_index=vocab['<pad>'])\n",
    "            metrics.add(len(y), loss.item(), acc)\n",
    "        return metrics.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data_loaders, num_epochs=100, lr=0.001, optimizer=torch.optim.Adam, device=device):\n",
    "    \"\"\"\n",
    "    Train a network on the given data set.\n",
    "    After every epoch compute validation loss and accuracy.\n",
    "    \"\"\"\n",
    "    # TODO: Your code here.\n",
    "    # Hint: See assignment 4.\n",
    "    net.to(device)\n",
    "    train_loader = data_loaders['train']\n",
    "    num_batches = len(train_loader)\n",
    "    optimizer = optimizer(net.parameters(), lr=lr)\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    plotter = Plotter(xlabel='epoch', xlim=[1, num_epochs],\n",
    "                      legend=['train loss', 'train acc', 'val loss', 'val acc'])\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        # Sum of training loss, sum of training accuracy, no. of examples\n",
    "        net.train()\n",
    "        metrics = Metrics(2)\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_prev = shift_targets(y)\n",
    "            pred_y = net(x, y_prev)\n",
    "            loss = loss_function(pred_y.transpose(1, 2), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            with torch.no_grad():\n",
    "                acc = accuracy(pred_y, y, ignore_index=vocab['<pad>'])\n",
    "                metrics.add(len(y), loss.item(), acc)\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                train_loss, train_acc = metrics.mean()\n",
    "                plotter.add(epoch + (i + 1) / num_batches, (train_loss, train_acc, None, None))\n",
    "        val_loss, val_acc = evaluate(net, data_loaders['val'], loss_function=loss_function, device=device)\n",
    "        plotter.add(epoch + 1, (None, None, val_loss, val_acc))\n",
    "    train_loss, train_acc = metrics.mean()\n",
    "    train_time = time.time() - start_time\n",
    "    print(f'train loss {train_loss:.3f}, train acc {train_acc:.3f}, '\n",
    "          f'val loss {val_loss:.3f}, val acc {val_acc:.3f}')\n",
    "    print(f'{metrics.count * num_epochs / train_time:.1f} examples/sec '\n",
    "          f'on {str(device)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Train a transformer network. Use 100 epochs with a learning rate of 0.001<span style=\"float:right\"> (no points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.019, train acc 0.605, val loss 1.690, val acc 0.440\n",
      "3659.1 examples/sec on cpu\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = TransformerNetwork()\n",
    "train(net, data_loaders, num_epochs=100, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Briefly discuss the results. Has the training converged? Is this a good calculator?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very poor calculator as the validation accuracy is worse than chance at 43.6%. It doesn't seem like the training has converged yet as the the train loss is still decreasing and hasn't levelled out yet. However it looks like the network may be starting to overfit because the validation loss is increasing after an initial drop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Run the trained network with input `\"123+123\"` and `\"321+321\"`.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 123+123=246\n",
      "tensor([[ -8.4386,  -0.8281,  -5.4316,   1.4194,   2.9509,   4.6788,   3.7779,\n",
      "           3.1972,   0.9710,   1.3220,   0.9842,  -0.6270,  -8.9443,  -5.8506,\n",
      "          -8.1154,  -8.4006],\n",
      "        [-12.3719,  -8.2244,   1.8477,   0.8369,   3.0697,   3.5785,   3.4350,\n",
      "           3.7305,  -2.1240,   1.4117,   1.6794,  -0.2560, -12.6761,  -5.3188,\n",
      "         -11.6388, -11.6288],\n",
      "        [ -5.9556,  -4.6204,  -2.7006,  -0.5771,   0.8332,   2.9586,   4.2669,\n",
      "           4.9000,   0.3325,   2.9850,  -1.0061,  -1.0782,  -5.9508,  -2.3627,\n",
      "          -8.4241,  -5.2852],\n",
      "        [ -3.5844,  -3.0461,  -1.4554,  -1.9503,   1.7475,   3.3329,   2.1905,\n",
      "           1.1986,  -2.7255,   1.5173,  -2.4445,  -2.1394,  -3.6216,   8.0438,\n",
      "          -3.1113,  -2.6692],\n",
      "        [  2.7132,   0.2959,   1.8889,   0.4256,   0.8163,   1.8464,   1.5978,\n",
      "          -0.1826,  -3.0893,  -3.7585,   1.6219,  -4.3490,   2.6499,   0.0233,\n",
      "          17.0374,   2.5332]])\n",
      "  Most Probable indices tensor([ 5,  7,  7, 13, 14])\n",
      "  Decoded ['3' '5' '5' '<eos>' '<pad>']\n",
      "  Pretty: 355<eos><pad>\n",
      "For 321+321=642\n",
      "tensor([[-8.4386, -0.8281, -5.4316,  1.4194,  2.9509,  4.6788,  3.7779,  3.1972,\n",
      "          0.9710,  1.3220,  0.9842, -0.6270, -8.9443, -5.8506, -8.1154, -8.4006],\n",
      "        [-8.4892, -3.2556,  0.0237,  0.7142,  5.0955,  6.0747,  5.5034,  3.4151,\n",
      "         -2.0927, -1.4673, -2.2976,  0.0504, -9.0145, -5.5861, -7.5559, -7.7634],\n",
      "        [-2.7393, -1.3783,  2.8971,  0.2462,  3.6682,  5.1241,  3.2678,  0.2995,\n",
      "         -3.9089, -0.8141, -3.0752, -1.0558, -2.8483, -1.0534, -3.4176, -1.9507],\n",
      "        [-2.3098, -1.6542, -2.4898, -1.7184,  2.0710,  3.2716,  1.9302,  2.5196,\n",
      "         -3.2016, -0.0536, -2.4039, -1.1826, -2.6387,  8.2694, -0.9087, -1.9089],\n",
      "        [ 2.9154,  0.9062,  1.1559,  0.9811,  0.4975,  2.4362,  1.7019, -0.7208,\n",
      "         -2.9200, -3.9916,  1.5216, -3.9175,  2.7899,  0.2009, 17.8274,  2.6377]])\n",
      "  Most Probable indices tensor([ 5,  5,  5, 13, 14])\n",
      "  Decoded ['3' '3' '3' '<eos>' '<pad>']\n",
      "  Pretty: 333<eos><pad>\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(linewidth=200)\n",
    "def predict(net, q, a):\n",
    "    # Run net to predict the output given the input `q` and y_prev based on `a`.\n",
    "    # Return predicted y\n",
    "    q = torch.tensor(pad_or_trim(tokenize_and_encode(q), 2*complexity+3)).unsqueeze(0)\n",
    "    a = torch.tensor(pad_or_trim(tokenize_and_encode(a), complexity+2)).unsqueeze(0)\n",
    "    y_prev = shift_targets(a)\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        pred_y = net(q, y_prev)\n",
    "    return pred_y\n",
    "\n",
    "for src, tgt in [('123+123', '246'), ('321+321', '642')]:\n",
    "    print(f'For {src}={tgt}')\n",
    "    y_pred = predict(net, src, tgt)\n",
    "    first = y_pred[0]\n",
    "    print(first)\n",
    "    most_probable = torch.argmax(first, 1)\n",
    "    print('  Most Probable indices', most_probable)\n",
    "    print('  Decoded', decode_tokens(most_probable))\n",
    "    print('  Pretty: ' + ''.join(decode_tokens(most_probable)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Compare the predictions for the first element of y with the two different inputs. Can you explain what happens?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, I'm not sure what is happening here, there doesn't seem to be a consistent pattern that I can seen. The first sample outputs 123+123=355 and the second sample outputs 321+321 = 333, both of which are incorrect. These additions don't require any carry operations so that shouldn't be the cause of the problem... It's probably caused by a lack of positional encodings such that the model doesn't know which digit of the first argument to add to which digit of the second argument, and which digit in the output the result corresponds to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) Does the validation accuracy estimate how often the model is able to answers formulas correctly? Explain your answer.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, the validation accuracy is calculated by using the cross entropy loss to tally up the number of correct predictions out of the total number of predictions therefore it translates to estimating how often the model is able to answer formulas correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) If the forward function takes the shifted output `y_prev` as input, how can we use it if we don't know the output yet?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should be able to feed in the `SOS` token to the decoder as the initial input `y_prev` which will then output the first predicted token `y_1`. We can then feed `<SOS>, y_1` into the decoder again at the next step to produce the next output `y_2`, etc in an autoregressive manner. Of course you wouldn't do this during training, but only during inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Positional encoding (5 points)\n",
    "\n",
    "We did not yet include positional encoding in the network.\n",
    "PyTorch does not include such an encoder. So, here we define such a module ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding.\"\"\"\n",
    "    def __init__(self, num_hiddens, max_len=100):\n",
    "        super().__init__()\n",
    "        # Create a long enough matrix of position encodings P\n",
    "        positions = torch.arange(max_len, dtype=torch.float32)\n",
    "        freqs = torch.pow(max_len, (1 + 2 * torch.arange(num_hiddens / 2)) / num_hiddens)\n",
    "        X = positions[:,None] / freqs[None,:]\n",
    "        self.P = torch.zeros((max_len, num_hiddens))\n",
    "        self.P[:, 0::2] = torch.sin(X)\n",
    "        self.P[:, 1::2] = torch.cos(X)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X + self.P[None, :X.shape[1], :].to(X.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Add positional encoding to the TransformerModel.<span style=\"float:right\"> (point given in earlier question)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... see 6.3 (d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Construct and train a network with positional encoding<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_pos = TransformerNetwork(positional_encoding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) How does the performance of a model with positional encoding compare to a model without?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 0.133, train acc 0.953, val loss 0.127, val acc 0.960\n",
      "3800.9 examples/sec on cpu\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(net_pos, data_loaders, num_epochs=100, lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models performs significantly better with positional encoding, with a validation accuracy of 98%, a huge performance increase over the original network. Additionally it does not seem to be overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Run the trained network with input `\"123+123\"` and `\"321+321\"`.<span style=\"float:right\"> (no points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 123+123=246\n",
      "  Most Probable indices tensor([ 4,  6,  8, 13, 14])\n",
      "  Decoded ['2' '4' '6' '<eos>' '<pad>']\n",
      "  Pretty: 246<eos><pad>\n",
      "For 321+321=642\n",
      "  Most Probable indices tensor([ 8,  6,  4, 13, 14])\n",
      "  Decoded ['6' '4' '2' '<eos>' '<pad>']\n",
      "  Pretty: 642<eos><pad>\n"
     ]
    }
   ],
   "source": [
    "for src, tgt in [('123+123', '246'), ('321+321', '642')]:\n",
    "    print(f'For {src}={tgt}')\n",
    "    y_pred = predict(net_pos, src, tgt)\n",
    "    first = y_pred[0]\n",
    "    most_probable = torch.argmax(first, 1)\n",
    "    print('  Most Probable indices', most_probable)\n",
    "    print('  Decoded', decode_tokens(most_probable))\n",
    "    print('  Pretty: ' + ''.join(decode_tokens(most_probable)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Compare the predictions for the first element of y with what you found earlier. Can you explain what happens?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With positional encoding the network now outputs the correct answers for both tests. I think this is because now it realizes that it needs to add the digit at each index of one argument to the corresponding digit at the same index in the second argument to produce the output digit at that same index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Explain in your own words why positional encoding is used in transformer networks.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional encoding is used in transformers because unlike RNNs or LSTMs, the position of the input elements is not inherently enforced by the architecture of the network. The standard transformer does not distinguish between two input sequences with the same tokens but arranged in different orders. Thus if we are dealing with data where position information is important, we must add positional encodings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) Look at the learning curve. Can you suggest a way to improve the model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall the model looks to train well and converge to a good solution. The training and validation loss seem to plateau right in the epochs leading up to epcoh 20, before dropping quickly again after epoch 20.  This plato may reflect many things, but it could mean that the model found a local minimum in the loss landscape, before Adam escaped it. To relate this back to the dataset that we are using, it might be that the model understood how to add numbers at that point, but did not learn subtraction, or vice versa. It could also be that this is a sub-group of 'simple' problems, where element wise operations are possible, so the questions where no element-wise addition leads to a number above 9 / no subtraction resulted in a number lower than zero. As this is just speculation, it would be interesting to find out why this \"hump\" in the training occurs with experimental data. Perhaps also simply increasing the dropout rate may help the model learns to generalize earlier on in the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(h) Optional: if time permits, try to train an even better model**\n",
    "\n",
    "Time does not permit :'("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 Predicting for new samples (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting an output given a new sample requires an appropriate search algorithm (see [d2l chapter 10.8](https://d2l.ai/chapter_recurrent-modern/beam-search.html)). Here, we will implement the simplest form: a greedy search algorithm that selects the token with the highest probability at each time step.\n",
    "\n",
    "**(a) Describe this search strategy in pseudo-code.<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each time step $t'$ we select the token with the highest conditional probabiilty. So we can iterate through the time steps and perform the following calculation:\n",
    "\n",
    "$$\n",
    "y_{t'} = \\text{argmax}_{y\\in \\mathcal{Y}} P(y|y_1,\\dots,y_{t'-1},\\boldsymbol{c})\n",
    "$$\n",
    "\n",
    "Once the model outputs `<eos>` (or we reach the maximum length $T'$) the output sequence is completed. Then we can concatenate all the output tokens to get a complete output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Implement a greedy search function to predict a sequence using `net_pos`.<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_greedy(net, src, length):\n",
    "    # Run net to predict the output given the input `q` and y_prev based on `a`.\n",
    "    # Return predicted y\n",
    "    src = torch.tensor(pad_or_trim(tokenize_and_encode(src), 2*complexity+3)).unsqueeze(0)\n",
    "    print(src)\n",
    "    y_prev = torch.tensor([[vocab['<bos>']]])\n",
    "    predicted_sequence = []\n",
    "    for i in range(length):\n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            pred_y = net(src, y_prev)\n",
    "            predicted_token = torch.argmax(pred_y[0, -1])\n",
    "            predicted_sequence.append(predicted_token.item())\n",
    "            y_prev = torch.cat((y_prev, predicted_token.unsqueeze(0).unsqueeze(0)), axis=1)\n",
    "    return predicted_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3,  4,  5,  0,  3,  4,  5, 13, 14]])\n",
      "['2' '4' '6' '<eos>' '<pad>' '<pad>']\n"
     ]
    }
   ],
   "source": [
    "predicted_sequence = predict_greedy(net_pos, '123+123', 6)\n",
    "print(decode_tokens(predicted_sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Does this search strategy give a high-quality prediction? Why, or why not?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the greedy search strategy performs quite well as it gives the correct answer by greedily selecting the most likely tokens at each time step. For calculations where we don't need to \"look\" ahead to see if there are carry bits, this strategy might work well because the output at a given position only depends on the digits at that position. For calculations with a carry bit this may not work so well..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) What alternative search strategy could we use to improve the predictions? Why would this help?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use beam search to search for the most likely sequence, rather than the most likely tokens by maximizing this expression:\n",
    "\n",
    "$$\n",
    "\\prod_{t'=1}^{T'}P(y_{t'}|y_1,...,y_{t'-1},\\boldsymbol{c})\n",
    "$$\n",
    "\n",
    "This would likely improve the overall solutions because it can \"look ahead\" to consider various possible sequences and select that sequence with highest probability, which might be higher than the sequence generated by taking the most probable token at each timestep. This may be useful for calculations that require a carry operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 Discussion (4 points)\n",
    "\n",
    "Last week, we looked at recurrent neural networks such as the LSTM. Both recurrent neural networks and transformers work with sequences, but in recent years the transformer has become more popular than the recurrent models.\n",
    "\n",
    "**(a) An advantage of transformers over recurrent neural is that they can be faster to train. Why is that?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are faster to train because we can process all the input tokens in parallel in a transformer, whereas RNNs and LSTMs must process input tokens sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Does this advantage also hold when predicting outputs for new sequences? Why, or why not?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, this advantage does not hold, prediciting ouput tokens at inference time is a sequential operation in both architectures. This is because the predict each output token we require the previous output token to perform autoregession. The next output token is conditioned on those tokens that came before it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Why is positional encoding often used in transformers, but not in convolutional or recurrent neural networks?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positional encoding is used in transformers because it has no other mechanism to distinguish between input sequences with identical tokens but ordered differently whereas RNNs and convolutional networks process information sequentially and thus implicitly define an ordering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of a recurrent neural network makes it very suitable for online predictions, such as real-time translation, because it only depends on prior inputs. You can design an architecture where the RNN produces an output token for every input token given to it, and it can produce that output without having to wait for the rest of the input.\n",
    "\n",
    "Note: 'online' means producing outputs continuously as new input comes in, as opposed to collecting a full dataset and analyzing it afterwards, it has nothing to do with the internet.\n",
    "\n",
    "**(d) How would a transformer work in an online application? Do you need to change the architecture?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work in an online procedure as described aboce, the transformer architecture would need to be complemented with a causal attention mask that simulates the recurrent input by hiding empty tokens and adding each newly predicted token to the input of the next call, like in the inference loop above. Technically speaking, the transformer is still taking a full sequence as an input, but by using the workaround with the attention mask and updating the input with each new token, the transformer architecture could be used 'live'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end\n",
    "\n",
    "Well done! Please double check the instructions at the top before you submit your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This assignment has 47 points.*\n",
    "<span style=\"float:right;color:#aaa;font-size:10px;\"> Version 4f237eb / 2024-10-08</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
