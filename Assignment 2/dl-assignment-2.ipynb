{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning &mdash; Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second assignment for the 2024 Deep Learning course (NWI-IMC070) of the Radboud University."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "**Names:**\n",
    "\n",
    "**Group:**\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instructions:**\n",
    "* Fill in your names and the name of your group.\n",
    "* Answer the questions and complete the code where necessary.\n",
    "* Keep your answers brief, one or two sentences is usually enough.\n",
    "* Re-run the whole notebook before you submit your work.\n",
    "* Save the notebook as a PDF and submit that in Brightspace together with the `.ipynb` notebook file.\n",
    "* The easiest way to make a PDF of your notebook is via File > Print Preview and then use your browser's print option to print to PDF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "In this assignment you will\n",
    "1. Implement a neural network in PyTorch;\n",
    "2. Use automatic differentiation to compute gradients;\n",
    "3. Experiment with SGD and Adam;\n",
    "4. Experiment with hyperparameter optimization;\n",
    "5. Experiment with regularization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, if PyTorch or pandas is not installed, install it now with `pip install`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install torchvision\n",
    "# !pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_formats = ['png']\n",
    "%matplotlib inline\n",
    "#import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import time\n",
    "import torchvision\n",
    "import tqdm.notebook as tqdm\n",
    "import collections\n",
    "import IPython\n",
    "import pandas as pd\n",
    "\n",
    "#np.set_printoptions(suppress=True, precision=6, linewidth=200)\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Fix the seed, so outputs are exactly reproducible\n",
    "torch.manual_seed(12345);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Implementing a model with PyTorch\n",
    "\n",
    "In the first assignment, you already worked with PyTorch tensors. Here we will show you some more ways to use them.\n",
    "\n",
    "### Shape and data type\n",
    "\n",
    "PyTorch tensors are multi-dimensional arrays. So, they can be scalars, vectors, matrices, or have an even higher dimension. You can see that by looking at their shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a 10x10 matrix filled with zeros\n",
    "x = torch.zeros([10, 10])\n",
    "print(x)\n",
    "# The shape shows it is a rank 2 tensor:\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors can store different types of data. The default is 32-bit floating point numbers. The `dtype` property tells you the data type. You can also make tensors with 64-bit floating point numbers, or various types of integers. (See the [PyTorch documentation](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype) for a complete list.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some tensors with a different datatype\n",
    "print((x == x).dtype)\n",
    "print(torch.ones(5, dtype=torch.int).dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be important to choose the correct datatype for your tensors, because this influences the precision, the computational cost, or the memory requirements. Some functions also specifically require integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving data to and from the GPU\n",
    "\n",
    "If you have a GPU, you can use it to increase the speed of PyTorch computations. To do this, you have to move your data to the GPU by calling `.to('cuda')` (or `.cuda()`). Afterwards, you can move the results back to the CPU by calling `to('cpu')` or `cpu()`.\n",
    "\n",
    "Note: in MacOS you can use the `'mps'` backend instead of `'cuda'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this only works if you have a GPU available\n",
    "if torch.cuda.is_available():\n",
    "    # define a variable on the CPU\n",
    "    x = torch.ones((3,))\n",
    "    # move the variable to the GPU\n",
    "    x = x.to('cuda')\n",
    "    print('x is now on the GPU:', x)\n",
    "    # move the variable back to the CPU\n",
    "    x = x.to('cpu')\n",
    "    print('x is now on the CPU:', x)\n",
    "elif torch.backends.mps.is_available():\n",
    "    # define a variable on the CPU\n",
    "    x = torch.ones((3,))\n",
    "    # move the variable to the GPU\n",
    "    x = x.to('mps')\n",
    "    print('x is now on the GPU:', x)\n",
    "    # move the variable back to the CPU\n",
    "    x = x.to('cpu')\n",
    "    print('x is now on the CPU:', x)\n",
    "else:\n",
    "    print('It looks like you don\\'t have a GPU available.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If you want to run a computation on the GPU, all variables of that function should be moved to the GPU first. If some variables are still on the CPU, PyTorch will throw an error.\n",
    "\n",
    "**(a) If you have a GPU, make the following code run without errors.<span style=\"float:right\"> (no points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    x = torch.tensor([1, 2, 3])\n",
    "    y = torch.tensor([1, 2, 3], device='cuda')\n",
    "    print('x is on the CPU:', x)\n",
    "    print('y is on the GPU:', y)\n",
    "    \n",
    "    # this will not work\n",
    "    # TODO: make this computation run on the GPU\n",
    "    z = x * y\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the GPU when it is available, and fall back to the CPU otherwise, a common trick is to define a global `device` constant. You can then use `tensor.to(device)` and `torch.tensor(device=device)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = detect_device()\n",
    "print(\"Running on\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting back to NumPy\n",
    "\n",
    "Sometimes, it is useful to convert PyTorch tensors back to NumPy arrays, for example, if you want to plot the performance of your network.\n",
    "\n",
    "Call `detach().cpu().numpy()` on the tensor variable to convert the variable to NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(5.)\n",
    "x_in_numpy = x.detach().cpu().numpy()\n",
    "x_in_numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here:\n",
    "* `detach` detaches the tensor from the other computation, by among other things, removing gradient information.\n",
    "* `cpu` transfers the tensor from GPU to CPU if needed.\n",
    "* Finally `numpy` converts from a pytorch tensor to a numpy array.\n",
    "Numpy arrays are very similar to torch tensors, they just come from different libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the gradients of a simple model\n",
    "\n",
    "We saw last week how to compute gradients. Here is a quick recap:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the gradient automatically\n",
    "\n",
    "You can compute an automatic gradient as follows:\n",
    "\n",
    "1. Tell PyTorch which variables need a gradient. You can do this by setting `requires_grad=True` when you define the variable.\n",
    "2. Perform the computation.\n",
    "3. Use the `backward()` function on the result to compute the gradients using backpropagation.\n",
    "4. The `grad` property of your variables will now contain the gradient.\n",
    "\n",
    "Have a look at this example, and compare the gradients with the gradients we computed manually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(1., requires_grad=True)\n",
    "x = torch.tensor(5.)\n",
    "\n",
    "# compute the function\n",
    "y = x * w + b\n",
    "\n",
    "# compute the gradients, given dy = 1\n",
    "y.backward()\n",
    "\n",
    "print('w.grad:', w.grad)\n",
    "print('b.grad:', b.grad)\n",
    "# x did not have requires_grad, so no gradient was computed\n",
    "print('x.grad:', x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also works for much more complicated functions (and even entire neural networks):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(1., requires_grad=True)\n",
    "x = torch.tensor(5.)\n",
    "\n",
    "y = torch.exp(torch.sin(x * w) + b)\n",
    "y.backward()\n",
    "\n",
    "print('w.grad:', w.grad)\n",
    "print('b.grad:', b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the automatic differentiation history\n",
    "\n",
    "PyTorch can compute these gradients automatically because it keeps track of the operations that generated the result.\n",
    "\n",
    "While you don't normally need to do this, you can look inside `y.grad_fn` to see how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(2., requires_grad=True)\n",
    "b = torch.tensor(1., requires_grad=True)\n",
    "x = torch.tensor(5.)\n",
    "\n",
    "y = x * w + b\n",
    "\n",
    "print('y.grad_fn:', y.grad_fn)\n",
    "print('  \\-->', y.grad_fn.next_functions)\n",
    "print('          \\-->', y.grad_fn.next_functions[0][0].next_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `grad_fn` of `y` contains a tree that reflects how `y` was computed:\n",
    "* the last operation was an addition (x * w) __plus__ b: `AddBackward0` knows how to compute the gradient of that;\n",
    "* one of the inputs to the addition was a multiplication x __times__ w: `MulBackward0` computes the gradient;\n",
    "* eventually, the backpropagation reaches the input variables: `AccumulateGrad` is used to store the gradient in the `grad` property of each variable.\n",
    "\n",
    "As long as you use operations for which PyTorch knows the gradient, the `backward()` function can perform automatic backpropagation and the chain rule to compute the gradients. If you want, can read more about this in the [PyTorch autograd tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 PyTorch neural network with torch.nn (2 points)\n",
    "\n",
    "The `torch.nn` module of PyTorch contains a large number of building blocks to construct your own neural network architectures. You will need them in this and future assignments. Have a look at the [documentation for `torch.nn`](https://pytorch.org/docs/stable/nn.html) to see what is available. In this assignment, we will use [`torch.nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) to build networks with linear layers, as well as some activation and loss functions.\n",
    "\n",
    "### A network module\n",
    "\n",
    "As a first example, the two-layer network from last week can be implemented like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Week1Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = torch.nn.Linear(64, 32)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.layer2 = torch.nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "net = Week1Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the following:\n",
    "* A network in PyTorch is usually implemented as a subclass of `torch.nn.Module`, as it is here.\n",
    "* The `__init__` function defines the layers that are used in the network.\n",
    "* The `forward` function computes the output of the network given one or more inputs (in this case: `x`).\n",
    "* The layers can be used as if they are functions, for example `self.layer1(x)`. This call will compute the output of the layer given input `x`.\n",
    "  * You can also call your own network with `net(x)`, internally this will call `net.forward(x)`.\n",
    "* The `Linear` layer is separate from the activation function, which is done in its own `ReLU` layer.\n",
    "\n",
    "### Network parameters\n",
    "\n",
    "Some of the components of the network have parameters, such as the weight and bias in the Linear layers. We can list them using the `parameters` or `named_parameters` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name)\n",
    "    print(param)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, these parameters have been initialized to non-zero values. \n",
    "\n",
    "**(a) Why are these parameters not zero?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shortcut: use `torch.nn.Sequential`\n",
    "\n",
    "Quite often, as in our network above, a network architecture consists of a number of layers that are computed one after the other. PyTorch has a class `torch.nn.Sequential` to quickly define these networks, without having to define a new class.\n",
    "\n",
    "For example, the network we implemented earlier can also be written like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_net():\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Linear(64, 32),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(32, 10)\n",
    "    )\n",
    "net = build_net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions\n",
    "\n",
    "You may have also noticed that there is no activation function (such as a sigmoid or softmax) at the end of the network. This is not a mistake. Instead we will be using a loss function that combines the sigmoid/softmax and the loss in one computation, because this is more numerically stable.\n",
    "\n",
    "Have a look at [`torch.nn.BCEWithLogitsLoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html), which implements binary cross entropy.\n",
    "The documentation mentions the \"log-sum-exp trick\", which refers to the fact that:\n",
    "\\begin{equation*}\n",
    "\\log(\\exp(a_1) + \\exp(a_2)) =\n",
    "%\\log(\\exp(c)\\exp(a_1 - c) + \\exp(c)\\exp(a_2 - c)) =\n",
    "c + \\log(\\exp(a_1 - c) + \\exp(a_2 - c))\n",
    "\\end{equation*}\n",
    "Then we pick $c$ to be $\\max(a_1,a_2)$, so all arguments to $\\exp$ are \u2264 1. And in fact this becomes\n",
    "\\begin{equation*}\n",
    "\\log(\\exp(a_1) + \\exp(a_2)) =\n",
    "\\max(a_1,a_2) + \\log(1 + \\exp(-|a_1-a_2|)).\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multi-class equivalent of binary cross entropy loss is categorical cross entropy. In PyTorch this is implemented in [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "\n",
    "**(b) Does `CrossEntropyLoss` expect probabilities or logits? In other words, should you apply a softmax activation function or not?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 A neural network for Fashion-MNIST (12 points)\n",
    "\n",
    "In this assignment, we will do experiments with the [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist). First, we download the dataset, and create a random training set with 1000 images and a validation set with 500 images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashionmnist = torchvision.datasets.FashionMNIST(\n",
    "    root=\".\", download=True,\n",
    "    transform=torchvision.transforms.Compose([\n",
    "         torchvision.transforms.ToTensor(),\n",
    "         lambda img: img.flatten()\n",
    "    ]))\n",
    "\n",
    "# use 1000 samples for training, 500 for validation, ignore the rest\n",
    "fashion_train, fashion_validation, _ = torch.utils.data.random_split(\n",
    "    fashionmnist, [1000, 500, len(fashionmnist) - (1000 + 500)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot some images\n",
    "\n",
    "The Fashion-MNIST contains images of 28 by 28 pixels, from 10 different classes. In our experiments we flatten the images to a vector with 28 x 28 = 784 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some of the images\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.imshow(np.hstack([fashion_train[i][0].reshape(28, 28) for i in range(9)]), cmap='gray')\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.axis('off')\n",
    "plt.title('labels: ' + str([fashion_train[i][1] for i in range(9)]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Can you, as a human, distinguish the different classes? Do you think a neural network should be able to learn to do this as well? Briefly explain your answer.<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "(Briefly means in at most 1 or 2 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the DataLoader to create batches\n",
    "\n",
    "We can use the `DataLoader` class from PyTorch (see the [documentation](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)) to automatically create random batches of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(fashion_train, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the data loader to loop over all batches in the dataset.\n",
    "\n",
    "For each batch, we get `x`, a tensor containing the images, and `y`, containing the labels for each image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in data_loader:\n",
    "    print('x.dtype:', x.dtype, 'x.shape:', x.shape)\n",
    "    print('y.dtype:', y.dtype, '  y.shape:', y.shape)\n",
    "    # one batch is enough for now\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a network\n",
    "\n",
    "We will construct a network that can classify these images.\n",
    "\n",
    "**(b) Implement a network with the following architecture using `torch.nn.Sequential`:<span style=\"float:right\"> (2 points)</span>**\n",
    "\n",
    "* Accept flattened inputs: 28 x 28 images become an input vector with 784 elements.\n",
    "* Linear hidden layer 1, ReLU activation, output 128 features.\n",
    "* Linear hidden layer 2, ReLU activation, output 64 features.\n",
    "* Linear output layer, to 10 classes.\n",
    "* No final activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_net():\n",
    "    # TODO: construct and return the network\n",
    "    return None\n",
    "net = build_net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Test your network by creating a data loader and computing the output for one batch:<span style=\"float:right\"> (3 points)</span>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a data loader to loop over fashion_train,\n",
    "# with a batch size of 16, shuffle the dataset\n",
    "# TODO: construct a DataLoader\n",
    "data_loader = ...\n",
    "\n",
    "# TODO: construct your network\n",
    "net = ...\n",
    "for x, y in data_loader:\n",
    "    print('x.shape:', x.shape)\n",
    "    print('y.shape:', y.shape)\n",
    "    # TODO: run your network to compute the output for x\n",
    "    output = ...\n",
    "    print('output.shape', output.shape)\n",
    "    assert len(output.shape) == 2\n",
    "    assert output.shape[0] == x.shape[0], \"Expected an output for every input sample\"\n",
    "    assert output.shape[1] == 10, \"Expected logit for 10 classes\"\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a network with PyTorch\n",
    "\n",
    "To train the network, we need a number of components:\n",
    "* A network, like the one you just defined.\n",
    "* A [DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) to loop over the training samples in small batches.\n",
    "* A loss function, such as the cross-entropy loss. See the [loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions) in the PyTorch documentation.\n",
    "* An optimizer, such as SGD or Adam: after we use the `backward` function to compute the gradients, the optimizer computes and applies the updates to the weights of the network. See the [optimization algorithms](https://pytorch.org/docs/stable/optim.html) in the PyTorch documentation.\n",
    "\n",
    "As an example, the code below implements all of these components and runs a single update step of the network.\n",
    "\n",
    "**(d) Have a look at the code to understand how it works. Then make the following changes:<span style=\"float:right\"> (4 points)</span>**\n",
    "* Set the batch size to 16\n",
    "* Use Adam as the optimizer and set the learning rate to 0.01\n",
    "* For each minibatch, compute the output of the network\n",
    "* Compute and optimize the cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a new instance of the network\n",
    "net = build_net()\n",
    "\n",
    "# construct a data loader for the training set\n",
    "data_loader = torch.utils.data.DataLoader(fashion_train, shuffle=True)\n",
    "\n",
    "# initialize the SGD optimizer\n",
    "# we pass the list of parameters of the network\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0)\n",
    "\n",
    "# TODO: Initialize cross-entropy loss function\n",
    "loss_function = ...\n",
    "\n",
    "# repeat for multiple epochs\n",
    "for epoch in range(10):\n",
    "    # compute the mean loss and accuracy for this epoch\n",
    "    loss_sum = 0.0\n",
    "    accuracy_sum = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    # loop over all minibatches in the training set\n",
    "    for x, y in data_loader:\n",
    "        # compute the prediction given the input x\n",
    "        # TODO: compute the output\n",
    "        output = ...\n",
    "\n",
    "        # compute the loss by comparing with the target output y\n",
    "        # TODO: use loss_function to compute the loss\n",
    "        loss = ...\n",
    "\n",
    "        # for a one-hot encoding, the output is a score for each class\n",
    "        # we assign each sample to the class with the highest score\n",
    "        pred_class = torch.argmax(output, dim=1)\n",
    "        # compute the mean accuracy\n",
    "        accuracy = torch.mean((pred_class == y).to(float))\n",
    "\n",
    "        # reset all gradients to zero before backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        # compute the gradient\n",
    "        loss.backward()\n",
    "        # use the optimizer to update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        accuracy_sum += accuracy.detach().cpu().numpy()\n",
    "        loss_sum += loss.detach().cpu().numpy()\n",
    "        steps += 1\n",
    "\n",
    "    # print('y:', y)\n",
    "    # print('pred_class:', pred_class)\n",
    "    # print('accuracy:', accuracy)\n",
    "    print('epoch:', epoch,\n",
    "          'loss:', loss_sum / steps,\n",
    "          'accuracy:', accuracy_sum / steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Run the optimization for a few epochs. Does the loss go down? Has the training converged?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Looking back at the network, we did not include a SoftMax activation function after the last linear layer. But typically you need to use a softmax activation when using cross-entropy loss. Was there a mistake?<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Hint: Look at the documentation of the cross-entropy loss function. Is the formula there the same as in the slides?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(optional) Why do you think the developers of PyTorch did it this way?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIONAL TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Training code for the rest of this assignment\n",
    "\n",
    "For the rest of this assignment, we will use a slightly more advanced training function. It runs the training loop for multiple epochs, and at the end of each epoch evaluates the network on the validation set.\n",
    "\n",
    "Feel free to look inside, but keep in mind that some of this code is only needed to generate the plots in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(net, train, validation, optimizer, epochs=25, batch_size=10, device=device):\n",
    "    \"\"\"\n",
    "    Train and evaluate a network.\n",
    "     - net:               the network to optimize\n",
    "     - train, validation: the training and validation sets\n",
    "     - optimizer:         the optimizer (such as torch.optim.SGD())\n",
    "     - epochs:            the number of epochs to train\n",
    "     - batch_size:        the batch size\n",
    "     - device:            whether to use a gpu ('cuda') or the cpu ('cpu')\n",
    "    \n",
    "    Returns a dictionary of training and validation statistics.\n",
    "    \"\"\"\n",
    "\n",
    "    # move the network parameters to the gpu, if necessary\n",
    "    net = net.to(device)\n",
    "    \n",
    "    # initialize the loss and accuracy history\n",
    "    history = collections.defaultdict(list)\n",
    "    epoch_stats, phase = None, None\n",
    "\n",
    "    # initialize the data loaders\n",
    "    data_loader = {\n",
    "        'train':      torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True),\n",
    "        'validation': torch.utils.data.DataLoader(validation, batch_size=batch_size)\n",
    "    }\n",
    "\n",
    "    # measure the length of the experiment\n",
    "    start_time = time.time()\n",
    "\n",
    "    # some advanced PyTorch to look inside the network and log the outputs\n",
    "    # you don't normally need this, but we use it here for our analysis\n",
    "    def register_measure_hook(idx, module):\n",
    "        def hook(module, input, output):\n",
    "            with torch.no_grad():\n",
    "                # store the mean output values\n",
    "                epoch_stats['%s %d: %s output mean' % (phase, idx, type(module).__name__)] += \\\n",
    "                    output.mean().detach().cpu().numpy()\n",
    "                # store the mean absolute output values\n",
    "                epoch_stats['%s %d: %s output abs mean' % (phase, idx, type(module).__name__)] += \\\n",
    "                    output.abs().mean().detach().cpu().numpy()\n",
    "                # store the std of the output values\n",
    "                epoch_stats['%s %d: %s output std' % (phase, idx, type(module).__name__)] += \\\n",
    "                    output.std().detach().cpu().numpy()\n",
    "        module.register_forward_hook(hook)\n",
    "\n",
    "    # store the output for all layers in the network\n",
    "    for layer_idx, layer in enumerate(net):\n",
    "        register_measure_hook(layer_idx, layer)\n",
    "    # end of the advanced PyTorch code\n",
    "    \n",
    "    for epoch in tqdm.tqdm(range(epochs), desc='Epoch', leave=False):\n",
    "        # initialize the loss and accuracy for this epoch\n",
    "        epoch_stats = collections.defaultdict(float)\n",
    "        epoch_stats['train steps'] = 0\n",
    "        epoch_stats['validation steps'] = 0\n",
    "        epoch_outputs = {'train': [], 'validation': []}\n",
    "\n",
    "        # first train on training data, then evaluate on the validation data\n",
    "        for phase in ('train', 'validation'):\n",
    "            # switch between train and validation settings\n",
    "            net.train(phase == 'train')\n",
    "\n",
    "            epoch_steps = 0\n",
    "            epoch_loss = 0\n",
    "            epoch_accuracy = 0\n",
    "            \n",
    "            # loop over all minibatches\n",
    "            for x, y in data_loader[phase]:\n",
    "                # move data to gpu, if necessary\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "                # compute the forward pass through the network\n",
    "                pred_y = net(x)\n",
    "\n",
    "                # compute the current loss and accuracy\n",
    "                loss = torch.nn.functional.cross_entropy(pred_y, y)\n",
    "                pred_class = torch.argmax(pred_y, dim=1)\n",
    "                accuracy = torch.mean((pred_class == y).to(float))\n",
    "\n",
    "                # add to epoch loss and accuracy\n",
    "                epoch_stats['%s loss' % phase] += loss.detach().cpu().numpy()\n",
    "                epoch_stats['%s accuracy' % phase] += accuracy.detach().cpu().numpy()\n",
    "\n",
    "                # store outputs for later analysis\n",
    "                epoch_outputs[phase].append(pred_y.detach().cpu().numpy())\n",
    "\n",
    "                # only update the network in the training phase\n",
    "                if phase == 'train':\n",
    "                    # set gradients to zero\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # backpropagate the gradient through the network\n",
    "                    loss.backward()\n",
    "\n",
    "                    # track the gradient and weight of the first layer\n",
    "                    # (not standard; we only need this for the assignment)\n",
    "                    epoch_stats['train mean abs grad'] += \\\n",
    "                        torch.mean(torch.abs(net[0].weight.grad)).detach().cpu().numpy()\n",
    "                    epoch_stats['train mean abs weight'] += \\\n",
    "                        torch.mean(torch.abs(net[0].weight)).detach().cpu().numpy()\n",
    "\n",
    "                    # update the weights\n",
    "                    optimizer.step()\n",
    "\n",
    "                epoch_stats['%s steps' % phase] += 1\n",
    "\n",
    "            # compute the mean loss and accuracy over all minibatches\n",
    "            for key in epoch_stats:\n",
    "                if phase in key and not 'steps' in key:\n",
    "                    epoch_stats[key] = epoch_stats[key] / epoch_stats['%s steps' % phase]\n",
    "                    history[key].append(epoch_stats[key])\n",
    "\n",
    "            # count the number of update steps\n",
    "            history['%s steps' % phase].append((epoch + 1) * epoch_stats['%s steps' % phase])\n",
    "            \n",
    "            # store the outputs\n",
    "            history['%s outputs' % phase].append(np.concatenate(epoch_outputs[phase]).flatten())\n",
    "\n",
    "        history['epochs'].append(epoch)\n",
    "        history['time'].append(time.time() - start_time)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper code to plot our results\n",
    "class HistoryPlotter:\n",
    "    def __init__(self, plots, table, rows, cols):\n",
    "        self.plots = plots\n",
    "        self.table = table\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "        self.histories = {}\n",
    "        self.results = []\n",
    "        \n",
    "        self.fig, self.axs = plt.subplots(ncols=cols * len(plots), nrows=rows,\n",
    "                                          sharex='col', sharey='none',\n",
    "                                          figsize=(3.5 * cols * len(plots), 3 * rows))\n",
    "        plt.tight_layout()\n",
    "        IPython.display.display(self.fig)\n",
    "        IPython.display.clear_output(wait=True)\n",
    "\n",
    "    # add the results of an experiment to the plot\n",
    "    def add(self, title, history, row, col):\n",
    "        self.histories[title] = history\n",
    "        self.results.append((title, {key: history[key][-1] for key in self.table}))\n",
    "\n",
    "        for plot_idx, plot_xy in enumerate(self.plots):\n",
    "            ax = self.axs[row, col * len(self.plots) + plot_idx]\n",
    "            for key in plot_xy['y']:\n",
    "                ax.plot(history[plot_xy['x']], history[key], label=key)\n",
    "            if 'accuracy' in plot_xy['y'][0]:\n",
    "                ax.set_ylim([0, 1.01])\n",
    "            ax.legend()\n",
    "            ax.set_xlabel(plot_xy['x'])\n",
    "            ax.set_title(title)\n",
    "        plt.tight_layout()\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        IPython.display.display(self.fig)\n",
    "\n",
    "    # print a table of the results for all experiments\n",
    "    def print_table(self):\n",
    "        df = pd.DataFrame([\n",
    "            { 'experiment': title, **{key: row[key] for key in self.table} }\n",
    "            for title, row in self.results\n",
    "        ])\n",
    "        IPython.display.display(df)\n",
    "\n",
    "    def done(self):\n",
    "        plt.close()\n",
    "        self.print_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Optimization and hyperparameters (10 points)\n",
    "\n",
    "An important part of training a neural network is hyperparameter optimization: finding good learning rates, minibatch sizes, and other parameters to train an efficient and effective network.\n",
    "\n",
    "In this part, we will explore some of the most common hyperparameters.\n",
    "\n",
    "### Learning rate with SGD and Adam\n",
    "\n",
    "First, we will investigate optimizers and learning rates:\n",
    "* The choice of optimizer: [SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD) and especially [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam) are common choices.\n",
    "* The learning rate determines the size of the updates by the optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimizing hyperparameters is often a matter of trial-and-error.\n",
    "\n",
    "We will run an experiment to train our network with the following settings:\n",
    "* Optimizer: SGD or Adam\n",
    "* Learning rate: 0.1, 0.01, 0.001, 0.0001\n",
    "* Minibatch size: 32\n",
    "* 150 epochs\n",
    "\n",
    "For each setting, we will plot:\n",
    "* The train and validation accuracy\n",
    "* The train and validation loss\n",
    "\n",
    "We will also print a table with the results of the final epoch.\n",
    "\n",
    "**(a) Run the experiment and have a look at the results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = HistoryPlotter(plots=[{'x': 'epochs', 'y': ['train loss', 'validation loss']},\n",
    "                                {'x': 'epochs', 'y': ['train accuracy', 'validation accuracy']},],\n",
    "                         table=['train accuracy', 'validation accuracy'],\n",
    "                         rows=4, cols=2)\n",
    "\n",
    "epochs = 150\n",
    "batch_size = 32\n",
    "\n",
    "for row, lr in enumerate((0.1, 0.01, 0.001, 0.0001)):\n",
    "    net = build_net()\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "    history = fit(net, fashion_train, fashion_validation, optimizer=optimizer, epochs=epochs, batch_size=batch_size)\n",
    "    plotter.add('SGD lr=%s' % str(lr), history, row=row, col=0)\n",
    "\n",
    "    net = build_net()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    history = fit(net, fashion_train, fashion_validation, optimizer=optimizer, epochs=epochs, batch_size=batch_size)\n",
    "    plotter.add('Adam lr=%s' % str(lr), history, row=row, col=1)\n",
    "\n",
    "plotter.done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, not every combination of hyperparameters works equally well.\n",
    "\n",
    "**(b) Was 150 epochs long enough to train the network with all settings? List the experiments that have/have not yet converged.<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Experiments that have converged to a good result:* TODO\n",
    "\n",
    "*Experiments that need more training:* TODO\n",
    "\n",
    "*Other experiments:* TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) How does the learning rate affect the speed of convergence?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) A larger learning rate does not always lead to better or faster training. What happened to Adam with a learning rate of 0.1?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) It seems that Adam works reasonably well with learning rates 0.01, 0.001, and 0.0001. Can you explain the difference between the three learning curves, in terms of performance, stability, and speed?<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Final performance:* TODO\n",
    "\n",
    "*Stability:* TODO\n",
    "\n",
    "*Speed:* TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same accuracy, increasing loss\n",
    "\n",
    "You may have noticed something interesting in the curves for \"Adam lr=0.001\": after 10 to 20 epochs, the loss on the validation set starts increasing again, while the accuracy remains the same. How is this possible?\n",
    "\n",
    "We can find a clue by looking at the output of the network. We will plot the final outputs: the prediction just before the softmax activation function. These values are also called 'logits'.\n",
    "\n",
    "**(f) Run the code below to generate the plots.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_output_stats():\n",
    "    fig, axs = plt.subplots(ncols=2, nrows=2,\n",
    "                                figsize=(6 * 2, 4 * 2))\n",
    "    # plot train and validation accuracy\n",
    "    axs[0, 0].plot(plotter.histories['Adam lr=0.001']['epochs'],\n",
    "                   plotter.histories['Adam lr=0.001']['train accuracy'],\n",
    "                   label='train accuracy')\n",
    "    axs[0, 0].plot(plotter.histories['Adam lr=0.001']['epochs'],\n",
    "                   plotter.histories['Adam lr=0.001']['validation accuracy'],\n",
    "                   label='validation accuracy')\n",
    "    axs[0, 0].set_xlabel('epochs')\n",
    "    axs[0, 0].set_ylabel('accuracy')\n",
    "    axs[0, 0].set_title('Adam lr=0.001')\n",
    "    axs[0, 0].legend()\n",
    "\n",
    "    # plot train and validation loss\n",
    "    axs[0, 1].plot(plotter.histories['Adam lr=0.001']['epochs'],\n",
    "                   plotter.histories['Adam lr=0.001']['train loss'],\n",
    "                   label='train loss')\n",
    "    axs[0, 1].plot(plotter.histories['Adam lr=0.001']['epochs'],\n",
    "                   plotter.histories['Adam lr=0.001']['validation loss'],\n",
    "                   label='validation loss')\n",
    "    axs[0, 1].set_xlabel('epochs')\n",
    "    axs[0, 1].set_ylabel('loss')\n",
    "    axs[0, 1].set_title('Adam lr=0.001')\n",
    "    axs[0, 1].legend()\n",
    "\n",
    "    # plot curve of mean absolute output values\n",
    "    axs[1, 0].plot(plotter.histories['Adam lr=0.001']['epochs'],\n",
    "                   plotter.histories['Adam lr=0.001']['train 4: Linear output abs mean'],\n",
    "                   label='output before softmax (training set)')\n",
    "    axs[1, 0].set_xlabel('epochs')\n",
    "    axs[1, 0].set_ylabel('mean absolute output')\n",
    "    axs[1, 0].set_title('Output before softmax (Adam lr=0.001)')\n",
    "    axs[1, 0].legend()\n",
    "\n",
    "    # plot distributions of output values\n",
    "    for epoch in (149, 80, 25, 5, 1):\n",
    "        axs[1, 1].hist(plotter.histories['Adam lr=0.001']['train outputs'][epoch], bins=50,\n",
    "                       label='epoch %d' % epoch)\n",
    "    axs[1, 1].set_xlabel('output before softmax (training set)')\n",
    "    axs[1, 1].set_ylabel('number of values')\n",
    "    axs[1, 1].set_title('Output before softmax (Adam lr=0.001)')\n",
    "    axs[1, 1].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_output_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bottom left: the mean of the final outputs for all training images at different epochs.<br>\n",
    "Bottom right: histograms showing the distribution of the output values at different epochs.\n",
    "\n",
    "You should now be able to answer this question:\n",
    "\n",
    "**(g) Why does the accuracy remain stable while the loss keeps increasing?<span style=\"float:right\"> (1 points)</span>**\n",
    "\n",
    "*Perhaps you can combine these plots with your knowledge of the softmax activation and cross-entropy loss function to explain this curious behaviour.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch size\n",
    "\n",
    "Another important hyperparameter is the minibatch size. Sometimes your minibatch size is limited by the available memory in your GPU, but you can often choose different values.\n",
    "\n",
    "We will run an experiment to train our network with different minibatch sizes:\n",
    "* Minibatch size: 4, 16, 32, 64\n",
    "\n",
    "We will fix the other hyperparameters to values that worked well in the previous experiment:\n",
    "* Optimizer: Adam\n",
    "* Learning rate: 0.0001\n",
    "* 150 epochs\n",
    "\n",
    "For each setting, we will plot:\n",
    "* The train and validation accuracy vs number of epochs\n",
    "* The train and validation loss vs number of epochs\n",
    "* The train and validation accuracy vs the number of gradient descent update steps\n",
    "* The train and validation accuracy vs the training time\n",
    "\n",
    "We will also print a table with the results of the final epoch.\n",
    "\n",
    "**(h) Run the experiment and have a look at the results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = HistoryPlotter(plots=[{'x': 'epochs', 'y': ['train loss', 'validation loss']},\n",
    "                                {'x': 'epochs', 'y': ['train accuracy', 'validation accuracy']},\n",
    "                                {'x': 'train steps', 'y': ['train accuracy', 'validation accuracy']},\n",
    "                                {'x': 'time', 'y': ['train accuracy', 'validation accuracy']}],\n",
    "                         table=['train accuracy', 'validation accuracy', 'time'],\n",
    "                         rows=5, cols=1)\n",
    "\n",
    "epochs = 150\n",
    "lr = 0.0001\n",
    "batch_sizes = [4, 16, 32, 64, 128]\n",
    "\n",
    "for row, batch_size in enumerate(batch_sizes):\n",
    "    net = build_net()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    history = fit(net, fashion_train, fashion_validation, optimizer=optimizer, epochs=epochs, batch_size=batch_size)\n",
    "    plotter.add('batch_size=%s' % str(batch_size), history, row=row, col=0)\n",
    "\n",
    "plotter.done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(i) Why is it useful to look at the number of training steps and the training time? How is this visible in the plots?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(j) What are the effects of making the minibatches very small?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(k) What are the effects of making the minibatches very large?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Batch normalization (6 points)\n",
    "\n",
    "Besides choosing the right hyperparameters, we can include other components to improve the training of the model.\n",
    "First we will experiment with batch normalization.\n",
    "\n",
    "Batch normalization can be implemented with the modules from `torch.nn` [(documentation)](https://pytorch.org/docs/stable/nn.html#normalization-layers).\n",
    "\n",
    "For a network with 1D feature vectors, you can use `torch.nn.BatchNorm1d` [(documentation)](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d). There is also a `BatchNorm2d` and a  `BatchNorm3d`.\n",
    "\n",
    "**(a) Construct a network with batch normalization:<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "Use the same structure as before, but include batchnorm after the hidden linear layers. So we have:\n",
    "* A linear layer from 784 to 128 features, followed by batchnorm and a ReLU activation.\n",
    "* A linear layer from 128 to 64 features, followd by batchnorm and ReLU activation.\n",
    "* A final linear layer from 64 features to 10 outputs, no activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_net_bn():\n",
    "    # TODO implement the network\n",
    "net = build_net_bn()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run an experiment to compare a network without batch normalization with a network with batch normalization.\n",
    "\n",
    "We will fix the other hyperparameters to values that worked well in the previous experiment:\n",
    "* Optimizer: Adam\n",
    "* Learning rate: 0.0001\n",
    "* Minibatch size: 32\n",
    "* 150 epochs\n",
    "\n",
    "**(b) Run the experiment and have a look at the results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = HistoryPlotter(plots=[{'x': 'epochs', 'y': ['train loss', 'validation loss']},\n",
    "                                {'x': 'epochs', 'y': ['train accuracy', 'validation accuracy']},],\n",
    "                         table=['train accuracy', 'validation accuracy'],\n",
    "                         rows=2, cols=1)\n",
    "\n",
    "epochs = 150\n",
    "lr = 0.0001\n",
    "batch_size = 32\n",
    "\n",
    "networks = [\n",
    "    ('without batchnorm', build_net),\n",
    "    ('with batchnorm',    build_net_bn)\n",
    "]\n",
    "histories = {}\n",
    "for row, (network_name, network_fn) in enumerate(networks):\n",
    "    net = network_fn()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    history = fit(net, fashion_train, fashion_validation, optimizer=optimizer, epochs=epochs, batch_size=batch_size)\n",
    "    plotter.add(network_name, history, row=row, col=0)\n",
    "    histories[network_name] = history\n",
    "\n",
    "plotter.done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Does batch normalization improve the performance of the network?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Does batch normalization affect the training or convergence speed?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look a bit closer at how batch normalization changes the network.\n",
    "\n",
    "We will plot some statistics about the values inside the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_layer_stats(layers, history):\n",
    "    fig, axs = plt.subplots(ncols=layers, nrows=2,\n",
    "                            figsize=(3.5 * layers, 3 * 2))\n",
    "\n",
    "    for layer in range(layers):\n",
    "        i = 0\n",
    "        for stat in ('mean', 'std'):\n",
    "            for phase in ('train', 'validation'):\n",
    "                keys = [key for key in history.keys()\n",
    "                        if '%s %d:' % (phase, layer) in key and 'output %s' % stat in key]\n",
    "                if len(keys) == 1:\n",
    "                    key = keys[0]\n",
    "                    ax = axs[i, layer]\n",
    "                    ax.plot(history['epochs'], history[key], label='%s output %s' % (phase, stat))\n",
    "                    ax.set_xlabel('epochs')\n",
    "                    ax.legend()\n",
    "                    ax.set_title(key.replace(' output %s' % stat, '').replace(phase, 'layer'))\n",
    "            i += 1\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will plot the statistics of the network without batch normalization.\n",
    "\n",
    "For each layer, the plots show the mean and standard deviation of the output values of that layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_stats(5, histories['without batchnorm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make similar plots for the network with batch normalization: (Note that the number of layers is slightly larger.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer_stats(7, histories['with batchnorm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) Compare the mean training values in the batch normalization network with those in the network without batch normalization. Can you explain this with how batch normalization works?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(f) Compare the train and validation curves for the batch normalization layers. The training curves are smooth, but the validation curve is noisy. Why does this happen?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(g) Batch normalization is supposed to normalize the values to $\\mu = 0$ and $\\sigma = 1$, but in layer 4, the mean and standard deviation are steadily increasing over time. Why and how does this happen?<span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Data augmentation (4 points)\n",
    "\n",
    "Next, we will look at data augmentation.\n",
    "\n",
    "We will run experiments with three types of data augmentation:\n",
    "* Adding random noise to the pixels, taken from a normal distribution $\\mathcal{N}(0, \\sigma)$ with $\\sigma$ = 0, 0.01, 0.1, or 0.2.\n",
    "* Flipping the image horizontally.\n",
    "* Shifting the image up, down, left, or right by one pixel.\n",
    "\n",
    "To implement data augmentation, we create a new dataset class that generates augmented images and use this instead of our normal training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, noise_sigma=0, flip_horizontal=False, shift=False):\n",
    "        self.dataset = dataset\n",
    "        self.noise_sigma = noise_sigma\n",
    "        self.flip_horizontal = flip_horizontal\n",
    "        self.shift = shift\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.dataset[idx]\n",
    "        # add random noise\n",
    "        x = x + self.noise_sigma * torch.randn(x.shape)\n",
    "        # flip the pixels horizontally with probability 0.5\n",
    "        if self.flip_horizontal and torch.rand(1) > 0.5:\n",
    "            x = torch.flip(x.reshape(28, 28), dims=(1,)).flatten()\n",
    "        # shift the image by one pixel in the horizontal or vertical directions\n",
    "        if self.shift:\n",
    "            x = x.reshape(28, 28)\n",
    "            # shift max one pixel\n",
    "            shifts = [*torch.randint(-1, 2, (2,)).numpy()]\n",
    "            x = torch.roll(x, shifts=shifts, dims=(0, 1))\n",
    "            x = x.flatten()\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up an experiment to see if data augmentation improves our results. We use combinations of the three augmentations: noise, flipping, and shifting.\n",
    "\n",
    "We will train for 250 epochs.\n",
    "\n",
    "We keep the other settings as before:\n",
    "* Optimizer: Adam\n",
    "* Learning rate: 0.0001\n",
    "* Minibatch size: 32\n",
    "\n",
    "**(a) Run the experiment and have a look at the results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = HistoryPlotter(plots=[{'x': 'epochs', 'y': ['train loss', 'validation loss']},\n",
    "                                {'x': 'epochs', 'y': ['train accuracy', 'validation accuracy']},],\n",
    "                         table=['train accuracy', 'validation accuracy', 'time'],\n",
    "                         rows=8, cols=2)\n",
    "\n",
    "epochs = 250\n",
    "lr = 0.0001\n",
    "batch_size = 32\n",
    "\n",
    "for row, noise_sigma in enumerate((0, 0.01, 0.1, 0.2)):\n",
    "    for row2, shift in enumerate([False,True]):\n",
    "        for col, flip_horizontal in enumerate([False,True]):\n",
    "            noisy_fashion_train = NoisyDataset(fashion_train, noise_sigma, flip_horizontal=flip_horizontal, shift=shift)\n",
    "            net = build_net()\n",
    "            optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "            history = fit(net, noisy_fashion_train, fashion_validation, optimizer=optimizer, epochs=epochs, batch_size=batch_size)\n",
    "            label = ('noise=%s' % str(noise_sigma)) + \\\n",
    "                    (', shift' if shift else '') + \\\n",
    "                    (', flip horizontal' if flip_horizontal else '')\n",
    "            plotter.add(label, history, row=row*2+row2, col=col)\n",
    "\n",
    "plotter.done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) How does data augmentation affect overfitting in the above experiment? Discuss each of the augmentation types.<span style=\"float:right\"> (3 points)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Adding noise:*\n",
    "TODO Your answer here.\n",
    "\n",
    "*Horizontal flips:*\n",
    "TODO Your answer here.\n",
    "\n",
    "*Shifting:*\n",
    "TODO Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) Why do we have to train the networks with data augmentation a bit longer than networks without data augmentation?<span style=\"float:right\"> (1 points)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Network architecture (5 points)\n",
    "\n",
    "An often overlooked hyperparameter is the architecture of the neural network itself. Here you can think about the width (the size of each hidden layer) or the depth (the number of layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Copy the `build_net` function from 2.3b and change it to take a parameter for the width of the first hidden layer. <span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "The second hidden layer should have half that width, so the network we have been using so far has `width=128`.\n",
    "\n",
    "Hint: `a // b` is the Python notation for integer division rounding down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_net(width = 128):\n",
    "    # TODO: construct and return the network\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Set up an experiment to see how the size of the network affects our results. <span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "We keep the other settings as before:\n",
    "* Optimizer: Adam\n",
    "* Epochs: 150\n",
    "* Learning rate: 0.0001\n",
    "* Minibatch size: 32\n",
    "* Widths: 16, 32, 64, 128, 256, 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = HistoryPlotter(plots=[{'x': 'epochs', 'y': ['train loss', 'validation loss']},\n",
    "                                {'x': 'epochs', 'y': ['train accuracy', 'validation accuracy']},],\n",
    "                         table=['train accuracy', 'validation accuracy', 'time'],\n",
    "                         rows=6, cols=1)\n",
    "\n",
    "epochs = 150\n",
    "lr = 0.0001\n",
    "batch_size = 32\n",
    "network_widths = [16, 32, 64, 128, 256, 512]\n",
    "\n",
    "for row, width in enumerate(network_widths):\n",
    "    # TODO: Run optimizer for a network of width width, add to plotter\n",
    "\n",
    "plotter.done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) For what network sizes do you observe underfitting? <span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(d) Do you see overfitting for the largest networks? How can you see this from the plots? <span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(e) How many parameters are there in a network with width 128? How does that compare to the number of training samples?<span style=\"float:right\"> (1 point)</span>**\n",
    "\n",
    "You don't have to give an exact value, as long as you are in the right order of magnitude it is okay.\n",
    "\n",
    "(Feel free to write some python code to do computations.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 Discussion (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) Several of the experiments have included a baseline with exactly the same hyperparameters (batch_size=32, network_size=128). Are the results exactly the same? What does this tell you about comparing results for picking the best hyperparameters?<span style=\"float:right\"> (2 points)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) Throughout this assignment we have used a validation set of 500 samples for selecting hyperparameters. Do you think that you will see the same results on an independent test set? Would the best results be obtained with the hyperparameters that are optimal on the validation set? <span style=\"float:right\"> (1 point)</span>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The end\n",
    "\n",
    "Well done! Please double check the instructions at the top before you submit your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This assignment has 42 points.*\n",
    "<span style=\"float:right;color:#aaa;font-size:10px;\"> Version 7ac2078 / 2024-09-11</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}